\documentclass[a4paper]{article}

\input{C:/Users/liula/Desktop/Latex/Headers V1.2.tex}

\pagestyle{fancy}
\fancyhf{}
\rhead{Labix}
\lhead{Lie Groups and Lie Algebra}
\rfoot{\thepage}

\title{Lie Groups and Lie Algebra}

\author{Labix}

\date{\today}
\begin{document}
\maketitle
\begin{abstract}
Potentially good books: Humphreys, Erdmann and Wildson
\end{abstract}
\pagebreak
\tableofcontents
\pagebreak

\section{Introduction to Lie Algebras}
\subsection{Lie Brackets and Lie Algebras}
\begin{defn}{Lie Brackets}{} Let $V$ be a vector space over a field $k$. Let $[-,-]:V\times V\to V$ be a bilinear map. We say that $[-,-]$ is a Lie bracket if the following are true. 
\begin{itemize}
\item The Alternating Property: $[X,X]=0$
\item Jacobi identity: $[[X,Y],Z]+[[Y,Z],X]+[[Z,X],Y]=0$
\end{itemize}
\end{defn}

Consider the cross product $\times:\R^3\times\R^3\to\R^3$ in $\R^3$. It is easy to see that it is a Lie bracket. 

\begin{defn}{Lie Algebras}{} A Lie algebra is a vector space $V$ over a field $K$ together with a Lie bracket $$[-,-]:V\times V\to V$$
\end{defn}

For $k$ a field, $M_n(k)$ for any $n\geq 1$ is a Lie algebra with Lie bracket defined as $[A,B]=AB-BA$ for $A,B\in M_n(k)$. 

\begin{lmm}{}{} Let $L$ be a Lie Algebra. Then for all $x,y\in L$, we have that $$[x,y]=-[y,x]$$ In other words, the Lie bracket is anti-commutative. \tcbline
\begin{proof}
We have that 
\begin{align*}
[x,y]+[y,x]&=[x,x]+[x,y-x]+[y,y]+[y,x-y]\tag{Bilinearity}\\
&=[x,x]+[y,y]-[x-y,x-y]\tag{Bilinearity}\\
&=0\tag{Alternating}
\end{align*}
and so we conclude. 
\end{proof}
\end{lmm}

Lie Algebras are not algebras (in the sense of Rings and Modules) because the Lie bracket fails associativity. Therefore we have to redefine all the standard notions one has in algebra. 

While Lie Algebras are not in general algebras, every associative algebra can be equipped with a Lie algebra. For $A$ an associative algebra over a field, we can define a bilinear map on $A$ by $$[a,b]=ab-ba$$ for all $a,b\in A$. There may also be more than one way to equip an algebra with a Lie algebra structure. One should not think that Lie Algebras encompasses associative algebras because of the different Lie algebras one can equip. Instead, we think of the Lie bracket as an extra structure on associative algebras such that they become Lie algebras. 

\begin{defn}{Structure Constants}{} Let $L$ be a Lie algebra such that its underlying vector space has basis $e_1,\dots,e_n$. Define the structure constants of $L$ to be the elements $c_{ij}^k\in\F$ such that $$[e_i,e_j]=\sum_{k=1}^nc_{ij}^ke_k$$ for all $1\leq i,j\leq n$. 
\end{defn}

The structure constants are useful in the following sense. Let $L$ be a Lie algebra and let $a=\sum_{k=1}^na_ke_k$ and $b=\sum_{k=1}^nb_ke_k$ be elements of $L$. Then there Lie bracket can be written as $$[a,b]=\sum_{1\leq i<j\leq n}(a_ib_j-a_jb_i)[e_i,e_j]$$ by bilinearity. Plugging in the structure constants, we obtain $$[a,b]=\sum_{1\leq i<j\leq n}(a_ib_j-a_jb_i)\sum_{k=1}^nc_{ij}^ke_k$$ Thus we can write $[a,b]$ in terms of the basis $e_1,\dots,e_n$ using structure constants. 

\subsection{Lie Subalgebras and Ideals}
\begin{defn}{Lie Subalgebra}{} Let $V$ be a Lie algebra over $K$. A lie subalgebra of $V$ is a subset $W\subseteq V$ such that 
\begin{itemize}
\item $W$ is a vector subspace of $V$
\item $[w_1,w_2]\in W$ for all $w_1,w_2\in W$
\end{itemize}
\end{defn}

It is clear that a Lie subalgebra is also a Lie algebra in its own right. 

\begin{defn}{Ideal}{} Let $V$ be a Lie algebra over $K$. Let $I$ be a subset of $V$. Then $I$ is an ideal of $V$ if the following are true. 
\begin{itemize}
\item $I$ is a vector subspace of $V$
\item $[v,i]\in I$ for all $v\in V$ and $i\in I$. 
\end{itemize}
\end{defn}

It is clear from definitions that every ideal of a Lie algebra is a Lie subalgebra. However, the converse is not always true. 

\begin{prp}{}{} Let $V$ be a Lie algebra and $I,J$ ideals of $V$. Then the following are also ideals of $V$. 
\begin{itemize}
\item The intersection $I\cap J$
\item The sum $I+J=\{i+j\;|\;i\in I\text{ and }j\in J\}$
\end{itemize} \tcbline
\begin{proof}~\\
\begin{itemize}
\item It is clear that $I\cap J$ is a vector subspace of $V$. Since $I$ is an ideal, $[V,I\cap J]\subseteq [V,I]\subseteq I$. Similarly, $[V,I\cap J]\subseteq[V,J]\subseteq J$. Hence $[V,I\cap J]\subseteq I\cap J$. 
\item It is clear that $I+J$ is a vector subspace of $V$. Let $i+j\in I+J$. Let $v\in V$. Then we have $$[v,i+j]=[v,i]+[v,j]\in I+J$$ Hence $I+J$ is an ideal of $V$. 
\end{itemize}
\end{proof}
\end{prp}

\begin{defn}{The Lie Bracket of Ideals}{} Let $V$ be a Lie algebra. Let $I,J$ be ideals of $V$. Define the Lie bracket of $I$ and $J$ to be $$[I,J]=\langle[i,j]\;|\;i\in I\text{ and }j\in J\rangle$$
\end{defn}

\begin{lmm}{}{} Let $V$ be a Lie algebra. Let $I,J$ be ideals of $V$. Then the Lie bracket $[I,J]$ is an ideal of $V$. \tcbline
\begin{proof}
By definition, $[I,J]$ is a vector subspace of $V$. Let $v\in V$ and $\sum_{k=1}^na_k[i_k,j_k]\in[I,J]$. Then we have
\begin{align*}
\left[v,\sum_{k=1}^na_k[i_k,j_k]\right]&=\sum_{k=1}^na_k[v,[i_k,j_k]]\\
&=\sum_{k=1}^na_k[v,[i_k,j_k]]\\
&=-\sum_{k=1}^na_k[[i_k,j_k],v]\\
&=-\sum_{k=1}^na_k\left(-[[j_k,v],i_k]-[[v,i_k],j_k]\right)\\
&=-\sum_{k=1}^na_k\left([[v,j_k],i_k]-[[v,i_k],j_k]\right)\\
&=\sum_{k=1}^na_k\left([[v,i_k],j_k]-[[v,j_k],i_k]\right)\\
\end{align*}
Since $I$ and $J$ are ideals, $s_k=[v,i_k]\in I$ and $t_k=[v,j_k]\in J$. We now have $$\left[v,\sum_{k=1}^na_k[i_k,j_k]\right]=\sum_{k=1}^na_k\left([s_k,j_k]-[t_k,i_k]\right)$$ But $[s_k,j_k],[t_k,i_k]$ are generators of $[I,J]$. Hence the sum also lies in $[I,J]$. 
\end{proof}
\end{lmm}

\subsection{Lie Algebra Homomorphisms}
\begin{defn}{Homomorphism of Lie algebra}{} Let $V$ and $W$ be Lie algebras over a field $K$. A homomorphism from $V$ to $W$ is an $K$-linear map $F:V\to W$ such that $$[F(a),F(b)]=F\left([a,b]\right)$$ for all $a,b\in V$. 
\end{defn}

\begin{defn}{Kernel of a Lie Algebra Homomorphism}{} Let $V$ and $W$ be Lie algebras over a field $K$. Let $F:V\to W$ be a Lie algebra homomorphism. Define the kernel of $F$ to be $$\ker(F)=\{v\in V\;|\;F(v)=0_W\}$$
\end{defn}

\begin{lmm}{}{} Let $V$ and $W$ be Lie algebras over a field $K$. Let $F:V\to W$ be a Lie algebra homomorphism. Then $\ker(F)$ is a Lie subalgebra of $V$. \tcbline
\begin{proof}
It is clear that $\ker(F)$ is a vector subspace of $V$. Let $k_1,k_2\in\ker(F)$. Then we have 
\begin{align*}
F([k_1,k_2])&=[F(k_1),F(k_2)]\tag{$F$ is a Lie algebra homomorphism}\\
&=[0,0]\\
&=0
\end{align*}
Hence $[k_1,k_2]\in\ker(F)$ and $\ker(F)$ is a Lie subalgebra of $V$. 
\end{proof}
\end{lmm}

\begin{defn}{Isomorphisms of Lie Algebras}{} Let $V$ and $W$ be Lie algebras over a field $k$. Let $\phi:V\to W$ be a Lie algebra homomorphism. We say that $F$ is a Lie algebra isomorphism if there exists a Lie algebra homomorphism $\psi:W\to V$ such that $\psi\circ\phi=\text{id}_V$ and $\phi\circ\psi=\text{id}_W$. 
\end{defn}

\begin{prp}{}{} Let $V$ and $W$ be Lie algebras over a field $k$. Let $\phi:V\to W$ be a Lie algebra isomorphism. Then $\phi$ is a vector space isomorphism. 
\end{prp}

\begin{thm}{First Isomorphism Theorem}{} Let $\phi:L_1\to L_2$ be a homomorphism of Lie algebras. Then the following are true. 
\begin{itemize}
\item $\ker(\phi)$ is an ideal of $L_1$
\item $\im(\phi)$ is a Lie subalgebra of $L_2$
\end{itemize}
Moreover, we have an isomorphism $$\frac{L_1}{\ker(\phi)}\cong\im(\phi)$$
\end{thm}

\begin{thm}{Second Isomorphism Theorem}{} Let $L$ be a Lie algebra. Let $I$ and $J$ be ideals of $L$. Then the following are true. 
\begin{itemize}
\item $I$ and $J$ are ideals of $I+J$
\item $I\cap J$ is an ideal of $I$ and $J$
\end{itemize}
Moreover, we have an isomorphism $$\frac{I+J}{J}\cong\frac{I}{I\cap J}$$
\end{thm}

\begin{thm}{Third Isomorphism Theorem}{} Let $L$ be a Lie algebra. Let $I$ and $J$ be ideals of $L$ such that $I\subseteq J$. Then $J/I$ is an ideal of $L/I$. Moreover, there is an isomorphism $$\frac{L/I}{J/I}\cong\frac{L}{J}$$
\end{thm}

\begin{thm}{Correspondence Theorem}{} Let $L$ be a Lie algebra with ideal $I$. Then there exists a bijective correspondence $$\{J\;|\;J\text{ is an ideal of }L\text{ and }I\subseteq J\}\;\;\;\;\overset{1:1}{\longleftrightarrow}\;\;\;\;\{K\;|\;K\text{ is an ideal of }L/I\}$$
\end{thm}

\subsection{Products and Quotients of Lie Algebras}
\begin{defn}{Direct Sum of Lie Algebras}{} Let $L_1$ and $L_2$ be Lie algebras. Define the direct sum of $L_1$ and $L_2$ by $$L_1\oplus L_2=\{(a_1,a_2)\;|\;a_1\in L_1,a_2\in L_2\}$$ together with component wise addition and scalar multiplication and Lie bracket operation $$[(a_1,a_2),(b_1,b_2)]=([a_1,b_1],[a_2,b_2])$$ which is component wise application of the Lie bracket for $(a_1,a_2),(b_1,b_2)\in L_1\oplus L_2$. 
\end{defn}

\begin{prp}{}{} Let $L_1$ and $L_2$ be Lie algebras. Then the following are true. 
\begin{itemize}
\item $[L_1\oplus L_2,L_1\oplus L_2]=[L_1,L_1]\oplus[L_2,L_2]$
\item $\{(x,0)\;|\;x\in L_1\}\cong L_1$ is an ideal of $L_1\oplus L_2$
\item $\{(0,y)\;|\;y\in L_2\}\cong L_2$ is an ideal of $L_1\oplus L_2$
\end{itemize}
\end{prp}

\begin{prp}{}{} Let $V$ be a Lie algebra and $U$ an ideal of $V$. Then $V/U$ has a unique Lie algebra structure such that the quotient map $V\to V/U$ is a Lie algebra homomorphism. 
\end{prp}

\begin{defn}{Quotient Lie Algebra}{} Let $V$ be a Lie algebra. Let $U$ be an ideal of $V$. Define the quotient Lie algebra to be the set $$V/U=\{v+U\;|\;v\in V\}$$ together with the Lie bracket defined by $[v+U,w+U]=[v,w]+U$. 
\end{defn}

\subsection{The Centers and Centralizers of Lie Algebras}
\begin{defn}{Center of a Lie Algebra}{} Let $L$ be a Lie algebra. Define the center of $L$ by $$Z(L)=\{z\in L\;|\;[z,x]=0\text{ for all }x\in L\}$$
\end{defn}

\begin{lmm}{}{} Let $L$ be a Lie algebra. Then $Z(L)$ is an ideal of $L$. 
\end{lmm}

\begin{prp}{}{} Let $L_1,L_2$ be Lie algebras over the same field $K$. Then $$Z(L_1\oplus L_2)=Z(L_1)\oplus Z(L_2)$$
\end{prp}

\begin{defn}{The Centralizer of a Subset}{} Let $L$ be a Lie algebra. Let $A\subseteq L$ be a subset. Define the centralizer of $A$ in $L$ to be the set $$C_L(A)=\{x\in L\;|\;[x,a]=0\text{ for all }a\in A\}$$
\end{defn}

\begin{lmm}{}{} Let $L$ be a Lie algebra. Let $A\subseteq L$ be a subset. Then $C_L(A)$ is a Lie subalgebra of $L$. 
\end{lmm}
\subsection{The Lie Algebra of Endomorphisms}
\begin{defn}{The Lie Algebra of Endomorphisms}{} Let $V$ be a vector space over a field $k$. Define the Lie algebra of endomorphisms of $V$ to be the vector space $$\text{End}_k(V)=\{T:V\to V\;|\;T\text{ is linear}\}$$ over $k$ together with Lie bracket $[-,-]:\text{End}_k(V)\to\text{End}_k(V)$ given by $$[T,S]=T\circ S-S\circ T$$
\end{defn}

A priori one needs to check that the above map is indeed a Lie bracket. Let $A\text{End}_k(V)$. Then $[A,A]=A^2-A^2=0$ so that the alternating property is satisfied. For the Jacobi identity, we have that 
\begin{align*}
[[A,B],C]+[[B,C],A]+[[C,A],B]&=[A,B]C-C[A,B]+[B,C]A-A[B,C]+[C,A]B-B[C,A]\\
&=ABC-BAC-CAB+CBA+BCA-CBA\\
&\;\;\;\;-ABC+ACB+CAB-ACB-BCA+BAC\\
&=0
\end{align*}
for all $A,B,C\in\text{End}_k(V)$. 

\begin{lmm}{}{} Let $V$ be a vector space over a field $k$. Let $T,S,R\in\text{End}_k(V)$. Then $$\text{tr}([T,S]\circ R)=\text{tr}(T\circ [S,R])$$
\end{lmm}

\begin{defn}{The Lie Algebra of the Matrix Ring}{} Let $k$ be a field. Let $n\in\N\setminus\{0\}$. The Lie algebra of the matrix ring $M_n(k)$ is given by the Lie bracket $[-,-]:\text{End}_k(V)\to\text{End}_k(V)$ defined by $$[A,B]=AB-BA$$
\end{defn}

\begin{prp}{}{} Let $k$ be a field. Let $n\in\N\setminus\{0\}$. Choose a basis $\{e_1,\dots,e_n\}$ of $k^n$. Then the map $\text{End}_k(k^n)\to M_n(k)$ defined by $$T\mapsto\begin{pmatrix}&&\\
T(e_1) & \cdots & T(e_n)\\
&&
\end{pmatrix}$$ is a Lie algebra isomorphism. 
\end{prp}

\pagebreak
\section{Types of Lie Algebras}
\subsection{Abelian Lie Algebras}
Lie algebras that are Abelian are the simplest Lie algebra there is to study. 

\begin{defn}{Abelian Lie Algebras}{} Let $L$ be a Lie algebra. We say that $L$ is abelian if $$[x,y]=0$$ for all $x,y\in L$. 
\end{defn}

\begin{lmm}{}{} Let $L$ be a Lie algebra. Then $Z(L)$ is abelian. 
\end{lmm}

\begin{lmm}{}{} Let $L$ be a Lie algebra. Let $I$ be an ideal of $L$. Then $L/I$ is abelian if and only if $$[L,L]\subseteq I$$ \tcbline
\begin{proof}
Let $L/I$ be abelian. Let $v,w\in L$. Since $L/I$ is abelian, we have that $[v+I,w+I]=I$. But $[v+I,w+I]=[v,w]+I$ implies that $[v,w]\in I$. Conversely, suppose that $[L,L]\subseteq I$. Then for any $v+I,w+I\in L/I$, $[v+I,w+I]=[v,w]+I=I$. Hence $L/I$ is abelian. 
\end{proof}
\end{lmm}

We can think of this as saying $I=[L,L]$ is the smallest ideal of $L$ for which $L/I$ is abelian. 

\subsection{Soluble Lie Algebras}
Let $L$ be a Lie algebra. We have seen that $\text{rad}(L)$ is soluble and $L/\text{rad}(L)$ is semisimple. Therefore to study a general Lie algebra, we need to understand soluble Lie algebras and semisimple Lie algebras. If we restrict the case to Lie algebras over $\C$, Lie's theorem will solve the first part of the problem, while the study of semisimple Lie algebras is postponed until section 5. 

\begin{defn}{Derived Series}{} Let $L$ be a Lie algebra. Define the derived series $L^{(n)}$ of $L$ to be the sequence recursively defined as follows. 
\begin{itemize}
\item For $n=0$, define $L^{(0)}=L$
\item When $n\in\N\setminus\{0\}$, define $$L^{(n)}=[L^{(n-1)},L^{(n-1)}]$$
\end{itemize}
\end{defn}

\begin{lmm}{}{} Let $L_1,L_2$ be Lie algebras. Let $\phi:L_1\to L_2$ be a Lie algebra homomorphism. Then $$\phi(L_1^{(k)})=\phi(L_1)^{(k)}$$
\end{lmm}

\begin{defn}{Soluble Lie Algebras}{} Let $L$ be a Lie algebra. We say that $L$ is soluble if there exists $n\in\N$ such that $$L^{(n)}=0$$
\end{defn}

\begin{lmm}{}{} Let $L$ be a Lie algebra. If $L$ is abelian, then $L$ is soluble. \tcbline
\begin{proof}
Let $L$ be abelian. Then $[L,L]=0$. Hence $L^{(1)}=[L,L]=0$ so that $L$ is soluble. 
\end{proof}
\end{lmm}

\begin{prp}{}{} Let $L$ be a Lie algebra. Let $I$ and $J$ be ideals of $L$. Then the following are true. 
\begin{itemize}
\item Let $\phi:L\to K$ be a Lie algebra homomorphism. If $L$ is soluble then $\phi(L)$ is soluble. 
\item Let $M$ be a Lie subalgebra of $L$. If $L$ is soluble, then $M$ is soluble. 
\item If $I$ and $L/I$ are soluble, then $L$ is soluble. 
\item If $I$ and $J$ are soluble, then $I+J$ is soluble. 
\end{itemize}
\end{prp}

\subsection{Nilpotent Lie Algebras}
\begin{defn}{Lower Central Series}{} Let $L$ be a Lie algebra. Define the lower central series $L^0,L^1,\dots,L^n,\dots$ as follows. 
\begin{itemize}
\item For $n=0$, define $L^0=L$
\item For $n\in\N\setminus\{0\}$, define $$L^n=[L,L^{n-1}]$$
\end{itemize}
\end{defn}

\begin{lmm}{}{} Let $L_1,L_2$ be Lie algebras. Let $\phi:L_1\to L_2$ be a Lie algebra homomorphism. Then $$\phi(L^k)=(\phi(L))^k$$ for all $k\in\N$. \tcbline
\begin{proof}
We prove by induction. The base case $k=0$ is clear. Suppose that $\phi(L^k)=(\phi(L))^k$. Then we have that 
\begin{align*}
\phi(L^{k+1})&=\phi([L,L^k])\\
&=[\phi(L),\phi(L^k)]\\
&=[\phi(L),\phi(L)^k]\\
&=(\phi(L))^{k+1}
\end{align*}
By induction, we conclude. 
\end{proof}
\end{lmm}

\begin{lmm}{}{} Let $L$ be a Lie algebra. Then there is an isomorphism $$[L,L^n]=[L^n,L]$$ for all $n\in\N$ given by the opposite map $x\mapsto-x$. 
\end{lmm}

\begin{lmm}{}{} Let $L$ be a Lie algebra. Then the following are true. 
\begin{itemize}
\item For all $n\in\N$, $L^n$ is an ideal of $L$. 
\item $L^{n+1}\subseteq L^n$. 
\end{itemize}
\end{lmm}

\begin{defn}{Nilpotent Lie Algebras}{} Let $L$ be a Lie algebra. We say that $L$ is nilpotent if there exists $n\in\N$ such that $$L^n=0$$
\end{defn}

\begin{lmm}{}{} Let $L$ be a Lie algebra. If $L$ is abelian, then $L$ is nilpotent. 
\end{lmm}

\begin{eg}{}{} Consider the following Lie algebras. 
\begin{itemize}
\item $SL(2,\C)$ is nilpotent. 
\item $b_n(\C)$ is not nilpotent for all $n\geq 2$. 
\item $U_3(\C)$ the Heisenberg Lie algebra is nilpotent ($3\times 3$ strictly upper triangular matrices)
\item $U_n(\C)$ is nilpotent for all $n\geq 3$. 
\end{itemize}
\end{eg}

\begin{lmm}{}{} Let $L$ be a Lie algebra. Then the following are true. 
\begin{itemize}
\item Let $M$ be a Lie subalgebra of $L$. If $L$ is nilpotent, then $M$ is nilpotent. 
\item If $L\neq 0$ is nilpotent, then $Z(L)\neq 0$
\item If $L/Z(L)$ is nilpotent, then $L$ is nilpotent. 
\end{itemize} \tcbline
\begin{proof}~\\
\begin{itemize}
\item Let $M$ be a Lie subalgebra of $L$. I claim that $M^k\subseteq L^k$ for all $k\in\N$. The base case $k=0$ is clearly true. Suppose that $M^k\subseteq L^k$. Let $x\in[M,M^k]=M^{k+1}$. Then $x=[m,t]$ for some $m\in M$ and $t\in M^k$. Then $t\in L^k$. Also $m\in L$ implies that $x=[m,t]\in[L,L^k]=L^{k+1}$. Thus $M^{k+1}\subseteq L^{k+1}$. Now since $L$ is nilpotent, there exists $n\in\N$ such that $L^n=0$. Then $M^n\subseteq L^n=0$ so that $M$ is also nilpotent. 
\item Suppose that $n\in\N$ is the smallest natural number such that $L^n=0$. Then $[L,L^{n-1}]=0$. Let $x\in L$. Then for all $y\in L^{n-1}$, we have that $[x,y]=0$. Thus $x\in Z(L)$. 
\item Since $L/Z(L)$ is nilpotent, there exists $n\in\N$ such that $(L/Z(L))^n=0$. Let $\pi:L\to L/Z(L)$ be the quotient homomorphism. Since $\pi$ is surjective, we use the above lemma to find that $$\pi(L^n)=\pi(L)^n=\left(\frac{L}{Z(L)}\right)^n=\frac{L^n+Z(L)}{Z(L)}=0$$ This means that $L^n\subseteq Z(L)$. It follows that $L^{n+1}=[L,L^n]\subseteq[L,Z(L)]=0$ and we conclude. 
\end{itemize}
\end{proof}
\end{lmm}

\subsection{Semisimple Lie Algebras}
\begin{prp}{}{} Let $L$ be a Lie algebra. Then there exists a unique soluble ideal $I$ of $L$ such that for any soluble ideal $J\subseteq L$, we have $J\subseteq I$. 
\end{prp}

\begin{defn}{Radical Ideals}{} Let $L$ be a Lie algebra. Define the radical ideal $\text{rad}(L)\subseteq L$ of $L$ to be the unique soluble ideal of $L$ that contains all other soluble ideals. 
\end{defn}

\begin{defn}{Semisimple Lie Algebras}{}{} Let $L$ be a Lie algebra. We say that $L$ is semisimple if $$\text{rad}(L)=\{0\}$$
\end{defn}

\begin{lmm}{}{} Let $L$ be a Lie algebra. Then $L/\text{rad}(L)$ is semisimple. \tcbline
\begin{proof}
Let $K$ be a soluble ideal of $L/\text{rad}(L)$. By the correspondence theorem, there exists an ideal $I$ of $L$ such that $\text{rad}(L)\subseteq I$ and $K=I/\text{rad}(L)$. Since $\text{rad}(L)$ and $K$ are soluble, we conclude that $I$ is soluble. Hence $I\subseteq\text{rad}(L)$. We conclude that $I=\text{rad}(L)$. Hence $K=\{0\}$. Thus $L/\text{rad}(L)$ is semisimple. 
\end{proof}
\end{lmm}

\begin{lmm}{}{} Let $L$ be a Lie algebra. Then $L$ is not semisimple if and only if $L$ contains a non-trivial abelian ideal. \tcbline
\begin{proof}
If $L$ is not semisimple, then $\text{rad}(L)\neq\{0\}$ is a non-trivial soluble ideal. This means that there exists a smallest $n\in\N$ such that $\text{rad}(L)^{(n)}=0$. But this is the same as saying that $$[\text{rad}(L)^{(n-1)},\text{rad}(L)^{(n-1)}]=\text{rad}(L)^{(n)}=0$$ This means that $\text{rad}(L)^{(n-1)}$ is an abelian ideal. In particular, it is non-trivial since $n$ is the smallest number for which $\text{rad}(L)^{(n)}$ is zero. \\~\\

If $L$ contains a non-trivial abelian ideal $I$, then by lmm 2.2.4 we have that $I$ is soluble. Hence $I\subseteq\text{rad}(L)$ and $\text{rad}(L)$ is non-zero. Hence $L$ is not semisimple. 
\end{proof}
\end{lmm}

\begin{prp}{}{} Let $L$ be a Lie algebra. If $L$ is semisimple, then $\dim(L)\geq 3$. \tcbline
\begin{proof}
Let $L$ be a $1$-dimensional Lie algebra. Let $x\in L$ be non-zero. Then $L=\langle x\rangle$. For $ax,bx\in\langle x\rangle=L$, we have that $$[ax,bx]=ab[x,x]=0$$ hence $L$ is abelian. By the above lemma, $L$ is not semisimple. \\~\\

Let $L$ be a $2$-dimensional Lie algebra. Suppose that $\{x,y\}$ is a basis for $L$. Then for $ax+by$ and $cx+dy$ in $L$, we have that 
\begin{align*}
[ax+by,cx+dy]&=ac[x,x]+ad[x,y]+bc[y,x]+bd[y,y]\\
&=(ad-bc)[x,y]
\end{align*}
\end{proof}
\end{prp}

\subsection{Simple Lie Algebras}
\begin{defn}{Simple Lie Algebras}{} Let $L$ be a Lie algebra. We say that $L$ is simple if $L$ is non-abelian and has no proper non-zero ideals. 
\end{defn}

\begin{lmm}{}{} Let $L$ be a Lie algebra. If $L$ is simple, then $L$ is semisimple. 
\end{lmm}

\begin{defn}{Simple Ideals}{} Let $L$ be a Lie algebra. Let $I$ be an ideal of $L$. We say that $I$ is simple if $I$ is simple as a Lie algebra. 
\end{defn}

\pagebreak
\section{Lie Subalgebras of the Ring of Endomorphisms}
\subsection{The Adjoint Homomorphism}
\begin{defn}{The Adjoint Homomorphism}{} Let $V$ be a Lie algebra. Define the adjoint homomorphism $\text{ad}:V\to\text{End}(V)$ to be the map given by $$\text{ad}(x)(y)=[x,y]$$
\end{defn}

\begin{lmm}{}{} Let $V$ be a Lie algebra. Then then the adjoint homomorphism $\text{ad}:V\to\text{End}(V)$ is a Lie algebra homomorphism. \tcbline
\begin{proof}~\\
\begin{itemize}
\item Linearity: Let $x,y\in V$ and let $a,b\in F$. For any $z\in V$, we have
\begin{align*}
\text{ad}(ax+by)(z)&=[ax+by,z]\\
&=a[x,z]+b[y,z]\\
&=a\text{ad}(x)(z)+b\text{ad}(y)(z)\\
&=\left(a\text{ad}(x)+b\text{ad}(y)\right)(z)
\end{align*}
so that $\text{ad}:V\to\text{End}(V)$ is a linear map. 
\item Preserving the Lie bracket: Let $x,y\in V$. For any $z\in V$, we have
\begin{align*}
[\text{ad}(x),\text{ad}(y)](z)&=\left(\text{ad}(x)\text{ad}(y)-\text{ad}(y)\text{ad}(x)\right)(z)\\
&=\text{ad}(x)\left(\text{ad}(y)(z)\right)-\text{ad}(y)\left(\text{ad}(x)(z)\right)\\
&=\text{ad}(x)([y,z])-\text{ad}(y)([x,z])\\
&=[x,[y,z]]-[y,[x,z]]\\
&=-[[y,z],x]+[[x,z],y]\\
&=-[[y,z],x]-[[z,y],x]-[[y,x],z]\\
&=-[[y,z],x]+[[y,z],x]-[[y,x],z]\\
&=[[x,y],z]\\
&=\text{ad}([x,y])(z)
\end{align*}
Thus we have showed that $[\text{ad}(x),\text{ad}(y)]=\text{ad}([x,y])$. 
\end{itemize}
\end{proof}
\end{lmm}

\begin{lmm}{}{} Let $V$ be a Lie algebra. Then the kernel of the adjoint homomorphism is equal to $$\ker(\text{ad})=Z(V)$$ the center of $V$. \tcbline
\begin{proof}
Let $k\in\ker(\text{ad})$. Let $v\in V$. Then $[k,v]=\text{ad}(k)(v)=0$ since $\text{ad}(k)=0\in\text{End}(V)$. Hence $k\in Z(V)$. Conversely, if $z\in Z(V)$ then we have $$\text{ad}(z)(v)=[z,v]=0$$ for all $v\in V$. Hence $\text{ad}(z)=0\in\text{End}(V)$ and $z\in\ker(\text{ad})$. 
\end{proof}
\end{lmm}

\subsection{Ad-Nilpotency}
\begin{defn}{Ad-Nilpotency}{} Let $L$ be a Lie algebra. Let $x\in L$. We say that $x$ is ad-nilpotent if $\text{ad}(x)$ is a nilpotent in $\text{End}(V)$ (as an element of a ring). 
\end{defn}

\begin{lmm}{}{} Let $L$ be Lie algebra. If $L$ is nilpotent, then all elements $x\in L$ are ad-nilpotent. 
\end{lmm}

\begin{lmm}{}{} Let $V$ be a vector space. Let $L\subseteq\text{End}(V)$ be a Lie subalgebra. If $T\in L$ is nilpotent, then $T$ is ad-nilpotent. 
\end{lmm}

Let $V$ be a vector space over a field $k$. Let $T\in\text{End}(V)$. Recall from Linear Algebra that a Jordan-Chevalley decomposition is two linear maps $D,S\in\text{End}(V)$ such that the following are true: 
\begin{itemize}
\item $T=D+S$
\item $D$ is diagonal and $S$ is nilpotent. 
\item $DS=SD$
\end{itemize}
We showed that such a decomposition always exists and is unique. 

\begin{prp}{}{} Let $V$ be a finite dimensional vector space over a field $k$. Let $T\in\text{End}_k(V)$. Let $T=D+S$ be the unique Jordan-Chevalley decomposition. Then $$\text{ad}(T)=\text{ad}(D)+\text{ad}(S)$$ is the Jordan-Chevalley decomposition of $\text{ad}(T)\in\text{End}(\text{End}(V))$. \tcbline
\begin{proof}
Let $T\in\text{End}_k(V)$ be an endomorphism. Let $T=D+S$ be the Jordan-Chevalley decomposition of $T$. Since $\text{ad}$ is linear, we have that $$\text{ad}(T)=\text{ad}(D)+\text{ad}(S)$$ \\~\\

For any $C\in\text{End}_k(V)$, $\text{ad}(D)$ is defined by $\text{ad}(D)(C)=DC-CD$. Since $D$ is diagonalizable, we can choose a basis $B=\{b_1,\dots,b_n\}$ of $V$ such that the matrix representing $D$ given by $D_B=\text{diag}(\alpha_1,\dots,\alpha_r)$ is diagonal on the basis. For the standard basis $\{e_{i,j}\;|\;1\leq i,j\leq n\}$ on $\text{End}_k(V)$, we have that $$\text{ad}(D)(e_{i,j})=[D_B,e_{i,j}]=(\alpha_i-\alpha_j)e_{i,j}$$ which shows that every standard basis vector is an eigenvector of $\text{ad}(D)$. Hence $\text{ad}(D)$ is diagonal. On the other hand, since $S$ is nilpotent, by the above $\text{ad}(S)$ is nilpotent. \\~\\

Finally, we have that $$(\text{ad}(D)\circ\text{ad}(S)-\text{ad}(S)\circ\text{ad}(D))(C)=[\text{ad}(D),\text{ad}(S)](C)=\text{ad}([D,S])(C)=\text{ad}(0)(C)=0$$ for all $C\in\text{End}_k(V)$. Hence $\text{ad}(D)$ and $\text{ad}(S)$ commutes. Thus $\text{ad}(T)=\text{ad}(D)+\text{ad}(S)$ is a Jordan-Chevalley decomposition. 
\end{proof}
\end{prp}

\subsection{Engel's Theorem}
\begin{prp}{}{} Let $V$ be a vector space. Let $L\subseteq\text{End}(V)$ be a Lie subalgebra such that for all $T\in L$, $T$ is nilpotent. Then there exists $v\in V$ such that $T(v)=0$ for all $T\in L$. \tcbline
\begin{proof}
We induct on the dimension of $L$. 
\end{proof}
\end{prp}

\begin{thm}{Engel's Theorem I}{} Let $V$ be a vector space. Let $L$ be a Lie subalgebra of $\text{End}(V)$. Suppose that for all $x\in L$, $x$ is ad-nilpotent. Then the following are true. 
\begin{itemize}
\item There exists a basis $B$ of $V$ such that every $T\in L$ is strictly upper triangular. 
\item $L$ is nilpotent. 
\end{itemize}
\end{thm}

\begin{thm}{Engel's Theorem II}{} Let $L$ be a Lie algebra. If $x\in L$ is ad-nilpotent for all $x\in L$, then $L$ is nilpotent. 
\end{thm}

\subsection{Lie's Theorem}
\begin{thm}{Lie's Theorem}{} Let $V$ be a vector space over $\C$. Let $L$ be a soluble Lie subalgebra of $\text{End}(V)$. Then there exists a basis $B$ of $V$ such that for all $M\in L$, $M$ is upper triangular. 
\end{thm}

\begin{prp}{}{} Let $L$ be a Lie algebra. Then $L$ is soluble if and only if $[L,L]$ is nilpotent. 
\end{prp}

\begin{prp}{}{} Let $V$ be a vector space over a field $k$. Let $L\leq\text{End}_k(V)$ be a Lie subalgebra. If $\text{tr}(xy)=0$ for all $x,y\in L$, then $L$ is soluble. 
\end{prp}

\section{The Killing Form of a Lie Algebra}
\subsection{The Killing Form}
Let $A=(a_{i,j})$ be an $n\times n$ matrix. Recall that the trace of $A$ is defined as $$\text{tr}(A)=\sum_{k=1}^na_{k,k}$$ Now let $T:V\to V$ be a linear map. Then we can also define the trace of $T$ abstractly so that any choice of representation of $T$ with a matrix gives coinciding trace. This is also given in Linear Algebra. The formula is $$\text{tr}(T)=\sum_{i=1}^n\langle T(e_i),e_i\rangle$$

\begin{defn}{The Killing Form}{} Let $L$ be a Lie algebra over $\C$. Define the killing form of $L$ to be the function $$k:L\times L\to\C$$ given by $k(x,y)=\text{tr}(\text{ad}(x)\circ\text{ad}(y))$. 
\end{defn}

\begin{lmm}{}{} Let $L$ be a Lie algebra. Then then following are true. 
\begin{itemize}
\item The killing form on $L$ is a symmetric bilinear form. 
\item $k([x,y],z)=k(x,[y,z])$ for all $x,y,z\in L$. 
\end{itemize} \tcbline
\begin{proof}
Let $L$ be a Lie algebra over a field $\F$. Let $x,y\in L$. Then we have that $$k(x,y)=\text{tr}(\text{ad}(x)\circ\text{ad}(y))=\text{tr}(\text{ad}(x)\text{ad}(y)$$ since the trace function preserve commutation. Thus $k$ is symmetric. \\~\\

Because it is symmetric, it is sufficient to show that $k$ is linear in the first variable for bilinearity. But the adjoint homomorphism is linear and composition preserves linearity. Hence $k$ is bilinear. \\~\\

Finally, we have that 
\begin{align*}
k([x,y],z)&=\text{tr}(\text{ad}([x,y])\circ\text{ad}(z))\\
&=\text{tr}([\text{ad}(x),\text{ad}(y)]\circ\text{ad}(z))\tag{$\text{ad}$ is a Lie algebra Hom}\\
&=\text{tr}(\text{ad}(x)\circ[\text{ad}(y),\text{ad}(z)])\tag{lmm1.6.2}\\
&=k(x,[y,z])
\end{align*}
\end{proof}
\end{lmm}

\begin{lmm}{}{} Let $L$ be a finite dimensional Lie algebra. Let $I$ be an ideal of $L$. If $k$ is the killing form of $L$ and $k|_I$ is the killing form of $I$, then $$k|_I=k|_{I\times I}$$ \tcbline
\begin{proof}
Let $I$ be an ideal of $L$ so that $I$ is also a Lie subalgebra of $L$. Let $B_I$ be a basis for $I$. Extend it to a basis $B_L$ of $L$. Let $x\in I$. Then we have a Lie algebra homomorphism $\text{ad}(x):L\to I$ since $[x,z]\in I$ for all $z\in L$. We can then represent $\text{ad}(x)$ in the basis $B$ using the matrix $$T=\begin{pmatrix}
A_x & B_x\\
0 & 0
\end{pmatrix}$$ where $A_x$ is the matrix representing the linear map $\text{ad}(x)|_I:I\to I$. For any $x,y\in I$, we have that 
\begin{align*}
k(x,y)&=\text{tr}(\text{ad}(x)\circ\text{ad}(y))\\
&=\text{tr}\left(\begin{pmatrix}
A_x & B_x\\
0 & 0
\end{pmatrix}\begin{pmatrix}
A_y & B_y\\
0 & 0
\end{pmatrix}\right)\\
&=\text{tr}\left(\begin{pmatrix}
A_xA_y & A_xB_y\\
0 & 0
\end{pmatrix}\right)\\
&=\text{tr}(A_xA_y)\\
&=k_I(x,y)
\end{align*}
\end{proof}
\end{lmm}

\subsection{Cartan's Two Criteria}
\begin{thm}{Cartan's First Criterion}{} Let $L$ be a Lie algebra over $\C$. Then $L$ is soluble if and only if $k(x,y)=0$ for all $x\in L$ and $y\in[L,L]$. \tcbline
\begin{proof}
Let $L$ be soluble. Then $\text{ad}(L)=[L,L]$ is a Lie subalgebra of $L$ and is soluble by 2.2.5. By Lie's theorem, there exists a basis $B$ of $L$ such that every element of $\text{ad}(L)\leq\text{End}_\C(L)$ is upper triangular. Hence every element of $\text{ad}([L,L])=[\text{ad}(L),\text{ad}(L)]$ is represented by a strictly upper triangular matrix (Why????). Hence $\text{tr}(\text{ad}(x)\circ\text{ad}(y))=0$ for all $x\in L$ and $y\in[L,L]$. \\~\\

Now suppose that $\text{tr}(\text{ad}(x)\circ\text{ad}(y))=0$ for all $x\in L$ and $y\in[L,L]$. This means that $\text{ad}([L,L])=[\text{ad}(L),\text{ad}(L)]$ is soluble by prp3.4.3. By 2.1.2 we know that $\text{ad}(L)/([\text{ad}(L),\text{ad}(L)])$ is abelian. By prp2.2.5, we conclude that $\text{ad}(L)$ is soluble. Since $Z(L)$ is abelian, $Z(L)$ is soluble. Hence by the same proposition we conclude that $L$ is soluble since $\text{ad}(L)=L/Z(L)$. 
\end{proof}
\end{thm}

Let $L$ be a Lie algebra. The killing form allows $L$ to be an inner product space. Recall from Linear Algebra that the orthogonal complement of a subspace $W$ of $L$ is given by $$W^\perp=\{x\in L\;|\;k(x,w)=0\text{ for all }w\in W\}$$

\begin{lmm}{}{} Let $L$ be a Lie algebra. Let $I$ be an ideal of $I$. Then $I^\perp$ is an ideal of $L$. \tcbline
\begin{proof}
We already know that $I^\perp$ is a vector space. We want to show that $[x,i]\in I^\perp$ for all $x\in L$ and $i\in I^\perp$. Let $j\in I$ be arbitrary. Then we have $$k([x,i],j)=k(x,[i,j])=0$$ since $[i,j]\in I$. Hence $[x,i]\in I^\perp$ and we are done. 
\end{proof}
\end{lmm}

Recall from Linear Algebra that a bilinear form is non-degenerate if  if $$V^\perp=\{v\in V\;|\;\tau(v,w)=0\text{ for all }w\in V\}=0$$ Because the killing form is symmetric bilinear, this is the same as saying the orthogonal complement $V^\perp=0$. 

\begin{thm}{Cartan's Second Criterion}{} Let $L$ be a Lie algebra over $\C$. Then $L$ is semisimple if and only if $k$ is non-degenerate. \tcbline
\begin{proof}
Assume that $k$ is degenerate. Then $L^\perp\neq\{0\}$ and hence a non-trivial ideal of $L$. Let $x\in L^\perp$ and $y\in[L^\perp,L^\perp]\subseteq L$. Then $k(x,y)=0$ by definition of $L^\perp$. Then $k_{L^\perp}=0$. By Cartan's first criterion, $L^\perp$ is soluble. Hence $L^\perp\subseteq\text{rad}(L)$ and $L$ is not semisimple. \\~\\

Now suppose that $L$ is not semisimple. Then $L$ contains a non-trivial abelian ideal $I$. Let $x\in L$ and $i\in I$. Then we have that $$\left(\text{ad}(i)\circ\text{ad}(x)\circ\text{ad}(i)\right)(y)=[i,[x,[i,y]]]=0$$ for any $y\in L$ because $[x,[i,y]]\in I$ and $I$ is abelian. Applying $\text{ad}(x)$ on both sides show that $$\left(\text{ad}(x)\circ\text{ad}(i)\right)^2=0$$ Thus $\text{ad}(x)\circ\text{ad}(i)$ is nilpotent in $\text{End}(L)$. By Engel's theorem, we can find a basis of $L$ such that $\text{ad}(x)\circ\text{ad}(i)$ is strictly upper triangular. This means that $k(x,i)=\text{tr}(\text{ad}(x)\circ\text{ad}(i))=0$. But this means that $I\subseteq L^\perp$ since $i\in I\subseteq L$ is such that $k(i,x)=0$ for all $x\in L$. Since $I$ is non-trivial, $L^\perp$ is non-trivial. Thus $k$ is degenerate. 
\end{proof}
\end{thm}

\begin{lmm}{}{} Let $L$ be a Lie algebra over $\C$. Let $I$ be an ideal of $L$. If $L$ is semisimple, then the following are true. 
\begin{itemize}
\item $I\cap I^\perp=\{0\}$
\item $L=I\oplus I^\perp$ as Lie algebras
\item $I$ and $I^\perp$ are semisimple
\end{itemize}
\end{lmm}

\begin{thm}{}{} Let $L$ be a Lie algebra over $\C$. Then $L$ is semisimple if and only if there exists simple ideals $I_1,\dots,I_k$ of $L$ such that $$L=I_1\oplus\cdots\oplus I_k$$ \tcbline
\begin{proof}
Suppose that $L$ is semisimple. We induct on the dimension of $L$. We have seen that $1$-dimensional and $2$-dimensional Lie algebras cannot be semisimple. So we begin with the base case $\dim(L)=3$. If $L$ has a proper non-trivial ideal $I$, then $I$ is either $1$-dimensional or $2$-dimensional. Then $I$ is soluble by the classification theorems. This means that $I\subseteq\text{rad}(L)$ is non-zero. This is a contradiction since we assumed that $L$ is semisimple. Hence $L$ has no proper non-trivial ideals. Hence $L$ is simple. \\~\\

Now suppose the result holds for all Lie algebras of dimension $<n$. Let $\dim(L)=n$. Let $I$ be a non-zero minimal ideal of $L$. By lmm 4.2.6, $L=I\oplus I^\perp$ where $I$ and $I^\perp$ are both semisimple. If $L=I$ then we are done. If $L\neq I$ then $I$ has dimension strictly less than $n$. By inductive hypothesis, there exists simple ideals $I_1,\dots,I_k$ of $I$ and simple ideals $J_1,\dots,J_s$ such that $$I=\bigoplus_{i=1}^k I_i\;\;\;\;\text{ and }\;\;\;\;I^\perp=\bigoplus_{j=1}^sJ_j$$ Now we want to show that $I_i$ and $J_j$ are ideals of $L$. Let $x\in L$. Then $x=a+b$ for $a\in I$ and $b\in I^\perp$. If $z\in I_i$, then $$[x,z]=[a,z]+[b,z]=[a,z]\in I_i$$ since $[b,z]\in I\cap I^\perp=\{0\}$. Similarly, one can show that $[x,r]\in J_j$ whenever $r\in J_j$. Thus $I_i$ and $J_j$ are ideals of $L$. Hence we obtain a direct sum decomposition $$L=I\oplus I^\perp=\bigoplus_{i=1}^kI_i\oplus\bigoplus_{j=1}^sJ_j$$ Thus the induction is complete. \\~\\

Now let $L=\bigoplus_{j=1}^kI_j$. Assume that $\text{rad}(L)\neq\{0\}$. This means that $R_j=\text{rad}(L)\cap I_j$ is a soluble ideal of $L$ and $I_j$. Since $I_j$ is simple, $I_j$ is semisimple. Hence $R_j=\{0\}$ so that $$[R,I_j]\subseteq R\cap I_j=R_j=\{0\}$$ Let $x\in R$ and $y\in L$. Then we can write $y$ as $y=y_1+\dots+y_k$ for $y_j\in I_j$. Since $[R,I_j]=\{0\}$, we have that $[x,y_j]=0$ so that $[x,y]=0$. Thus $x\in Z(L)$. This means that $R\subseteq Z(L)$. But $Z(L)$ is abelian so $Z(L)\subseteq R$. Hence $R=Z(L)$. By prp 1.5.3, we have that $$Z(L)=\bigoplus_{j=1}^kZ(I_j)=\{0\}\oplus\cdots\oplus\{0\}=\{0\}$$ Hence $R=\{0\}$ and $L$ is semisimple. 
\end{proof}
\end{thm}

Given a Lie algebra $L$ over $\C$, the following are now equivalent. 
\begin{itemize}
\item $L$ is semisimple ($\text{rad}(L)=\{0\}$)
\item $L$ contains no non-zero abelian ideals. 
\item The killing form $k:L\times L\to L$ is non-degenerate. 
\item $L$ decomposes into a finite direct sum of simple Lie algebras. 
\end{itemize}


\pagebreak
\section{Special Subalgebras of a Lie Algebra}
\subsection{Derivations of Lie Algebras}
\begin{defn}{Derivations}{} Let $L$ be a Lie algebra over $\F$. Let $\phi:L\to L$ be a Lie algebra homomorphism. We say that $\phi$ is a derivation if $$\phi([a,b])=[a,\phi(b)]+[\phi(a),b]$$ for all $a,b\in L$. 
\end{defn}

\begin{defn}{The Lie Subalgebra of All Derivations}{} Let $L$ be a Lie algebra over a field $\F$. Define the Lie subalgebra of derivations to be the set $$\text{Der}_\F(L)=\{\phi:L\to L\;|\;\phi\text{ is a derivation }\}$$ together with Lie algebra structure inherited from $\text{End}_\F(L)$. 
\end{defn}

Recall the we have seen from Linear Algebra the following fact: If $T\in GL(V)$ is a linear map on $V$ with minimal polynomial factorizing as $$\mu_T=(x-\lambda_1)^{a_1}\cdots(x-\lambda_k)^{a_k}$$ where the eigenvalues $\lambda_i$ are distinct and $a_i\geq 1$, then $V$ decomposes as a direct sum of $T$-invariant subspaces $$V=V_1\oplus\cdots\oplus V_k$$ where $V_i=\ker(T-\lambda_i I)^{a_i}$ is the generalized eigenspace. 

\begin{prp}{}{} Let $L$ be a semisimple Lie algebra over $\C$. Then $$\text{Der}_\C(L)=\{\text{ad}(x):L\to L\;|\;x\in L\}$$ In other words, the only derivations of $L$ is given exactly by the adjoints. 
\end{prp}

\subsection{Abstract Jordan Decomposition}
\begin{defn}{Abstract Jordan Decomposition}{} Let $L$ be a semisimple Lie algebra over $\C$. Let $x\in L$. Define an abstract Jordan decomposition of $x$ to be a pair of elements $d,n\in L$ such that the following are true. 
\begin{itemize}
\item $x=d+n$
\item $\text{ad}(d)\in\text{End}_\C(L)$ is diagonalizable and $\text{ad}(n)\in\text{End}_\C(L)$ is nilpotent. 
\item $[d,n]=0$
\end{itemize}
\end{defn}

\begin{crl}{}{} Let $L$ be a semisimple Lie algebra over $\C$. Let $x\in L$ be an element. Then there exists a unique abstract Jordan decomposition of $x$. 
\end{crl}

\subsection{Cartan Subalgebras}
\begin{defn}{Cartan Subalgebra}{} Let $L$ be a Lie algebra. Let $H\leq L$ be a Lie subalgebra of $L$. We say that $H$ is a Cartan subalgebra of $L$ if the following are true. 
\begin{itemize}
\item $H$ is abelian
\item For each $h\in H$, the abstract Jordan decomposition $h=d+n$ of $h$ is such that $n=0$ (In other words $\text{ad}(h)$ is semisimple????)
\item $H$ is maximal with respect to these two properties
\end{itemize}
\end{defn}

The reason we would like to consider such a subalgebra is due to the following result in Linear Algebra: 

\begin{lmm}{}{} Let $V$ be a vector space over $k$. Suppose that $T_1,\dots,T_n\in GL(V)$ is diagonalizable. Then there exists a basis of $GL(V)$ such that $T_1,\dots,T_n$ are diagonal if and only if $T_1,\dots,T_n$ pairwise commute. 
\end{lmm}

\begin{lmm}{}{} Let $L$ be a semisimple Lie algebra over $\C$. Then there exists a non-trivial Cartan subalgebra. 
\end{lmm}

\pagebreak
\section{Representation Theory of Lie Algebras}
\subsection{Modules over Lie Algebras}
\begin{defn}{Modules over Lie Algebras}{} Let $L$ be a Lie algebra over $k$. An $L$-module consists of a vector space $V$ over $k$ together with an action $$\cdot:L\times V\to V$$ such that the following are true. 
\begin{itemize}
\item Linearity in $L$: For all $a,b\in k$ and $x,y\in L$, we have that $$(ax+by)\cdot v=a(x\cdot v)+b(y\cdot v)$$ for all $v\in V$. 
\item Linearity in $V$: For all $x\in L$, we have that $$x\cdot(av+bw)=a(x\cdot v)+b(x\cdot w)$$ for all $a,b\in k$ and $v,w\in V$. 
\item Commutes with the Lie bracket: $[x,y]\cdot v=x\cdot(y\cdot v)-y\cdot(x\cdot v)$. 
\end{itemize}
\end{defn}

\begin{defn}{Submodule of Modules over Lie Algebras}{} Let $L$ be a Lie algebra. Let $V$ be an $L$-module. A submodule of $V$ is a vector subspace $W$ that is a module over $L$ in its own right. 
\end{defn}

\begin{defn}{L-Module Homomorphisms}{} Let $L$ be a Lie algebra. Let $V,W$ be $L$-modules. An $L$-module homomorphism is a linear transformation $\phi:V\to W$ such that $$\phi(x\cdot v)=x\cdot\phi(v)$$ for all $x\in L$ and $v\in V$. 
\end{defn}

\begin{defn}{L-Module Isomorphisms}{} Let $L$ be a Lie algebra. Let $V,W$ be $L$-modules. Let $\phi:V\to W$ be an $L$-module homomorphism. We say that $\phi$ is an $L$-module isomorphism if $\phi$ is an isomorphism of vector spaces. 
\end{defn}

\subsection{Reducibility of Modules over Lie Algebras}
\begin{defn}{Direct Sum of Modules over Lie Algebras}{} Let $L$ be a Lie algebra. Let $V$ be an $L$-module. Let $U$ and $W$ be vector subspaces of $V$. We say that $V$ is the direct sum of $U$ and $W$ if $$V=U\oplus W$$ as vector spaces and $U$ and $W$ are $L$-submodules of $V$. 
\end{defn}

\begin{defn}{Simple Modules over Lie Algebras}{} Let $L$ be a Lie algebra. Let $V$ be an $L$-module. We say that $V$ is simple if $V$ has no proper non-trivial $L$-submodules. 
\end{defn}

\begin{defn}{Completely Reducible}{} Let $L$ be a Lie algebra. Let $V$ be an $L$-module. We say that $V$ is completely reducible if for all $L$-submodules $W$, there exists an $L$-submodule $U$ of $V$ such that $$V=W\oplus U$$
\end{defn}

Simple modules are thus vacuously completely reducible. 

\begin{thm}{Weyl's Theorem}{} Let $L$ be a semi-simple Lie algebra over $\C$. Let $V$ be an $L$-module. Then $V$ is completely reducible. 
\end{thm}

\subsection{Basic Representation Theory of Lie Algebras}
\begin{defn}{Representations of a Lie Algebra}{} Let $L$ be a Lie algebra. Let $V$ be a vector space. A representation of $L$ is a Lie algebra homomorphism $$\rho:L\to\text{End}(V)$$
\end{defn}

\begin{lmm}{}{} Let $L$ be a Lie algebra. Then the adjoint homomorphism $$\text{ad}:L\to\text{End}(L)$$ is a representation of $L$. 
\end{lmm}

\begin{prp}{}{} Let $L$ be a Lie algebra over $k$. Then representations of $L$ and $L$-modules are in bijection $$\{\cdot:L\times V\to V\text{ an }L\text{-module}\}\;\;\overset{1:1}{\leftrightarrow}\;\;\{L\to\text{End}(V)\text{ a Lie algebra representation}\}$$ This bijection is given by sending an $L$-module $\cdot:L\times V\to V$ to the Lie algebra homomorphism $\phi:L\to GL(V)$ defined by $\phi(l)(v)=l\cdot v$. 
\end{prp}

These two ways two think about the same thing is natural. Recall that a representation of a group can be thought of as either group homomorphism $G\to GL(V)$ or a $k[G]$-module. \\

Idea???? This bijection is a ???-homomorphism. 

\subsection{The Case of $\mathfrak{sl}(2,\C)$}

\pagebreak
\section{Some Examples of Lie Algebras}
\subsection{1-Dimensional Lie Algebras}
\begin{prp}{}{} Let $L$ be a Lie algebra of dimension $1$. Then $L$ is abelian. \tcbline
\begin{proof}
If $L$ is $1$-dimensional over a field $k$, let $x$ be a spanning element of $L$. Then for $s,t\in L$, there exists $a,b\in k$ such that $s=ax$ and $t=bx$. Now we have that $$[ax,bx]=ab[x,x]=0$$ Hence $L$ is abelian. 
\end{proof}
\end{prp}

\begin{prp}{}{} Let $L$ be a Lie algebra over a field $k$ such that $\dim(L)=1$. Then $L$ is isomorphic to $k$ equipped with the trivial Lie bracket. 
\end{prp}

\subsection{2-Dimensional Lie Algebras}
\begin{prp}{}{} Let $L$ be a Lie algebra of dimension $2$. Then $L$ is soluble. \tcbline
\begin{proof}
If $L$ is abelian, then lmm 2.2.4 implies that $L$ is soluble. So suppose that $L$ is non-abelian. Then $L^{(1)}=[L,L]\neq 0$. Suppose for a contradiction that $[L,L]=L$. Let $v,w\in L$ be non-zero. Define $x=[v,w]$ and extend it to a basis $\{x,y\}$ of $L$. Suppose that $v=ax+by$ and $w=cx+dy$. Then we have that $$x=[v,w]=[ax+by,cx+dy]=(ad-bc)[x,y]$$ Since $[L,L]=L$, there exists $f,g\in L$ such that $[f,g]=y$. Let $f=px+qy$ and $g=rx+sy$. Then we have that $$y=[f,g]=[px+qy,rx+sy]=(ps-qr)[x,y]$$ Then we have that $$\frac{1}{ad-bc}x=[x,y]=\frac{1}{(ps-qr)}y$$ which means that $x$ and $y$ are linearly dependent. This is a contradiction. Hence $[L,L]\neq L$. \\~\\

Since $[L,L]$ is a Lie subalgebra of $L$, $[L,L]$ must be $1$-dimensional. By the classification of $1$-dimensional Lie algebras, $L^{(1)}=[L,L]$ is abelian. Hence $L^{(2)}=[L^{(1)},L^{(1)}]=0$. Thus $L$ is soluble. 
\end{proof}
\end{prp}

\begin{prp}{}{} Let $L$ be a Lie algebra over a field $k$ such that $\dim(L)=2$. Then $L$ is isomorphic to one of the following. 
\begin{itemize}
\item The vector space $k^2$ together with the trivial Lie bracket $[x,y]=0$. 
\item The vector space $k^2$ together with the Lie bracket defined by 
\end{itemize}
\end{prp}

\subsection{3-Dimensional Lie Algebras}

\pagebreak
\pagebreak
\section{Introduction to Lie Groups}
\subsection{Lie Groups}
\begin{defn}{Lie Groups}{} A Lie group $G$ is a group $G$ that is also a smooth manifold such that the following are true. 
\begin{itemize}
\item The multiplication map $\cdot:G\times G\to G$ defined by $$(g,h)\mapsto gh$$ is a smooth map of manifolds. 
\item The inverse map $(-)^{-1}:G\times G$ defined by $$g\mapsto g^{-1}$$ is a smooth map of manifolds. 
\end{itemize}
\end{defn}

Some immediate examples of Lie groups include the following: 
\begin{itemize}
\item Any finite group is discrete and hence are zero-dimensional manifolds. So they are Lie groups. 
\item $(\R^n,+)$ and $(\R^n\setminus\{0\},\times)$. 
\item The torus $(\R^n/\Z^n,+)$. 
\item $\GL(V)$ for any finite dimensional real vector space $V$. 
\item $U(n), SU(n), O(n), SO(n), SL(n), PSL(n)$ for any $n\in\N$. 
\end{itemize}

\begin{prp}{}{} Let $G$ be a Lie group. Let $H$ be a subgroup of $G$. If $H$ is closed in $G$, then $H$ inherits the structure of a Lie group from $G$. 
\end{prp}

\begin{defn}{Lie Group Homomorphism}{} Let $G,H$ be Lie groups. A Lie group homomorphism is a map of sets $$\phi:G\to H$$ that is a group homomorphism and a smooth map. 
\end{defn}

\subsection{Relation between Lie Groups and Lie Algebras}
For a group $G$, denote the left multiplication map of $h\in G$ by $l_h$. If $G$ is a Lie group, we have seen that $l_h$ is a smooth map, and so it induces a differential $(l_h)_\ast$. 

\begin{defn}{Left Invariant Vector Field}{} Let $G$ be a Lie group and $X$ a vector field on $G$. We say that $X$ is left invariant if $$(l_h)_\ast(X_g)=X_{hg}$$ for all $X_g\in T_g(G)$. 
\end{defn}

\begin{prp}{}{} Let $G$ be a Lie group. The vector space of left invariant vector fields of $G$ is a Lie algebra of dimension $\dim(G)$. 
\end{prp}

\begin{prp}{}{} Let $G$ be a Lie group. Let $v\in T_{1_G}(G)$ be a tangent vector at the identity. Then there exist a unique left invariant vector field $X:G\to TG$ on $G$ such that $X({1_G})=v$. 
\end{prp}

\begin{defn}{Lie Algebra of a Lie Group}{} Let $G$ be a Lie group. Define the Lie algebra of $G$ to be the Lie algebra $T_{1_G}(G)$. 
\end{defn}

Recall that given a homomorphism of Lie groups $\phi:G\to H$, it induces a differential $\phi_\ast:T_g(G)\to T_{\phi(g)}(H)$. 

\begin{prp}{}{} Let $\phi:G\to H$ be a homomorphism of Lie groups with Lie algebras $V$ and $W$ respectively. Then the induced map from the differential $$(\phi_\ast)_{1_G}:T_{1_G}G\to T_{1_H}H$$ is a Lie algebra homomorphism. 
\end{prp}

In other words, we constructed a functor from Lie groups to Lie algebras sending a Lie group to the tangent space at the identity. 

\subsection{The Exponential Map}

\pagebreak
\section{Root Systems of Vector Spaces}
\subsection{Root Systems}
\begin{defn}{Root Systems of an Inner Product Space over $\R$}{} Let $E$ be a finite dimensional inner product space over $\R$. Let $R\subseteq E$. We say that $R$ is a root system in $E$ if the following are true. 
\begin{itemize}
\item $R$ is finite and $0\notin R$
\item $\R\langle R\rangle=E$ ($R$ spans $E$)
\item If $a\in R$, the $ca\in R$ if and only if $c=\pm1$
\item If $a,b\in R$, then $$\sigma_a(b)=b-2\frac{\langle a,b\rangle}{\langle a,a\rangle}a\in R$$
\item If $a,b\in R$, then $\langle a,b\rangle\in\Z$
\end{itemize}
\end{defn}

We can see some first examples in low dimensions. \\

Let $E=\R^1$ with the standard inner product. For any $\alpha\in\R\setminus\{0\}$, $\{\alpha,-\alpha\}$ is clearly a root system. \\

Let $E=\R^2$ again with the standard inner product. Notice that any root system must span $\R^2$ and hence must contain at least two linearly independent vectors and their additive inverses. So choose $\alpha$ and $\beta$ in $\R^2$ that are linearly independent, and WLOG choose $\|\alpha\|\leq\|\beta\|$. Let $\theta$ be the angle between $\alpha$ and $\beta$. WLOG we can assume that $\theta\geq\frac{\pi}{2}$ because otherwise we can take the negative of $\alpha$ or $\beta$ in the root system to get an obtuse angle. Let us consider some cases of $\theta$. 

\begin{itemize}
\item If $\theta=\frac{\pi}{2}$, then $R=\{\pm\alpha,\pm\beta\}$ is a root system. We draw the following diagram to indicate the root system: 
\item If $\theta=\frac{2\pi}{3}$, then if $R=\{\pm\alpha,\pm\beta,\pm(\alpha+\beta)\}$ is a root system provided that $\|\alpha\|=\|\beta\|$. 
\end{itemize}

It is not true that any choice of $\theta$ gives a root system, which we explain through the following. 

\begin{lmm}{}{} Let $E$ be a finite dimensional inner product space over $\R$. Let $R$ be a root system of $E$. Let $\alpha,\beta\in R$. Then the following are true. 
\begin{itemize}
\item The value of $\langle\alpha,\beta\rangle\langle\beta,\alpha\rangle$ must lie in the set $\{0,1,2,3\}$
\item The angle between $\alpha$ and $\beta$ must lie in the set $$\left\{\frac{\pi}{2},\frac{\pi}{3},\frac{2\pi}{3},\frac{\pi}{4},\frac{3\pi}{4},\frac{\pi}{6},\frac{5\pi}{6}\right\}$$
\end{itemize}
\end{lmm}

We can draw a table of the possible values of $\langle\alpha,\beta\rangle$ and the corresponding angle between $\alpha$ and $\beta$: 

\begin{table}[!h]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
$\langle\alpha,\beta\rangle\langle\beta,\alpha\rangle$ & $\langle\alpha,\beta\rangle$ & $\langle\beta,\alpha\rangle$ & $\theta$         \\ \hline
$0$                                                    & $0$                          & $0$                          & $\frac{\pi}{2}$  \\[1.5ex] \hline
\multirow{2}{*}{$1$}                                   & $1$                          & $1$                          & $\frac{\pi}{3}$  \\[1.5ex] \cline{2-4} 
                                                       & $-1$                         & $-1$                         & $\frac{2\pi}{3}$ \\[1.5ex] \hline
\multirow{2}{*}{$2$}                                   & $1$                          & $2$                          & $\frac{\pi}{4}$  \\[1.5ex] \cline{2-4} 
                                                       & $-1$                         & $-2$                         & $\frac{3\pi}{4}$ \\[1.5ex] \hline
\multirow{2}{*}{$3$}                                   & $1$                          & $3$                          & $\frac{\pi}{6}$  \\[1.5ex] \cline{2-4} 
                                                       & $-1$                         & $-3$                         & $\frac{5\pi}{6}$ \\[1.5ex] \hline
\end{tabular}
\end{table}

Here we assume $\|\alpha\|\leq\|\beta\|$ so we exclude for instance the case $\langle\alpha,\beta\rangle=2$ and $\langle\beta,\alpha\rangle=1$. 

\begin{defn}{Irreducible Root Systems}{} Let $E$ be a finite dimensional inner product space over $\R$. Let $R$ be a root system of $E$. We say that $R$ is reducible if there exists two disjoint root systems $R_1$ and $R_2$ of $E$ such that $\langle r_1,r_2\rangle=0$ for $r_1\in R_1$ and $r_2\in R_2$. Otherwise, we say that $R$ is irreducible. 
\end{defn}

\begin{prp}{}{} Let $E$ be a finite dimensional inner product space over $\R$. Let $R$ be a root system of $E$. Then the following are true. 
\begin{itemize}
\item There is a decomposition $$R=R_1\amalg\cdots\amalg R_k$$ where each $R_i$ is an irreducible root system of $E_i=\R\langle R_i\rangle$
\item There is a decomposition $$E=E_1\oplus\cdots\oplus E_k$$ where $E_i$ is orthogonal to $E_j$ for $i\neq j$. 
\end{itemize}
\end{prp}

\begin{defn}{Isomorphic Root Systems}{} Let $E,F$ be a finite dimensional inner product spaces over $\R$. Let $R$ and $S$ be root systems of $E$ and $F$ respectively. We say that $E$ and $F$ are isomorphic if there exists an isomorphism $$\phi:E\to F$$ of vector spaces such that the following are true. 
\begin{itemize}
\item $\phi(R)=S$
\item For all $a,b\in R$, $\langle\phi(a),\phi(b)\rangle=\langle a,b\rangle$. 
\end{itemize}
\end{defn}

\subsection{Bases for Root Systems}
\begin{defn}{Bases for Root Systems}{} Let $E$ be a finite dimensional inner product space over $\R$. Let $R$ be a root system of $E$. Let $B\subseteq R$. We say that $B$ is a base of $R$ if the following are true. 
\begin{itemize}
\item $B$ is a basis for $E$
\item For any $r\in R$, there is a decomposition $$r=\sum_{b\in B}k_b\cdot b$$ where $k_b\in\Z$ and either all $k_b$ are positive or negative. 
\end{itemize}
\end{defn}

\begin{prp}{}{} Let $E$ be a finite dimensional inner product space over $\R$. Let $R$ be a root system of $E$. Then $R$ admits a base. 
\end{prp}

\subsection{The Weyl Group of a Root System}
Recall that $\sigma_a(b)=b-2\frac{\langle a,b\rangle}{\langle a,a\rangle}a$ for any $a,b$ in a root system $R$ of a finite dimensional inner product space $E$. In particular, they are elements of the general linear group $GL(E)$. 

\begin{defn}{The Weyl Group of a Root System}{} Let $E$ be a finite dimensional inner product space over $\R$. Let $R$ be a root system of $E$. Define the Weyl group of $R$ to be the subgroup $$W(R)=\langle\sigma_a\;|\;a\in R\rangle\leq GL(E)$$
\end{defn}

\subsection{Dynkin Diagrams}
\begin{defn}{The Dynkin Diagram of a Root System}{} Let $E$ be a finite dimensional inner product space over $\R$. Let $R$ be a root system of $E$. Let $B$ be a base of $R$. Define a graph $\Delta(R)$ as follows. 
\begin{itemize}
\item There is one vertex $v_b$ for each $b\in B$
\item For any two vertices $v_a$ and $v_b$, there are $d_{a,b}=\langle a,b\rangle\langle b,a\rangle$ number of undirected edges between $v_a$ and $v_b$. 
\end{itemize}
\end{defn}

\begin{prp}{}{} Let $E,F$ be a finite dimensional inner product spaces over $\R$. Let $R$ and $S$ be root systems of $E$ and $F$ respectively. Then $R\cong S$ if and only if $\Delta(R)\cong\Delta(S)$. 
\end{prp}

\begin{prp}{}{} Let $E$ be a finite dimensional inner product space. Let $R$ be a root system of $E$. Then $R$ is irreducible if and only if $\Delta(R)$ is a connected graph. 
\end{prp}

\pagebreak
\section{Classification of Semisimple Lie Algebras over $\C$}
\subsection{Weights of Lie Sub-Algebras of GL(V)}
\begin{defn}{Eigenvectors of GL(V)}{} Let $V$ be a vector space over a field $k$. Let $M$ be a Lie subalgebra of $GL(V)$. We say that $v\in V$ is an eigenvector of $M$ if for all $T\in M$, $v$ is an eigenvector of $T$ in the sense of Linear Algebra. 
\end{defn}

This notion of eigenvectors for Lie algebras is different to the standard notion of eigenvectors in linear algebra. Notice that an eigenvector of a Lie algebra $M$ is a vector $v\in V$ that is simultaneously an eigenvector of all linear maps $T\in M\leq GL(V)$. Now we can rephrase this in another way. 

\begin{lmm}{}{} Let $V$ be a vector space over a field $k$. Let $M$ be a Lie subalgebra of $GL(V)$. Then $v\in V$ is an eigenvector of $M$ if and only if there exists a linear map $\lambda:M\to k$ such that $$T(v)=\lambda(T)v$$ for all $T\in M$. 
\end{lmm}

We use the existence of a linear map $\lambda:M\to k$, conditional on $v\in V$, to determine whether $v$ is an eigenvector of $M$. 

\begin{defn}{Subspace of Eigenvectors of GL(V)}{} Let $V$ be a vector space over a field $k$. Let $M$ be a Lie subalgebra of $GL(V)$. Define the subspace of eigenvectors of $M$ by $$V_\lambda=\{v\in V\;|\;T(v)=\lambda(T)v\text{ for all }T\in M\}$$
\end{defn}

\begin{defn}{Weights}{} Let $V$ be a vector space over a field $k$. Let $M$ be a Lie subalgebra of $GL(V)$. We say that a linear map $\lambda:M\to k$ is a weight of $M$ if $V_\lambda\neq 0$. 
\end{defn}

\begin{lmm}{}{} Let $V$ be a vector space over a field $k$. Let $M$ be a Lie subalgebra of $GL(V)$. Let $I$ be an ideal of $M$. Let $W=\{w\in V\;|\;T(w)=0\text{ for all }T\in I\}$. Then $W$ is an $M$-invariant subspace of $V$. 
\end{lmm}

\subsection{Roots Systems Arising from Cartan Subalgebras}
Let $V$ be a vector space. We denote the dual space of $V$ by $V^\ast$. \\

Let $L$ be semisimple over $\C$. Let $H$ be a Cartan subalgebra. By lmm 5.1.2, we can choose a basis $v_1,\dots,v_m$ of $GL(V)$ such that $\text{ad}(h)$ is diagonal for all $h\in H$. In particular, such a basis consists of common eigenvectors of $\text{ad}(h)$. Fix such a common eigenvector $v$ and write its eigenvalue by $\alpha(\text{ad}(h))$ (dependent on $h$). Then we have $$\text{ad}(h)(v)=\alpha(\text{ad}(h))(v)$$ and in particular $\alpha:\text{ad}(H)\to\C$ is a weight of $\text{ad}(H)$. Since $L$ is semisimple, we have an isomorphism $\text{ad}(H)\cong H$. So we can think of the weight as an element $$\alpha:H\cong\text{ad}(H)\to\C$$ of the dual space $H^\ast$. This motivates the following definition. 

\begin{defn}{Roots of Lie Algebra with respect to a Cartan Subalgebra}{} Let $L$ be a semi-simple Lie algebra over $\C$. Let $H\leq L$ be a Cartan subalgebra of $L$. A root of $L$ relative to $H$ is an element $\alpha\in H^\ast$ such that $\alpha\neq 0$ and the weight space $$L_\alpha=\{x\in L\;|\;[h,x]=\alpha(h)(x)\text{ for all }h\in H\}$$ is non-zero. In this case, we call $L_\alpha$ the root space of $\alpha$. 
\end{defn}

\begin{defn}{The Set of Roots relative to a Cartan Subalgebra}{} Let $L$ be a semi-simple Lie algebra over $\C$. Let $H\leq L$ be a Cartan subalgebra of $L$. Define the set of roots of $L$ relative to $H$ to be $$\Psi=\{\alpha\in H^\ast\;|\;\alpha\text{ is a root of }L\text{ with respect to }H\}$$
\end{defn}

Note that $\Psi$ is finite since we assume that $L$ is finite dimensional. 

\begin{lmm}{}{} Let $L$ be a semisimple Lie algebra over $\C$. Then there is a decomposition of direct sums $$L=L_0\oplus\bigoplus_{\alpha\in\Psi}L_\alpha$$
\end{lmm}

\begin{prp}{}{} Let $L$ be a semisimple Lie algebra over $\C$. Let $H\leq L$ be a Lie subalgebra of $L$. Then the following are true. 
\begin{itemize}
\item $[L_\alpha,L_\beta]\subseteq L_{\alpha+\beta}$
\item If $\alpha+\beta\neq 0$, then $k(L_\alpha,L_\beta)=0$ where $k$ is the killing form of $L$. 
\item $L_0\cap L_0^\perp=\{0\}$. In particular, $k|_{L_0}$ is non-degenerate. 
\end{itemize}
\end{prp}

\begin{prp}{}{} Let $L$ be a semisimple Lie algebra over $\C$. Let $H\leq L$ be a Lie subalgebra of $L$. Then $$H=C_L(H)$$
\end{prp}

\begin{prp}{}{} Let $L$ be a semi-simple Lie algebra over $\C$. Let $H$ be a Cartan subalgebra of $L$. Let $\Psi$ be the set of roots relative to $H$. Then $\Psi$ is a root system. 
\end{prp}

\begin{prp}{}{} Let $L$ be a semi-simple Lie algebra over $\C$. Let $H$ and $K$ be two Cartan subalgebras of $L$. Let $\Psi_H$ and $\Psi_K$ be the set of roots relative to $H$ and $K$ respectively. Then $\Psi_H\cong\Psi_K$. 
\end{prp}

This proves that although the set of roots come from a chosen Cartan subalgebra of the Lie algebra $L$, it is an invariant of $L$ and not the Cartan subalgebra. Up until this point, we have constructed an invariant for Lie algebras following the below schematic: \\~\\
\adjustbox{scale=1.0,center}{\begin{tikzcd}
	&&& \begin{array}{c} \substack{\text{Associated}\\\text{Weyl Group}} \end{array} \\
	\begin{array}{c} \substack{\text{Semi-Simple}\\\text{Lie Algebras}} \end{array} & \begin{array}{c} \substack{\text{Set of Cartan}\\\text{Sub-Algebras}} \end{array} & \begin{array}{c} \substack{\text{Associated}\\\text{Root System}} \end{array} \\
	&&& \begin{array}{c} \substack{\text{Dynkin}\\\text{Diagram}} \end{array}
	\arrow[from=2-1, to=2-2]
	\arrow[from=2-2, to=2-3]
	\arrow[from=2-3, to=1-4]
	\arrow[from=2-3, to=3-4]
\end{tikzcd}} \\

We will now show that Simple Lie algebras give rise to irreducible Dynkin diagrams and vice versa. 

\subsection{Simple Lie Algebras and Irreducible Dynkin Diagrams}
We now want to enrich the above diagram: \\~\\
\adjustbox{scale=1.0,center}{\begin{tikzcd}
	&&& \begin{array}{c} \substack{\text{Associated}\\\text{Weyl Group}} \end{array} \\
	\begin{array}{c} \substack{\text{Semi-Simple}\\\text{Lie Algebras}} \end{array} & \begin{array}{c} \substack{\text{Set of Cartan}\\\text{Sub-Algebras}} \end{array} & \begin{array}{c} \substack{\text{Associated}\\\text{Root System}} \end{array} \\
	&&& \begin{array}{c} \substack{\text{Dynkin}\\\text{Diagram}} \end{array} \\
	\begin{array}{c} \substack{\text{Simple}\\\text{Lie Algebras}} \end{array} && \begin{array}{c} \substack{\text{Irreducible}\\\text{Root System}} \end{array} & \begin{array}{c} \substack{\text{Connected}\\\text{Dynkin Diagram}} \end{array}
	\arrow[from=2-1, to=2-2]
	\arrow[from=2-2, to=2-3]
	\arrow[from=2-3, to=1-4]
	\arrow[from=2-3, to=3-4]
	\arrow[from=4-1, to=4-3]
	\arrow[from=4-3, to=4-4]
\end{tikzcd}} \\

\begin{prp}{}{} Let $L$ be a simple Lie algebra over $\C$. Let $H$ be a Cartan sub-algebra of $L$. Let $\Psi$ be the root system associated to $H$. Then $\Psi$ is irreducible. 
\end{prp}

\begin{thm}{Serre's Theorem}{} Let $R$ be a root system with base $B=\{\alpha_1,\dots,\alpha_l\}$. Let $L$ be the Lie algebra defined by $$L=\C\langle e_i,f_i,h_i, 1\leq i\leq l\;|\;S\rangle$$ where $S$ are the following relations: 
\begin{itemize}
\item $[h_s,h_t]=0$ for $1\leq s,t\leq l$. 
\item $[h_s,e_t]=\langle\alpha_t,\alpha_s\rangle e_t$ for $1\leq s,t\leq l$. 
\item $[h_s,f_t]=-\langle\alpha_t,\alpha_s\rangle f_t$ for $1\leq s,t\leq l$. 
\item $[e_s,f_t]=\begin{cases}
h_s & \text{ if } s=t\\
0 & \text{otherwise}
\end{cases}$
\item $\left(\text{ad}(e_s)\right)^{1-\langle\alpha_t,\alpha_s\rangle}(e_t)=0=\left(\text{ad}(f_s)\right)^{1-\langle\alpha_t,\alpha_s\rangle}(f_t)$ for $1\leq s,t\leq l$. 
\end{itemize}
Then the following are true. 
\begin{itemize}
\item $L$ is finite dimensional and semi-simnple. 
\item $H=\C\langle h_1,\dots,h_l\rangle$ is a Cartan sub-algebra of $L$. 
\item The root system $\Psi_H$ of $H$ is equal to $R$. 
\end{itemize}
\end{thm}







\end{document}
