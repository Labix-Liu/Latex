\documentclass[a4paper]{article}

\input{C:/Users/liula/Desktop/Latex/Headers V1.2.tex}

\pagestyle{fancy}
\fancyhf{}
\rhead{Labix}
\lhead{Probability Theory}
\rfoot{\thepage}


\title{Probability Theory}

\author{Labix}

\date{\today}
\begin{document}
\maketitle
\begin{abstract}
Notes for the basics of Probability Theory. 
\end{abstract}
\tableofcontents
\pagebreak

\section{Foundations of Probability Theory}
\subsection{Definition of Probability}
\begin{defn}{Probability Space}{} A probability space is a measure space $(\Omega,\mF,P)$ where the measure $P$ lands in $[0,1]$. 
\end{defn}

Explicitly, a probability space is a triple $(\Omega,\mF,P)$ consisting of the following data: 
\begin{itemize}
\item $\Omega\neq\emptyset$ is a set called the sample space. 
\item $\mF\subseteq\mP(\Omega)$ is a $\sigma$-algebra called events.  \item $P:\mF\to[0,1]$ is a set function. 
\end{itemize}
such that the following are true: 
\begin{itemize}
\item $P(\Omega)=1$. 
\item If $\{A_n\;|\;n\in\N\}\subseteq\mF$ are pairwise disjoint, then $$P\left(\bigcup_{k=1}^\infty A_k\right)=\sum_{k=1}^{\infty}P(A_k)$$
\end{itemize}

\begin{prp}{}{} Let $(\Omega,\mF,P)$ be a probability space. Let $A,B\in\mF$ be events. Then the following are true. 
\begin{itemize}
\item $P(\Omega\setminus A)=1-P(A)$
\item $A\subset B\implies P(A)\leq P(B)$
\end{itemize} \tcbline
\begin{proof} Let $A\subset B\subset\Omega$ be events in $\Omega$. 
\begin{itemize}
\item $A$ and $\Omega\setminus A$ are disjoint and $P(\Omega)=P(A)+P(\Omega\setminus A)$ and $P(\Omega\setminus A)=1-P(A)$
\item We have that $A$ and $B\setminus A$ are disjoint. Thus $P(B)=P(A)+P(B\setminus A)$. Since $P(B\setminus A)\geq 0$, we have $P(A)\leq P(B)$.  
\end{itemize}
\end{proof}
\end{prp}

\begin{defn}{Uniform Probability Measure}{} Let $\Omega$ be a sample space. A probability measure $P$ is uniform if fo all $a,b\in\Omega$, $$P(\{a\})=P(\{b\})$$
\end{defn}

\begin{thm}{}{} Let $\Omega$ be a sample space and $P$ a uniform probability measure of $\Omega$. Then for all $A\subset\Omega$, $$P(A)=\frac{\abs{A}}{\abs{\Omega}}$$
\end{thm}
\begin{proof} Suppose that $A$ consists of $\abs{A}$ distinct elements and the event space $\abs{\Omega}$ contains $\abs{\Omega}$ distinct elements. Since every singleton set is pairwise disjoint, we have $P(A)=\abs{A}P(\{a\})$ for any $a\in A$. Similarly, we have $P(\Omega)=\abs{\Omega}P(\{a\})$.  Thus we have that $P(A)=\frac{\abs{A}P(\Omega)}{\abs{\Omega}}$ and $P(A)=\frac{\abs{A}}{\abs{\Omega}}$
\end{proof}

\begin{thm}{Principle of Inclusion Exclusion}{} Let $A,B\subset\Omega$ be a sample space and $P$ the probability measure. $$P(A\cup B)=P(A)+P(B)-P(A\cap B)$$
\end{thm}
\begin{proof} Note that 
\begin{align*}
A\cup(B\setminus A)&=A\cup(B\cap A^c)\\
&=(A\cup B)\cap(A\cup A^c)\\
&=A\cup B
\end{align*} Note also that $A\cap (B\setminus A)=\emptyset$. Thus $P(A\cup B)=P(A)+P(B\setminus A)=P(A)+P(B)-P(A\cap B)$
\end{proof}

\begin{thm}{Extended Principle of Inclusion Exclusion}{} Let $A_k\subset\Omega$ be a sample space and $P$ the probability measure for all $k\leq n\in\mathbb{N}$. Then $$P\left(\bigcup_{k=1}^n A_k\right)=\sum_{k=1}^n(-1)^{k+1}\sum_{1\leq i_1\leq\dots\leq n}P(A_{i_1}\cap A_{i_2}\cap\dots\cap A_{i_k})$$
\end{thm}

\subsection{Multiplication Principle}
\begin{thm}{The Multiplication Principle}{} Suppose that Experiment A has $a$ outcomes and Experiment B has $b$ outcomes. Then the performing both $A$ and $B$ results in $ab$ possible outcomes. 
\end{thm}

\begin{thm}{Sampling with replacement - Ordered}{} In the case of sampling $k$ balls with replacement from an urn containing $n$ balls, there are $\abs{\Omega}=n^k$ possible outcomes when the order of the objects matters, where $\Omega=\{(s_1,\dots,s_k):s_i\in\{1,\dots,n\}\forall i\in\{1,\dots,k\}\}$. 
\end{thm}

\begin{thm}{Sampling without replacement - Ordered}{} In the case of sampling $k$ balls without replacement from an urn containing $n$ balls, there are $\abs{\Omega}=\frac{n!}{(n-k)!}$ possible outcomes when the order of the objects matters, where $\Omega=\{(s_1,\dots,s_k):s_i\in\{1,\dots,n\}\forall i\in\{1,\dots,k\},i\neq j\implies s_i\neq s_j\}$. 
\end{thm}

\begin{thm}{Sampling without replacement - Unordered}{} In the case of sampling $k$ balls without replacement from an urn containing $n$ balls, there are $\abs{\Omega}=\binom{n}{k}$ possible outcomes when the order of the objects does not matter, where $\Omega=\{\omega\subset\{1,\dots,n\}:\abs{\omega}=k\}$. 
\end{thm}

\begin{thm}{Sampling with replacement - Unordered}{} In the case of sampling $k$ balls with replacement from an urn containing $n$ balls, there are $\abs{\Omega}=\binom{n+k-1}{k}$ possible outcomes when the order of the objects does not matter, where $\Omega=\{\omega\subset\{1,\dots,n\}:\omega\text{ is a $k$ element multiset with elements from }\{1,\dots,n\}\}$. 
\end{thm}

\subsection{Conditional Probability}
\begin{defn}{Conditional Probability}{} Consider a probability space $(\Omega,P)$. Let $A,B\subset\Omega$ with $P(B)>0$. Then the conditional probability of $A$ given $B$, denoted by $P(A|B)$ is defined as $$P(A|B)=\frac{P(A\cap B)}{P(B)}$$
\end{defn}

\begin{thm}{Multiplication Rule}{} Let $n\in\mathbb{N}$. Then for any events $A_1,\dots,A_n$ such that $P(A_2\cap\dots\cap A_n)>0$, we have $$P(A_1\cap\dots\cap A_n)=P(A_1)P(A_2|A_1)P(A_3|A_1\cap A_2)\dots P(A_n|A_1\cap\dots\cap A_{n-1})$$
\end{thm}
\begin{proof} From the right hand side, we have
\begin{align*}
&P(A_1)P(A_2|A_1)P(A_3|A_1\cap A_2)\dots P(A_n|A_1\cap\dots\cap A_{n-1})\\
&=P(A_1)\frac{P(A_2\cap A_1)}{P(A_1)}\frac{P(A_3\cap A_2\cap A_1)}{P(A_2\cap A_1)}\dots \frac{P(A_n\cap\dots\cap A_1)}{P(A_1\cap\dots\cap A_{n-1})}\\
&=P(A_1\cap\dots\cap A_n)
\end{align*}
\end{proof}

\begin{thm}{Bayes' Rule}{} Let $(\Omega,P)$ be a probability measure. Let $A,B\subset\Omega$ with $P(A),P(B)>0$. Then $$P(A|B)=\frac{P(B|A)P(A)}{P(B)}$$
\end{thm}
\begin{proof} We have that $P(A\cap B)=P(A|B)P(B)$ and $P(A\cap B)=P(B|A)P(A)$. 
\end{proof}

\begin{thm}{Law of Total Probability}{} Let $(\Omega,P)$ be a probability measure. Let $A_1,\dots,A_n$ be a partition of $\Omega$ with $P(A_i)>0$ for all $i=1,\dots,n$. Then for any $B\subset\Omega$, $$P(B)=\sum_{k=1}^nP(A_k)P(B|A_k)$$
\end{thm}
\begin{proof} Note that since $A_1,\dots,A_n$ is a partition, $B\cap A_1,\dots,B\cap A_n$ is also a parition. 
\begin{align*}
\sum_{k=1}^nP(A_k)P(B|A_k)&=\sum_{k=1}^nP(B\cap A_k)\\
&=P\left(\bigcup_{k=1}^nB\cap A_k\right)\\
&=P(B\cap\Omega)\\
&=P(B)
\end{align*}
\end{proof}

\begin{thm}{General Bayes' Rule}{} Let $(\Omega,P)$ be a probability measure. Let $A_1,\dots,A_n$ be a partition of $\Omega$ with $P(A_i)>0$ for all $i=1,\dots,n$. Then for any $B\subset\Omega$ with $P(B)>0$, $$P(A_i|B)=\frac{P(B|A_i)P(A_i)}{P(B)}=\frac{P(B|A_i)P(A_i)}{\sum_{k=1}^nP(B|A_i)P(A_i)}$$
\end{thm}
\begin{proof} Apply Bayes' rule and apply the mulitplication rule. 
\end{proof}

\subsection{Independence of Events}
\begin{defn}{Independent Events}{} Two events $A,B$ are said to be independent if $$P(A\cap B)=P(A)P(B)$$
\end{defn}

\begin{prp}{}{} If $A,B$ are independent, then $A^c,B$, $A,B^c$ and $A^c,B^c$ are independent. 
\end{prp}
\begin{proof} We only proof the first and third item. 
\begin{itemize}
\item Without loss of generality we prove the first and reader mirrors the second. 
\begin{align*}
P(A^c\cap B)&=P(B)-P(A\cap B)\\
&=P(B)(1-P(A))\\
&=P(B)P(A^c)
\end{align*}
\item Note that $P(A\cap B)=P(A)P(B)$
\begin{align*}
P(A^c\cap B^c)&=1-P(A\cap B)\\
&=1-P(A)-P(B)+P(A\cap B)\\
&=1-P(A)-P(B)+P(A)P(B)\\
&=(1-P(A))(1-P(B))\\
&=P(A^c)P(B^c)
\end{align*}
\end{itemize}
\end{proof}

\pagebreak
\section{Probability Distributions}
\subsection{Random Variables and its Distribution}
\begin{defn}{Random Variable}{} Let $(\Omega,\mathcal{F},P)$ be a probability space. Let $(E,\mE)$ be a measurable space. An $(E,\mathcal{E})$ valued random variable is an $\mF$-measurable function $X:\Omega\to E$. 
\end{defn}

\begin{defn}{Independent Random Variables}{} Let $(\Omega,\mF,P)$ be a probability space. Let $(E,\mE)$ be a measurable space. Let $X,Y:\Omega\to E$ be random variables. We say that $X$ and $Y$ are independent if for any $A,B\in\mE$, we have that $X^{-1}(A)$ and $Y^{-1}(B)$ are independent events in $\mF$. 
\end{defn}

\begin{defn}{Discrete and Continuous Random Variables}{} Let $(\Omega,\mF,P)$ be a probability space. Let $X:\Omega\to\R$ be a random variable. 
\begin{itemize}
\item We say that $X$ is discrete if $\im(X)$ is a countable subset of $\R$. 
\item We say that $X$ is continuous otherwise. 
\end{itemize}
\end{defn}

Recall that $X$ is an $\mF$-measurable function if $X^{-1}(B)\in\mF$ for $B\in\mE$. 

\begin{defn}{Probability Distribution}{} Let $(\Omega,E,\Prj)$ be a probability space. Let $(E,\mE)$ be a measurable space. Let $X:\Omega\to E$ be a measurable function. Define the probability distribution of $X$ to be the pushforward measure $P\circ X^{-1}=P_X:\mE\to[0,1]$ defined by $$P_X(A)=P(X^{-1}(A))$$ for $A\in\mE$. 
\end{defn}

\begin{defn}{Probability Density Function}{} Let $(\Omega,\mF,P)$ be a probability space. Let $X:\Omega\to\R$ be a random variable. Define the probability density function of $X$ to be the Radon–Nikodym derivative $$f_X=\frac{d X_\ast P}{d\mu}$$ where $\mu$ is the Lebesgue measure. 
\end{defn}

Recall that this means that $f_X$ satisfies the property that $$P_X(A)=\int_A f_X d\mu$$ for any measurable set $A\subseteq\R$. In particular, if $A=\{a\}\subseteq\mF$, then we have $$P_X(a)=f_X(a)$$ The probability distribution function has its input as every measurable subset of $\R$, while the probability density function takes input as individual points of $\R$. They are really the same thing because having its probability be determined on singletons is sufficient to determine the probability of every measurable subset. 

\begin{prp}{}{} Let $(\Omega,\mF,P)$ be a probability space. Let $X:\Omega\to\R$ be a discrete random variable. Let $g:\R\to\R$ be a function. Then the probability density function of $Y=g\circ X$ is given by $$f_Y(y)=\sum_{x\in g^{-1}(y)}f_X(x)$$
\end{prp}

\begin{prp}{}{} Suppose that $X$ is a continuous random variable with density $f_X$ and $g:\mathbb{R}\to\mathbb{R}$ is strictly monotone and differentiable with inverse function denoted $g^{-1}$, then $Y=g(X)$ has density $$f_Y(y)=f_X(g^{-1}(y))\abs{\frac{d}{dy}(g^{-1}(y))}$$ for all $y\in\mathbb{R}$
\end{prp}

\begin{eg}{Bernoulli Distribution}{} Let $(\Omega,\mF,P)$ be a probability space. Let $X:\Omega\to\R$ be a random variable. We say that $X$ has a Bernoulli distribution if the probability density function of $X$ is given by $$f_X(x)=\begin{cases}
p & \text{ if }x=1\\
1-p & \text{ if }x=0\\
0 & \text{otherwise}
\end{cases}$$ for some $p\in [0,1]$. 
\end{eg}

\begin{eg}{Binomial Distribution}{} Let $(\Omega,\mF,P)$ be a probability space. Let $X:\Omega\to\R$ be a random variable. We say that $X$ has a binomial distribution if the probability density function of $X$ is given by $$f_X(x)=\binom{n}{x}p^x(1-p)^{n-x}$$ for some $p\in[0,1]$. 
\end{eg}

\begin{defn}{Poisson Distribution}{} A discrete random variable $X$ is said to have Poisson Distribution with parameter $\lambda>0$ if $\im(X)=\mathbb{N}_0$ and $$p_X(x)=\frac{\lambda^x}{x!}e^{-x}$$
\end{defn}

\begin{defn}{Geometric Distribution}{} A discrete random variable $X$ is said to have Geometric Distribution with parameter $p\in(0,1)$ if $\im(X)=\mathbb{N}_0$ and $$p_X(x)=p(1-p)^{x-1}$$
\end{defn}

Let $I\subseteq\R$ be an interval. Recall that $\mB(I)$ refers to the borel measurable subsets of $I$. Denote $\lambda$ the Lebesgue measure on $\R^n$. 

\begin{eg}{Uniform Distribution}{} Let $[a,b]\subseteq\R$ be an interval. Let $X$ be a random variable on the probability space $([a,b],\mB([a,b]),P)$. We say that $X$ has a uniform distribution if its probability density function is given by $$f_X(A)=\frac{\lambda(A)}{b-a}$$ for $A\subseteq[a,b]$.
\end{eg}

In particular, when $A=\{c\}\subseteq[a,b]$ is the one-point set, we have $P_X(c)=\frac{1}{b-a}$ so that the probability of any one point set is uniform. 

\begin{eg}{}{} Let $X$ be a uniform distribution on $[a,b]$. Then the probability density function of $X$ is given by $$F_X(x)=\frac{1}{b-a}$$
\end{eg}

\begin{eg}{Normal Distribution}{} Let $X$ be a random variable on the probability space $(\R,\mB(\R),P)$. We say that $X$ has a normal distribution if its probability density function is given by $$f_X(x)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$ for some $\mu\in\R$ and $\sigma>0$. 
\end{eg}

\begin{defn}{Exponential Distribution}{} A conitnuous random variable $X$ is said to have Exponential Distribution with parameter $\lambda>0$ if its density function is given by $$f_X(x)=\begin{cases}
\lambda e^{-\lambda x} & \text{if $x>0$}\\
0 & \text{otherwise}
\end{cases}$$ and its cumulative function given by $$F_X(x)=\begin{cases}
0 & \text{if $x\leq 0$}\\
1-e^{-\lambda x} & \text{if $x>0$}
\end{cases}$$
\end{defn}

\begin{defn}{Gamma Distribution}{} A conitnuous random variable $X$ is said to have Gamma Distribution with shape parameter $\alpha>0$ and rate parameter $\beta>0$ if its density function is given by $$f_X(x)=\begin{cases}
\frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x} & \text{if $x>0$}\\
0 & \text{otherwise}
\end{cases}$$
\end{defn}

\subsection{Cumulative Density Functions}
\begin{defn}{Cummulative Distribution Function}{} Let $(\Omega,\mF,P)$ be a probability space. Let $X:\Omega\to\R$ be a random variable. Define the cummulative distribution function $F_X:\R\to\R$ of $X$ to be $$F_X(x)=P_X(X\leq x)$$
\end{defn}

\begin{prp}{}{} Let $(\Omega,\mF,P)$ be a probability space. Let $X:\Omega\to\R$ be a random variable. Then the following are true. 
\begin{itemize}
\item $f_X=\frac{dF_X}{dx}$. 
\item $F_X(x)=\int_{-\infty}^xf_X(t)\;dt$
\end{itemize}
\end{prp}

\begin{prp}{}{} Let $(\Omega,\mF,P)$ be a probability space. Let $X:\Omega\to\R$ be a random variable. Then the following are true regarding the cumulative distribution function $F_X$. 
\begin{itemize}
\item $F_X$ is monotonically increasing: $x\leq y\implies F_X(x)\leq F_X(y)$
\item $F_x$ is right continuous: If $(x_n)$ is a sequence such that $x_1\geq\dots\geq x_n\geq x_{n+1}\geq\dots\geq x$ and $(x_n)\to x$, then $F_X(x_n)\to F_X(x)$
\item $F_X(-\infty)=0$ and $F_X(\infty)=1$
\end{itemize}
\end{prp}

\begin{prp}{}{} Suppose that $X$ is a random variable on a probability space $(\Omega,E,\Prj)$ with cumulative distribution function $F_X$. If $a<b$, then $\Prj(a<X\leq b)=F_X(b)-F_X(a)$
\end{prp}

\subsection{Multivariate Random Variables}
Let $(\Omega,E,\Prj)$ be a probability space. The definition of random variables and probability distribution is well-adapted to the case when the random variable $X$ lands in $\R^n$. In this case, we may find the relationship between the probability density function of $X$ and the probability density function of its individual components. 






\begin{defn}{Joint Probability Mass Function}{} Let $X,Y$ be discrete random variables. The joint probability mass function of $X$ and $Y$ is the function $$p_{X,Y}(x,y)=P(\{\omega\in\Omega:X(\omega)=x,Y(\omega)=y\})=P((X,Y)=(x,y))$$ for all $(x,y)\in\mathbb{R}^2$
\end{defn}

\begin{thm}{}{} Let $p_{X,Y}$ be the joint probability mass function of two random variables $X,Y$. 
\begin{itemize}
\item $p_X(x)=\sum_{y}p_{X,Y}(x,y)$
\item $p_Y(y)=\sum_{x}p_{X,Y}(x,y)$
\end{itemize}
\end{thm}

\begin{defn}{Joint Cumulative Distribution Function}{} Let $X,Y$ be random variables. The joint cumulative distribution function of $X$ and $Y$ is the function $$F_{X,Y}(x,y)=P(\{\omega\in\Omega:X(\omega)\leq x,Y(\omega)\leq y\})=P(X\leq x,Y\leq y)$$ for all $(x,y)\in\mathbb{R}^2$
\end{defn}

\begin{thm}{}{} Let $F_{X,Y}$ be the joint cumulative distribution function of two random variables $X,Y$. 
\begin{itemize}
\item $\lim_{x,y\to-\infty}F_{X,Y}(x,y)=0$
\item $\lim_{x,y\to\infty}F_{X,Y}(x,y)=1$
\item $x\leq x'$ and $y\leq y'$ implies $F_{X,Y}(x,y)\leq F_{X,Y}(x',y')$
\item $F_X(x)=\lim_{y\to\infty}F_{X,Y}(x,y)$
\item $F_Y(x)=\lim_{x\to\infty}F_{X,Y}(x,y)$
\end{itemize}
\end{thm}

\begin{defn}{Jointly Continuous}{} Let $X,Y$ be random variables. $X$ and $Y$ are jointly continuous if $$F_{X,Y}(x,y)=\int_{-\infty}^{x}\int_{-\infty}^{y}f_{X,Y}(u,v)\,dv\,du$$ for a function $f_{X,Y}:\mathbb{R}^2\to\mathbb{R}^2$ satisfying
\begin{itemize}
\item $f_{X,Y}(u,v)\geq 0$
\item $\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f_{X,Y}(u,v)\,dv\,du=1$
\end{itemize}
We call $f_{X,Y}$ the joint density function of $(X,Y)$. 
\end{defn}

\begin{thm}{}{} Let $F_{X,Y}$ be the joint cumulative distribution function of two random variables $X,Y$. 
\begin{itemize}
\item $f_{X,Y}(x,y)=\begin{cases}
\frac{\partial^2}{\partial x\partial y}F_{X,Y}(x,y)&\text{if the derivative exists at $(x,y)$}\\
0&\text{otherwise}
\end{cases}$
\item $f_X(x)=\int_{-\infty}^{\infty}f_{X,Y}(x,y)\,dy$
\item $f_Y(x)=\int_{-\infty}^{\infty}f_{X,Y}(x,y)\,dx$
\end{itemize}
\end{thm}

\begin{prp} Let $(\Omega,E,\Prj)$ be a probability space. Let $(E,\mE)$ be a measurable space. Let $X,Y:\Omega\to\R$ be a random variables. Then the following are equivalent. 
\begin{itemize}
\item $X$ and $Y$ are independent. 
\item $f_{(X,Y)}=f_Xf_Y$. 
\item $F_{(X,Y)}=F_XF_Y$
\end{itemize}
\end{prp}

\subsection{Algebra of Random Variables}
\begin{prp}{}{} Let $(\Omega,E,\Prj)$ be a probability space. Let $X,Y:\Omega\to\R$ be a random variables. Then we have $$f_{X+Y}(z)=\int_{-\infty}^\infty f_{(X,Y)}(t,z-t)\;dt$$
\end{prp}


\begin{prp}{}{} Let $X\approx$Poi$(\lambda)$ and $Y\approx$Poi$(\mu)$ be independent. $X+Y\approx$Poi$(\lambda+\mu)$. 
\end{prp}
\begin{proof} 
\begin{align*}
p_{X+Y}(m)&=\sum_{k\in\Z}\frac{\lambda^k}{k!}e^{-k}\frac{\mu^{m-k}}{(m-k)!}e^{k-m}\\
&=\frac{1}{m!}e^{-m}\sum_{k=0}^mm!\frac{\lambda^k}{k!}\frac{\mu^{m-k}}{(m-k)!}\\
&=\frac{1}{m!}e^{-m}\sum_{k=0}^m\binom{m}{k}\lambda^k\mu^{m-k}\\
&=\frac{(\lambda+\mu)^m}{m!}e^{-m}
\end{align*}
\end{proof}

\begin{prp}{}{} Let $X_1,\dots,X_n\approx$Bern$(p)$ be independent. $X_1+\dots+X_n\approx$Bin$(n,p)$. 
\end{prp}
\begin{proof} We prove by induction. When $n=2$,
\begin{align*}
p_{X_1+X_2}(0)&=p_{X_1}(0)p_{X_2}(0)\\
&=1-2p+p^2\\
p_{X_1+X_2}(1)&=p_{X_1}(0)p_{X_2}(1)+p_{X_1}(1)p_{X_2}(0)\\
&=(1-p)(p)+p(1-p)\\
&=2p(1-p)\\
p_{X_1+X_2}(2)&=p_{X_1}(0)p_{X_2}(2)+p_{X_1}(1)p_{X_2}(1)+p_{X_1}(2)p_{X_2}(0)\\
&=p^2\\
p_{\text{Bin}(2,p)}(x)&=\binom{2}{x}p^x(1-p)^{n-x}\\
\end{align*} For $x\in\{0,1,2\}$, the two probability density functions match thus for the case $n=2$, it is true. Now suppose that $X_1+\dots+X_{n-1}\approx$Bin$(n-1,p)$. Let $Y=$Bin$(n-1,p)+X_n$. For $m\in\{0,\dots,n\}$,
\begin{align*}
p_Y(m)&=\sum_{k\in\Z}p_{\text{Bin}(n-1,p)}(k)p_{X_n}(m-k)\\
&=\sum_{k=0}^mp_{\text{Bin}(n-1,p)}(k)p_{X_n}(m-k)\\
&=\sum_{k=0}^m\binom{n-1}{k}p^k(1-p)^{n-1-k}p_{X_n}(m-k)\\
&=\sum_{k=m-1}^m\binom{n-1}{k}p^k(1-p)^{n-1-k}p_{X_n}(m-k)\\
&=\binom{n-1}{m-1}p^{m-1}(1-p)^{n-m}p_{X_n}(1)+\binom{n-1}{m}p^m(1-p)^{n-1-m}p_{X_n}(0)\\
&=\binom{n-1}{m-1}p^{m}(1-p)^{n-m}+\binom{n-1}{m}p^m(1-p)^{n-m}\\
&=\binom{n}{m}p^{m}(1-p)^{n-m}
\end{align*} Thus for the case $X_1+\dots+X_n$ it is true. 
\end{proof}

\begin{prp}{}{} Let $X\approx$Bin$(m,p)$ and $Y\approx$Bin$(n,p)$ be independent. $X+Y\approx$Bin$(m+n,p)$. 
\end{prp}
\begin{proof}
\begin{align*}
p_{X+Y}(t)&=\sum_{k\in\Z}p_X(k)p_Y(t-k)\\
&=\sum_{k=0}^t\binom{m}{k}p^k(1-p)^{m-k}\binom{n}{t-k}p^{t-k}(1-p)^{n-t+k}\\
&=\sum_{k=0}^t\binom{m}{k}\binom{n}{t-k}p^t(1-p)^{m+n-t}\\
&=p^t(1-p)^{m+n-t}\sum_{k=0}^t\frac{m!}{k!(m-k)!}\frac{n!}{(t-k)!(n-t+k)!}\\
\end{align*}
\end{proof}

\begin{prp}{}{} Let $\lambda>0$. Let $n\in\mathbb{N}$. Let $T_1,\dots,T_n$ be independent random variables with exponential distribution parameter $\lambda$. Then $$Z=\sum_{k=1}^nT_k\approx\text{Gamma}(n,\lambda)$$
\end{prp}
\begin{proof} We prove by induction. When $n=2$, 
\begin{align*}
f_Z(z)&=\int_{-\infty}^\infty f_{T_1}(x)f_{T_2}(z-x)\,dx\\
&=\int_0^z\lambda e^{-\lambda x}\lambda e^{-\lambda(z-x)}\,dx\\
&=\lambda^2e^{-\lambda z}\int_0^z\,dx\\
&=\lambda^2ze^{-\lambda z}
\end{align*} Thus the case $n=2$ is true. Suppose that it is true for $n=k-1$. Let $X\approx$Gamma$(n-1,\lambda)$. 
\begin{align*}
f_Z(z)&=\int_{-\infty}^\infty f_X(x)f_{T_n}(z-x)\,dx\\
&=\int_0^z\frac{\lambda^{n-1}}{\Gamma(n-1)}x^{n-2}e^{-\lambda x}\lambda e^{-\lambda(z-x)}\,dx\\
&=\frac{\lambda^{n}}{\Gamma(n-1)}e^{-\lambda z}\int_0^zx^{n-2}\,dx\\
&=\frac{\lambda^{n}}{\Gamma(n-1)}e^{-\lambda z}\frac{1}{n-1}z^{n-1}\\
&=\frac{\lambda^{n}}{\Gamma(n)}z^{n-1}e^{-\lambda z}\\
\end{align*} Thus we are done
\end{proof}

\begin{prp}{}{} Let $m,n\in\mathbb{N}$ and $\lambda>0$. Let $X\approx$Gamma$(m,\lambda)$ and $Y\approx$Gamma$(n,\lambda)$ be independent. $X+Y\approx$Gamma$(m+n,\lambda)$. 
\end{prp}
\begin{proof}
\begin{align*}
f_Z(z)&=\int_{-\infty}^\infty f_X(x)f_Y(z-x)\,dx\\
&=\int_0^z\frac{\lambda^{m}}{\Gamma(m)}x^{m-1}e^{-\lambda x}\frac{\lambda^{n}}{\Gamma(n)}(z-x)^{n-1}e^{-\lambda(z-x)}\,dx\\
&=\frac{\lambda^{m+n}}{\Gamma(m)\Gamma(n)}e^{-\lambda z}\int_0^zx^{m-1}(z-x)^{n-1}\,dx\\
&=\frac{\lambda^{m+n}}{\Gamma(m)\Gamma(n)}e^{-\lambda z}\int_0^zx^{m-1}\sum_{k=0}^{n-1}\binom{n-1}{k}z^{n-1-k}(-x)^k\,dx\\
&=\frac{\lambda^{m+n}}{\Gamma(m)\Gamma(n)}e^{-\lambda z}\sum_{k=0}^{n-1}\binom{n-1}{k}z^{n-1-k}(-1)^k\int_0^zx^{m-1+k}\,dx\\
&=\frac{\lambda^{m+n}}{\Gamma(m)\Gamma(n)}z^{m+n-1}e^{-\lambda z}\sum_{k=0}^{n-1}\binom{n-1}{k}(-1)^k\frac{1}{m+k}\\
\end{align*}
\end{proof}

\begin{thm}{}{} Suppose that $T_1,T_2,\dots$ are independent random variables with exponential distribution parameter $\lambda$. Define for $t\geq0$, $$N_t=\begin{cases}
0&\text{if $T_1>t$}\\
1&\text{if $T_1\leq t<T_1+T_2$}\\
2&\text{if $T_1+T_2\leq t<T_1+T_2+T_3$}\\
\cdots\\
\end{cases}$$
Then, for any $t\geq0$, we have that $N_t\approx$Poi$(\lambda t)$. 
\end{thm}

\begin{defn}{Poisson Process}{} The family of random variables $\{N_t:t\geq 0\}$ is said to be Poisson process of intensity $\lambda$ if 
\begin{itemize}
\item $N_0=0$
\item for any $t_0,\dots,t_n$ with $0=t_0<t_1<t_2<\dots<t_n$, the random variables $N_{t_1}$, $N_{t_2}-N_{t_1}$, $N_{t_3}-N_{t_2}$, $\dots$, $N_{t_n}-N_{t_{n-1}}$ are independent, and $N_{t_i}-N_{t_{i-1}}\approx$Poi$(\lambda(t_i-t_{i-1}))$
\end{itemize}
\end{defn}

\pagebreak
\section{Expectation and Variance}
\subsection{Expectations}
\begin{defn}{Expectations}{} Let $(\Omega,\mathcal{F},P)$ be a probability space. Let $X:\Omega\to\R$ be a random variable. Define the expectation of $X$ to be $$E[X]=\int_\Omega XdP$$
\end{defn}

\begin{lmm}{}{} Let $(\Omega,\mathcal{F},P)$ be a probability space. Let $X:\Omega\to\R$ be a random variable. Then we have $$E[X]=\int_\R xf_X(x)\;dx$$
\end{lmm}

\begin{prp}{Law of the Unconscious Staticians}{} Let $(\Omega,\mathcal{F},P)$ be a probability space. Let $X_1,\dots,X_n:\Omega\to\R$ be random variables. Let $g:\R\to\R$ be a function. Then we have $$E[g\circ(X_1,\dots,X_n)]=\int_{\R^n}g(x_1,\dots,x_n)f_{(X_1,\dots,X_n)}(x_1,\dots,x_n)\;dx_1\cdots dx_n$$
\end{prp}

\begin{prp}{}{} Let $(\Omega,\mathcal{F},P)$ be a probability space. Let $X,Y:\Omega\to\R$ be random variables. Then the following are true. 
\begin{itemize}
\item If $X,Y$ are random variables and $a,b\in\R$, then $$E[aX+bY]=aE[X]+bE[Y]$$
\item If $P(X\geq Y)=1$, then $$E[X]\geq E[Y]$$
\end{itemize}
\end{prp}

\begin{prp}{}{} Let $(\Omega,\mathcal{F},P)$ be a probability space. Let $X,Y:\Omega\to\R$ be random variables. Then $X,Y$ are independent if and only if $$E[g(X)h(Y)]=E[g(X)]E[h(Y)]$$ for any two functions $g,h:\R\to\R$. 
\end{prp}

\subsection{Variance and Covariance}
\begin{defn}{Variance}{} Let $(\Omega,\mathcal{F},P)$ be a probability space. Let $X:\Omega\to\R$ be a random variable. Define the variance of $X$ to be $$\text{Var}(X)=E[(X-E[X])^2]$$
\end{defn}

\begin{lmm}{}{} Let $(\Omega,\mathcal{F},P)$ be a probability space. Let $X:\Omega\to\R$ be a random variable. Then the following are true. 
\begin{itemize}
\item $\text{Var}(X)\geq 0$. 
\item $\text{Var}(X)=0$ if and only if $P_X(E[X])=1$. 
\item $\text{Var}(X)=E[X^2]-E[X]^2$
\item $\text{Var}(aX+b)=a^2\text{Var}(X)$ for any $a,b\in\R$. 
\end{itemize}
\end{lmm}

\begin{prp}{}{} Suppose that $X_1,\dots,X_n$ are independent variables with finite variance. Then $$\vari\left(\sum_{k=1}^nX_k\right)=\sum_{k=1}^n\vari(X_k)$$
\end{prp}

\begin{defn}{Covariance}{} Let $X,Y$ be two random variables. The covariance of $X,Y$ is defined as $$\cov(X,Y)=E[(X-E(X))(Y-E(Y))]$$
\end{defn}

\begin{prp}{}{} Suppose that $X,Y$ are random variables. 
\begin{itemize}
\item $\cov(X,Y)=\cov(Y,X)$
\item $\cov(X,X)=\vari(X)$
\item $\cov(X,Y)=E(XY)-E(X)E(Y)$
\item If $X,Y$ are independent, $\cov(X,Y)=0$
\item $\cov(aX+bY,Z)=a\cov(X,Z)+b\cov(Y,Z)$
\end{itemize}
\end{prp}

\begin{prp}{Variance of Sums}{} For random variables $X_1,\dots,X_n$, we have $$\vari\left(\sum_{i=1}^{n}X_i\right)=\sum_{i=1}^n\vari\left(X_i\right)+2\sum_{1\leq i<j\leq n}\cov\left(X_i,X_j\right)$$
\end{prp}

\begin{thm}{}{} Given two random variables $X$ and $Y$, we have $$\abs*{\cov(X,Y)}\leq\sqrt{\vari(X)\vari(Y)}$$
\end{thm}

\begin{thm}{Correlation Coefficient}{} The correlation coefficient between two random variables $X$ and $Y$ is given by $$\rho(X,Y)=\frac{\cov(X,Y)}{\sqrt{\vari(X)\vari(Y)}}$$
\end{thm}

\begin{prp}{}{} Let $X$ and $Y$ be random variables. We have $$-1\leq\rho(X,Y)\leq 1$$ Moreover, for any $a,b,c,d\in\mathbb{R}$ with $a,c>0$, we have $$\rho(aX+b,cY+d)=\rho(X,Y)$$
\end{prp}

\begin{prp}{}{} Let $X$, $Y$ be random variables. 
\begin{itemize}
\item $\rho(X,X)=1$
\item $\rho(X,-X)=-1$
\item $X,Y$ are uncorrelated if $\rho(X,Y)=0$
\end{itemize}
\end{prp}

\subsection{Moments}
\begin{defn}{$k$th Moment}{} Let $X$ be a random variable. For $k\in\mathbb{N}$ we define the $k$th  moment of $X$ as $E[X^k]$ whenever the expectation exists. 
\end{defn}

\begin{defn}{Moment Generating Function}{} The moment-generating function of a random variable $X$ is the function $M_X$ defined as $$M_X(t)=E[e^{tX}]$$ for all $t\in\mathbb{R}$ for which the expectation is well defined. 
\end{defn}

\begin{thm}{}{} Assume that $M_X$ exists in a neighbourhood of $0$, that is, there exists $\epsilon>0$ such that for all $t\in(-\epsilon,\epsilon)$ we have $M_X(t)<\infty$. Then for all $k\in\mathbb{N}$ the $k$th moment of $X$ exists, and $$E[X^k]=\frac{d^k}{dt^k}M_X(t)\bigg|_{t=0}$$
\end{thm}
\begin{proof} We have that $E[X^k]=\int_{-\infty}^\infty x^kf_X(x)\,dx$ for any continuous cummulative probability. On the other hand, 
\begin{align*}
\frac{d^k}{dt^k}M_X(t)\bigg|_{t=0}&=\frac{d^k}{dt^k}\int_{-\infty}^\infty e^{tx}f_X(x)\,dx\bigg|_{t=0}\\
&=\int_{-\infty}^\infty\frac{\partial^k}{\partial t^k}e^{tx}f_X(x)\,dx\bigg|_{t=0}\\
&=\int_{-\infty}^\infty x^ke^{tx}f_X(x)\,dx\bigg|_{t=0}\\
&=\int_{-\infty}^\infty x^kf_X(x)\,dx\\
\end{align*}
\end{proof}

\begin{prp}{}{} Assume that all expectations in the statement are well defined. 
\begin{itemize}
\item For any $a,b\in\mathbb{R}$, $M_{aX+b}(t)=e^{tb}M_X(at)$
\item If $X$, $Y$ are independent, then $M_{X+Y}(t)=M_X(t)M_Y(t)$
\end{itemize}
\end{prp}

\begin{thm}{}{} Let $X$, $Y$ be two random variables. Assume that the moment generating functions of $X$, $Y$ exists and are finite on an interval of the form $(-\epsilon,\epsilon)$. Assume further that $M_X(t)=M_Y(t)$ for all $t\in(-\epsilon,\epsilon)$. Then $X$, $Y$ have the same distribution. 
\end{thm}

\begin{thm}{}{} Let $X$ be a non-negative random variable whose expectation is well defined. We then have $$P(X\geq x)\leq\frac{E(X)}{x}$$
\end{thm}

\begin{thm}{}{} Let $X$ be a random variable whose variance is well defined. Then $$P(\abs{X-E(X)}\geq x)\leq\frac{\vari(X)}{x^2}$$ for all $x>0$
\end{thm}

\subsection{Conditional Expectations}
\begin{defn}{Conditional Expectations on Subalgebras}{} Let $(\Omega,\mF,P)$ be a probability space. Let $X:\Omega\to\R$ be a random variable. Let $\mH$ be a $\sigma$-subalgebra of $\mF$. Define $E[X\;|\;\mH]:\Omega\to\R$ to be a random variable such that the following are true. 
\begin{itemize}
\item $E[X\;|\;\mH]$ is $\mH$-measurable. 
\item For any $A\in\mH$, we have $E[X\cdot 1_A]=E[E[X\;|\;\mH]\cdot 1_A]$
\end{itemize}
\end{defn}

\begin{lmm}{}{} Let $(\Omega,\mF,P)$ be a probability space. Let $X:\Omega\to\R$ be a random variable. Let $\mH$ be a $\sigma$-subalgebra of $\mF$. Then the random variable $E[X\;|\;\mH]$ exists and is unique up to almost surely equality. 
\end{lmm}

\begin{lmm}{}{} Let $(\Omega,\mF,P)$ be a probability space. Let $X,Y:\Omega\to\R$ be random variables. Let $\mH$ be a $\sigma$-subalgebra of $\mF$. Then the following are true. 
\begin{itemize}
\item Stability: If $X$ is $\mH$-measurable, then $E[XY\;|\;\mH]=XE[Y\;|\;\mH]$. 
\item Independence: If $\sigma(X)$ and $\mH$ are independent, then $E[X\;|\;\mH]=E[X]$. 
\end{itemize}
\end{lmm}

\begin{defn}{Conditional Expectation on Random Variables}{} Let $(\Omega,\mF,P)$ be a probability space. Let $X,Y:\Omega\to\R$ be random variables. Define the conditional expectation of $X$ on $Y$ to be $$E[X\;|\;Y]=E[X\;|\;\sigma(Y)]$$
\end{defn}

\begin{defn}{Conditional Density}{} Let $(\Omega,\mF,P)$ be a probability space. Let $X,Y:\Omega\to\R$ be random variables. Define the conditional density of $X$ on the event $\{\omega\in\Omega\;|\;Y(\omega)=y\}$ by $$f_{X\;|\;Y}(x,y)=\frac{f_{X,Y}(x,y)}{f_Y(y)}$$
\end{defn}

\begin{lmm}{}{} Let $(\Omega,\mF,P)$ be a probability space. Let $X,Y:\Omega\to\R$ be random variables. Then we have $$E[X\;|\;Y](\omega)=E[X\;|\;Y=Y(\omega)]=\int_{-\infty}^\infty xf_{X\;|\;Y}(x,Y(\omega))\;dx$$
\end{lmm}

\pagebreak
\section{Convergence of Random Variables}
\subsection{Convergence}
\begin{defn}{Convergence in Mean Square}{} We say that a sequence of random variables $X_1,X_2,\dots$ converges in mean square to a random variable $X$ if $$\lim_{n\to\infty}E[(X_n-X)^2]=0$$
\end{defn}

\begin{defn}{Convergence in Probability}{} We say that a sequence of random variables $X_1,X_2,\dots$ converges in probability to a random variable $X$ if for every $\epsilon>0$, we have $$\lim_{n\to\infty}P(\abs{X_n-X}>\epsilon)=0$$
\end{defn}

\begin{thm}{}{} Let $X_1,X_2,\dots$ be a sequence of random variables, and $X$ another random variable. If $X_n\to X$ in mean square as $n\to\infty$ then $X_n\to X$ in probability as $n\to\infty$. 
\end{thm}

\begin{defn}{Convergence in Distribution}{} We say that a sequence of random variables $X_1,X_2,\dots$ converges in distribution to a random variable $X$ if $$\lim_{n\to\infty}F_{X_n}(x)=F_X(x)$$ for every $x$ in the set $C=\{x\in\mathbb{R}:F_X\text{ is continuous at } x\}$. 
\end{defn}

\begin{thm}{}{} For any random variable $X$, the set of discontinuity points of $F_X$ is countable. 
\end{thm}

\begin{thm}{}{} Let $X_1,X_2,\dots$ be a sequence of random variables, and $X$ another random variable. If $X_n\to X$ in probability, then $X_n\to X$ in distribution. 
\end{thm}

\begin{thm}{}{} Let $X_1,X_2,\dots$ be a sequence of random variables such that $X_n\to c$ in distribution, where $c\in\mathbb{R}$, then $X_n$ converges in probability to $c$. 
\end{thm}

\begin{thm}{Law of large numbers in mean square}{} Let $X_1,X_2,\dots$ be a sequence of independent random variable, each with mean $\mu$ and variance $\sigma^2$. Then $$\lim_{n\to\infty}\frac{X_1+\dots+X_n}{n}\to\mu$$ in mean square. 
\end{thm}

\begin{thm}{Weak law of large numbers}{} Let $X_1,X_2,\dots$ be a sequence of independent random variable, each with mean $\mu$ and variance $\sigma^2\neq 0$. Then $$\lim_{n\to\infty}\frac{X_1+\dots+X_n}{n}\to\mu$$ in probability. 
\end{thm}

\subsection{Standardized Random Variables}
\begin{defn}{Standardized Random Variables}{} Let $X$ be a random variable with finite variance. We define the standardized version of $X$ to be the random variable $Z$ given by $$Z=\frac{X-E(X)}{\sqrt{\vari(X)}}$$
\end{defn}

\begin{thm}{Central Limit Theorem}{} Let $X_1,X_2,\dots$ be a sequence of independent and identically distributed random variables, each with mean $\mu$ and variance $\sigma^2\neq 0$. Let $S_n=X_1+\dots+X_n$. Then the standardized version of $S_n$, $$Z_n=\frac{S_n-E(S_n)}{\sqrt{\vari(S_n)}}=\frac{S_n-n\mu}{\sigma\sqrt{n}}$$ converges in distribution as $n\to\infty$ to a Gaussian random variable with mean $0$ and variance $1$. That is, $$\lim_{n\to\infty}P(Z_n\leq x)=\lim_{n\to\infty}F_{Z_n}(x)=F_Y(y)=\int_{-\infty}^{x}-\frac{1}{\sqrt{2\pi}}e^{-y^2/2}$$
\end{thm}

\pagebreak
\section{Introduction to Stochastic Processes}
\subsection{Basic Terminology}
\begin{defn}{Stochastic Processes}{} Let $(\Omega,\mathcal{F},P)$ be a probability space. Let $(S,\Sigma)$ be a measurable space. A stochastic process is a collection of random variables $$\{X(t):\Omega\to S\;|\;t\in T\}$$ indexed by some set $T$. In this case we write the stochastic process as $X:\Omega\times T\to S$. 
\end{defn}

\begin{defn}{Terminology}{} Let $(\Omega,\mathcal{F},P)$ be a probability space. Let $(S,\Sigma)$ be a measurable space. Let $X:\Omega\times T\to S$ be a stochastic process. 
\begin{itemize}
\item Define the state space of the stochastic process to be $S$. 
\item Write $\mF_n=\sigma(X_0,\dots,X_n)$. 
\end{itemize}
\end{defn}

\begin{defn}{Discrete Time Stochastic Processes}{} Let $(\Omega,\mathcal{F},P)$ be a probability space. Let $(S,\Sigma)$ be a measurable space. Let $X:\Omega\times T\to S$ be a stochastic process. We say that the stochastic process has discrete time if $T\subseteq\N$. 
\end{defn}

\begin{defn}{Filtrations}{} Let $\mF=\{\mF_t\;|\;t\in T\}$ be a family of $\sigma$-algebras. We say that $\mF$ is a filtration if $\mF_t\leq\mF_s$ for $t\leq s$. 
\end{defn}

\begin{defn}{Adapted Stochastic Process}{} Let $(\Omega,\mathcal{F},P)$ be a probability space. Let $(S,\Sigma)$ be a measurable space. Let $X:\Omega\times T\to S$ be a stochastic process. Let $\mF=\{\mF_t\;|\;t\in T\}$ be a filtration. We say that $X$ is adapted to $\mF$ if $X_t$ is $\mF_t$-measurable for all $t\in T$. 
\end{defn}

\subsection{Hitting Time and Stopping Time}
\begin{defn}{Hitting Time}{} Let $(\Omega,\mathcal{F},P)$ be a probability space. Let $(S,\Sigma)$ be a measurable space. Let $X:\Omega\times T\to S$ be a stochastic process. Let $A\in\Sigma$ be a measurable set. Define the hitting time of $X$ to be the random variable $H_A:\Omega\to T$ given by $$H_A(\omega)=\inf\{t\in T\;|\;X(\omega,t)\in A\}$$
\end{defn}

Recall that for a $\sigma$-algebra $\mF$ and a subset $Y\subseteq\mF$, $\sigma(Y)$ is the smallest $\sigma$-algebra that contains $Y$. 

\begin{defn}{Stopping Time}{} Let $(\Omega,\mathcal{F},P)$ be a probability space. Let $(S,\Sigma)$ be a measurable space. Let $X:\Omega\times T\to S$ be a stochastic process. Let $S:\Omega\to T$ be a random variable. We say that $S$ is a stopping time if $S$ has the property that $$\{\omega\in\Omega\;|\;S(\omega)\leq t\}\subseteq\sigma(X_t)$$ for all $t\in T$. 
\end{defn}

In other words, events determined by $S$ up to some time $t\in T$ is dictated entirely using the variables $X_0,\dots,X_t$. 

\begin{lmm}{}{} Let $(\Omega,\mathcal{F},P)$ be a probability space. Let $(S,\Sigma)$ be a measurable space. Let $X:\Omega\times T\to S$ be a stochastic process. Let $S:\Omega\to T$ be a random variable. Then $S$ is a stopping time if and only if $S$ has the property that $$\{\omega\in\Omega\;|\;S(\omega)=t\}\subseteq\sigma(X_t)$$ for all $t\in T$. 
\end{lmm}

\begin{thm}{Début Theorem}{}
\end{thm}

\pagebreak
\section{Markov Chains}
\subsection{Markov Chains}
\begin{defn}{Stochastic Matrix}{} Let $P=(p_{i,j})\in M_n(\R)$ be a matrix. We say that $P$ is a stochastic matrix if the following are true. 
\begin{itemize}
\item $0\leq p_{i,j}\leq 1$ for $1\leq i,j\leq n$. 
\item For any fixed $1\leq k\leq n$, we have $$\sum_{j=1}^np_{k,j}=1$$
\end{itemize}
\end{defn}

\begin{defn}{Markov Chain}{} Let $(\Omega,\mF,\Prj)$ be a probability space. Let $I=\{1,\dots,k\}\subseteq\N$. Let $\{X_n:\Omega\to I\;|\;n\in\N\}$ be a discrete stochastic process. Let $\lambda:I\to[0,1]$ be a probability distribution. Let $P=(p_{i,j})$ be a stochastic matrix. We say that $(X_n)_{n\geq 0}$ is a Markov chain with initial distribution $\lambda$ and transition matrix $P$ if the following are true. 
\begin{itemize}
\item $P_{X_0}=\lambda$
\item For any $i_0,\dots,i_{n+1}\in I$, we have $$\Prj(X_{n+1}=i_{n+1}\;|\;X_k=i_k\text{ for }0\leq k\leq n)=\lambda(i_0)\cdot\prod_{j=0}^n p_{i_j,i_{j+1}}$$ 
\end{itemize}
In this case we say that $(X_n)_{n\geq 0}$ is $\text{Markov}(\lambda,P)$. 
\end{defn}

In other words, Markov chains are a sequence of random variables where the next step does not depends on what states you visited previously, but only on the state now. There is an important property that makes studying Markov chains worth while. 

\begin{prp}{}{} Let $(\Omega,\mF,\Prj)$ be a probability space. Let $I=\{1,\dots,k\}$. Let $\{X_n:\Omega\to I\;|\;n\in\N\}$ be a sequence of random variables. Let $\lambda:I\to[0,1]$ be a probability distribution. Then $(X_n)_{n\geq 0}$ is a Markov chain if and only if $$\Prj(X_{n+1}=i_{n+1}\;|\;X_k=i_k\text{ for }0\leq k\leq n)=\Prj(X_{n+1}=i_{n+1}\;|\;X_n=i_n)$$ for any $i_0,\dots,i_{n+1}\in I$. In this case, the transition matrix of the Markov chain is given by $$P=\begin{pmatrix}
\Prj(X_1=1\;|\;X_0=1) & \cdots & \Prj(X_1=k\;|\;X_0=1)\\
\vdots & \ddots & \vdots\\
\Prj(X_1=1\;|\;X_0=k) & \cdots & \Prj(X_1=k\;|\;X_0=k)
\end{pmatrix}$$
\end{prp}

\begin{thm}{Markov Property}{} Let $(X_n)_{n\geq0}$ be a markov chain. Suppose that $X_m=E_m$ is given. Then $(X_{m+n})_{n\geq 0}$ is a Markov chain. 
\end{thm}


Notice that the transition probabbility $\Prj(X_{n+1}=j|X_n=i)$ still depedns on $i,j,n$. We further restrict out study of Markov chains to the following type. 

\begin{defn}{Homogenous Markov Chains}{} We say that a Markov chain $(X_n)_{n\geq 0}$ is homogenous if $$\Prj(X_{n+1}=j|X_n=i)=\Prj(X_1=j|X_0=i)$$
\end{defn}

This means that homogenous Markov chains are time independent. 

\begin{lmm}{}{} The transition matrix $P$ of a homogenous Markov chain is a Stochastic matrix. 
\end{lmm}

\begin{thm}{}{} Let $(X_n)_{n\geq 0}$ be Markov. Then for all $m,n\geq0$, we have
\begin{itemize}
\item $\Prj(X_n=j|X_0=i)=\Prj(X_1=j|X_0=i)^n$
\item $\Prj(X_n=j)=\sum_{i\in I}\Prj(X_0=i)\Prj(X_n=j|X_0=i)$
\end{itemize}
\end{thm}

This theorem is saying that $\Prj(X_n=j|X_0=i)$ is equal to the $i,j$th entry of the matrix $P^n$ if $P$ is the transition matrix. To find out the total probability of reaching the $j$th state at the $n$the step regardless of the starting point, you sum up the $\Prj(X_n=j|X_0=i)$ multiplying the probability that you start at state $i$. \\~\\

We often like to create new Markov chains from old (at least from exams). If we want to show that a squence $(X_n)_{n\geq 0}$ of random vriables is Markov, try and write $X_{n+1}=X_n+\text{ some term only related to }n+1$. 

We use $$h_i^A=\Prj(H^A<\infty|X_0=i)$$ to denote the corresponding probability. We use $$k_i^A=E_i[H_A]=\sum_{k=0}^\infty k\Prj(H_A=k|X_0=i)+\infty\Prj(H_A=\infty|X_0=i)$$ to denote the expectation. 

\begin{prp}{}{} We can find the probabilities of all hitting times by solving the system of linear equations $$\begin{cases}
\Prj(H_A<\infty|X_0=i)=1 & i\in A\\
\Prj(H_A<\infty|X_0=i)=\sum_{j\in I}\Prj(X_1=j|X_0=i)\Prj(H_A<\infty|X_0=j) & x\notin A
\end{cases}$$
\end{prp}

\begin{prp}{}{} We can find the expected number of hitting times by solving the system of linear equations $$\begin{cases}
E_i[H_A]=0 & i\in A\\
E_i[H_A]=1+\sum_{j\notin A}\Prj(X_1=j|X_0=i)E_j[H_A] & i\notin A
\end{cases}$$
\end{prp}

\begin{thm}{Strong Markov Property}{} Let $(X_n)_{n\geq 0}$ be Markov$(\lambda,P)$ and $T$ a stopping time of $(X_n)_{n\geq 0}$. Then conditional on both $\{X_T=i\}$ and $\{T<\infty\}$, we have $(X_{T+n})_{n\geq 0}$ is Markov$(\delta_i,P)$ and independent of $X_0,\dots,X_T$. 
\end{thm}

\subsection{Communicating Classes}
\begin{defn}{Talks and Communicates}{} Let $i,j$ be states. We say that $i$ talks to $j$ which is $i\rightarrow j$ if there exists $n\in\N$ such that $$\Prj(X_n=j|X_0=i)>0$$ We say that two states $i$ and $j$ communicate if $i\leftrightarrow j$. 
\end{defn}

\begin{defn}{Communicating Class}{} Let $C\subseteq I$. $C$ is a communicating class if $\forall i,j\in C$, $i\leftrightarrow j$ and $\forall i\in C$ and $\forall k\in I\setminus C$, $i\not{\rightarrow} k$ 
\end{defn}

\begin{defn}{Closed and Absorbing}{} We say that a class is closed if no states in the class talks to any states outside of that class. We say that a class is absorbing if it forms a closed class by itself. 
\end{defn}

\begin{defn}{Irreducible Markov Chains}{} A Markov chain is said to be irreducible if for all $i,j\in I$, $i\leftrightarrow j$. 
\end{defn}

\subsection{Recurrence and Transcience}
\begin{defn}{Recurrence and Transcience}{} A state $i\in I$ is recurrent if $\Prj_i(X_n=i\text{ for infinitely many }n)=1$. A state $i\in I$ is transient if $\Prj_i(X_n=i\text{ for infinitely many }n)=0$. 
\end{defn}

\begin{defn}{$k$-th Passage Time}{} The first passage time of state $i$ is a stopping time such that $$T_i(\omega)=\inf\{n\geq 1|X_n(\omega)=i\}$$ The $k$-th passage time is a stopping time such that $$T_i^{(k)}(\omega)=\inf\{n\geq T_i^{(k-1)}|X_n(\omega)=i\}$$ The $k$-th excursion time is defined by $$S_i^{(k)}=\begin{cases}
T_i^{(k)}-T_i^{(k-1)} & \text{ if }T_i^{(k-1)}<\infty\\
\infty & \text{ otherwise }
\end{cases}$$
\end{defn}

Intuitively, $T_i^{(k)}$ outputs the time it takes for the Markov chain to reach state $i$ for the $k$th time and $S_i^k$ outputs the number of steps taken between consecutive visits. 

\begin{lmm}{}{} For $k=2,3,\dots$ and if $T_i^{(k-1)}<\infty$, then $S_i^{(k)}$ is independent of $\{X_m:m\leq T_i^{(k-1)}\}$ and $$\Prj(S_i^{(k)}=n|T_i^{(k-1)}<\infty)=\Prj_i(T_i=n)$$
\end{lmm}

\begin{defn}{Visit Counting Function}{} Let $i\in I$ be a state. Define $$V_i=\sum_{k=0}^\infty1_{\{X_n=i\}}$$ the number of visits ever to state $i$. 
\end{defn}

\begin{lmm}{}{} If $V_i$ counts the number of vists to state $i$, then $$E_i(V_i)=\sum_{k=0}^\infty P_{ii}^{(k)}$$
\end{lmm}

\begin{lmm}{}{} For $k=0,1,2,\dots$, we have $$\Prj_i(V_i>k)=(\Prj_i(T_i<\infty))^k$$
\end{lmm}

This lemma means that $V_i$ asserts a geometric distribution. 

\begin{thm}{Characteristic of Recurrent and Transient States}{} Let $(X_n)_{n\geq 0}$ be a Markov chain. Let $T_i$ be the first passage time of state $i$. If $\Prj_i(T_i<\infty)=1$, then $i$ is recurrent and $\sum_{k=1}^\infty\Prj(X_1=k|X_0=k)^n=\infty$. If $\Prj_i(T_i<\infty)<1$, then $i$ is transient and $\sum_{k=1}^\infty\Prj(X_1=k|X_0=k)^n<\infty$. 
\end{thm} 

\begin{thm}{}{} Let $C$ be a communicating class. Then either all states in $C$ are transient or all are recurrent. 
\end{thm}

\begin{thm}{}{} Let $C\subset I$ be a class of a Markov chain. Then
\begin{itemize}
\item Every recurrent class is closed
\item Every finite closed class is recurrent
\end{itemize}
\end{thm}

\begin{thm}{}{} If $P$ is irreducible and recurrent then for all $j\in I$, $$\Prj(T_j<\infty)=1$$
\end{thm}

\begin{defn}{Component Independent Simple Random Walk on $\Z^d$}{} A component independent simple random walk on $\Z^d$ for $d\in\N$ is defined as $$X_{n+1}=X_n+Z_{n+1}$$ where $Z_{n+1}=(Z_{n+1}^1,\dots,Z_{n+1}^d)$ with $$Z_m^j=\begin{cases}
1 & \text{ with probability }p_j\\
-1 & \text{ with probabilty }q_j
\end{cases}$$
where $Z_m^j$ are independent for all $j,m$. 
\end{defn}

\begin{prp}{}{} Let $(X_n)_{n\geq 0}$ be a CISRW on $\Z^d$. If there exists $j\in\{1,\dots,d\}$ such that $p_j\neq\frac{1}{2}$ then $(X_n)$ is transient. 
\end{prp}

\begin{prp}{}{} Let $(X_n)_{n\geq 0}$ be a CISRW on $\Z^d$ and $p_j=q_j=\frac{1}{2}$ for all $j$. Then
\begin{itemize}
\item If $d\leq 2$ then all states are recurrent
\item If $d\geq 3$ then all states are transient
\end{itemize}
\end{prp}

\subsection{Branching Process}
\begin{defn}{Branching Process}{} Let $(X_n)_{n\geq 0}$ be the number of individuals in a population at time $n$ where $X_n\in\{0\}\cup\N$. At every time step each individual in the population gives birth to a random number of offspring. Thus $X_{n+1}$ is defined to be $$X_{n+1}=\sum_{k=1}^{X_n}Z_k^n$$ where $Z_k^n\sim Z$ and $\Prj(Z\geq 0)=1$. Then $(X_n)_{n\geq 0}$ is said to be a branching process. 
\end{defn}

\begin{prp}{}{} Define $G(s)=E[s^Z]$ and $F_{n+1}(s)=E[S^{X_{n+1}}|X_0=1]$ for $s\in(0,1)$. Then $$F_n(s)=G(F_{n-1}(s))$$
\end{prp}

\begin{prp}{}{} For a braching process $(X_n)_{n\geq 0}$ with off-spring distribution $Z$ where $E[Z]=\mu$, we have $$E[X_n]=\mu^n$$
\end{prp}

\begin{thm}{Extinction Probability}{} The extinction probability is the smallest non-negative root of $G(\alpha)=\alpha$. 
\end{thm}

\begin{thm}{}{} Suppose that $G(0)>0$, $\mu=E[Z]=G'(1)$. Then $\mu\leq 1$ implies certain extiction. $\mu>1$ implies uncertain extinction. 
\end{thm}

\subsection{Invariant Distributions on a Markov Chain}
\begin{defn}{Invariant Distribution}{} A measure $\lambda=(\lambda_i:i\in I)$ with non-negative entries is said to be an invariant distribution if 
\begin{itemize}
\item $\lambda P=P$
\item $\pi_i\in[0,1]$ for all $i$
\item $\sum_{i\in I}\pi_i=1$
\end{itemize}
\end{defn}

The following theorem shows that after applying a Markov process to an invariant distribution, the new distribution will be the same as the old one. Notice here that right multiplication of a distribution gives the next step on the Markov chain instead of the usual left multiplication. 

\begin{thm}{}{} Let $(X_n)_{n\geq 0}$ be Markov$(\pi,P)$, where $\pi$ is an invariant distribution for $P$. Then $\forall m\geq 0$, $(X_{n+m})_{n\geq 0}$ is Markov$(\pi,P)$. \tcbline
\begin{proof}
By the Markov property, the new sequence will be Markov. But we do not yet know its distribution. We have that 
\begin{align*}
\Prj(X_m=i)&=(\pi P^m)_i\tag{this denotes the $i$th entry of $P$}\\
&=(\pi PP^{m-1})_i\\
&=(\pi P^{m-1})_i
\end{align*}
Thus by induction, we see that $\Prj(X_m=i)=\pi_i$ thus we are done. 
\end{proof}
\end{thm}

\begin{thm}{}{} Let $I$ be finite. Suppose that for some $i\in I$, $$P_{ij}^n\to\pi_j$$ for all $j\in I$. Then $\pi=(\pi_j:j\in I)$ is an invariant distribution. 
\end{thm}

Let us look at an example. 

\begin{eg}{Gene Mutation}{} Recall the gene mutation example that has transition matrix $P=\begin{pmatrix}
1-\alpha & \alpha\\
\beta & 1-\beta
\end{pmatrix}$. Recall that $$P^n=\begin{pmatrix}
\frac{\beta}{\alpha+\beta}+\frac{\alpha}{\alpha+\beta}(1-\alpha-\beta)^n & \frac{\alpha}{\alpha+\beta}-\frac{\alpha}{\alpha+\beta}(1-\alpha-\beta)^n\\
? & ?
\end{pmatrix}$$
As $n\to\infty$, observe that the first entry tends to $\frac{\beta}{\alpha+\beta}$ the second entry tends to $\frac{\alpha}{\alpha+\beta}$. This means that $\pi=\left(\frac{\beta}{\alpha+\beta},\frac{\alpha}{\alpha+\beta}\right)$ is an invariant distribution for the gene mutation model. \\~\\
We can also find the invariant distribution directly: Let $\pi=(\pi_1,\pi_2)$ be an invariant distribution for $P$. We have that 
\begin{align*}
\pi P&=\pi\\
\begin{pmatrix}
\pi_1(1-\alpha)+\pi_2\beta & \pi_1\alpha+\pi_2(1-\beta)
\end{pmatrix}&=\begin{pmatrix}
\pi_1 & \pi_2
\end{pmatrix}\\
\begin{pmatrix}
\pi_2\beta-\pi_1\alpha & \pi_1\alpha-\pi_2\beta
\end{pmatrix}&=0\\
\end{align*} Notice that these two equations mean the same thing. (In general if you have an $n\times n$ transition matrix, only the first $n-1$ equations will be useful and the last one will be redundant.) This is why we need to use the fact that $\pi_1+\pi_2=1$ as our final equation. \\~\\
Now solving it, we have that $\pi=\left(\frac{\beta}{\alpha+\beta},\frac{\alpha}{\alpha+\beta}\right)$ which is the exact same answer as the one given in the first method. \\~\\
A third way to think about it is that notice that $P^T\pi^T=\pi^T$ is the equation that we are attempting to solve. Recall that $1$ is necessarily an eigenvector of a transtition matrix which means that this equation really is just a question of finding eigenvectors from the eigenvalue $1$. 
\end{eg}

The above examples gives a lot of practical information when calculating the invariant distribution of a transition matrix. 

\begin{defn}{Expected Time Spent}{} Define the expected time spent in state $i$ in between visits to state $k$ by $$\gamma_i^k=E_k\left(\sum_{n=0}^{T_k-1}\mathcal{X}_{X_n=i}\right)$$
\end{defn}

This uses $k$ as a reference, and as we keep going back to $k$, this records how much time we have spent in $i$ in the process of leaving and returning to $k$. 

\begin{thm}{}{} Let $P$ be irreducible and recurrent. Then
\begin{itemize}
\item $\gamma_k^k=1$
\item $\gamma^k=(\gamma_i^k:i\in I)$ is an invariant measure satisfying $\gamma^kP=\gamma^k$
\item $0<\gamma_i^k<\infty\;\forall i\in I$
\end{itemize}
\end{thm}

\begin{thm}{}{} Let $P$ be irreducible and $\lambda$ and invariant measure and $\lambda_k=1$ for some $k$. Then $\lambda\geq\gamma^k$. If we also have that $P$ is recurrent, then $\lambda=\gamma^k$
\end{thm}

This theorem assesrts that as long as $P$ is irreducible and recurrent, then any invariant measure is exactly equal to the $\gamma^k$ we defined, which means that there is really only one invariant measure up to rescaling. 

\begin{defn}{Positive Recurrent}{} A state $i\in I$ is positive recurrent if it is recurrent and $$E_i[T_i]<\infty$$ A state which is not positive recurrent but is recurrent is called null recurrent. 
\end{defn}

\begin{thm}{}{} Let $P$ be irreducible. Then the following are equivalent. 
\begin{itemize}
\item Every state is positive recurrent
\item Some state $i$ is positive recurrent
\item $P$ has an invariant distribution $\pi$. More over, $\pi_i=\frac{1}{E_i[T_i]}$
\end{itemize}
\end{thm}

\pagebreak
\section{Brownian Motion}
\begin{defn}{Brownian Motion}{} Let $(\Omega,E,P)$ be a probability space. Let $(S,\Sigma)$ be a measurable space. Let $B:\Omega\times[0,T]\to S$ be continuous stochastic processes. We say that $B$ is a Brownian motion if the following are true. 
\begin{itemize}
\item $B_0=0$. 
\item For any $0\leq t_1<t_2<t_3\leq T$, $$B_{t_2}-B_{t_1}\;\;\;\;\text{ and }\;\;\;\;B_{t_3}-B_{t_2}$$ are independent variables. 
\item For any $0\leq t_1<t_2\leq T$, $$B_{t_2}-B_{t_1}\sim N(0,t_2-t_1)$$
\item For any $\omega\in\widetilde{\Omega}$ such that $P(\widetilde{\Omega})=1$, we have that $B(\omega,-):T\to S$ is a continuous function. 
\end{itemize}
\end{defn}

\pagebreak
\section{Martingale Processes}
\subsection{Basic Definitions}
\begin{defn}{Martingale Process}{} Let $(\Omega,E,P)$ be a probability space. Let $(S,\Sigma)$ be a measurable space. Let $M:\Omega\times[0,T]\to S$ be a continuous stochastic process adapted to a filtration $\mF=\{\mF_t\;|\;t\in[0,T]\}$. We say that $M$ is Martingale with respect to $\mF$ if the following are true. 
\begin{itemize}
\item $E[\abs{M_k}]<\infty$ for all $k$. 
\item $E[M_{t_2}\;|\;\mF_{t_1}]=M_{t_1}$ for all $0\leq t_1<t_2\leq T$. 
\end{itemize}
\end{defn}

\begin{defn}{Continuous Martingale Process}{} Let $(\Omega,E,P)$ be a probability space. Let $(S,\Sigma)$ be a measurable space. Let $M:\Omega\times[0,T]\to S$ be a continuous stochastic process. We say that $M$ is continuous if for all $\omega\in\widetilde{\Omega}$ such that $P(\widetilde{\Omega})=1$, the function $$M(\omega,-):[0,T]\to S$$ is continuous. 
\end{defn}

\begin{defn}{Standard Brownian Filtration}{}
\end{defn}

\subsection{Martingale Transformations}

\pagebreak
\section{Stochastic Calculus}
\subsection{Ito's Integral}
Given a Martingale process $M:\Omega\times[0,T]\to\R$ on some filtered probability space, we may ask to integrate $M$ over the product measure of the measurable space $\Omega\times[0,T]$ because $M$ is in particular a measurable function. Ito's integral gives a construction of such an integral under certain assumption on $M$. 

\begin{defn}{Simple Functions}{} Let $(\Omega,\mF,P)$ be a probability space. Define $\mH_0^2\subseteq L^2(\Omega\times[0,T])$ to be the vector subspace whose measurable functions are of the form $$f(\omega,t)=\sum_{i=0}^{n-1}a_i(\omega)1_{(t_i,t_{i+1}]}$$
\end{defn}

\begin{defn}{Ito Integral of Simple Functions}{} Let $(\Omega,\mF,P)$ be a probability space. Let $f\in\mH_0^2$. Define the Ito integral of $f(\omega,t)=\sum_{i=0}^{n-1}a_i(\omega)1_{(t_i,t_{i+1}]}$ by the formula $$I(f)(\omega)=\sum_{t=0}^{n-1}a_i(\omega)(B_{t_{i+1}}-B_{t_i})$$ In particular, $I$ is a function $I:\mH_0^2\to L^2(\Omega)$. 
\end{defn}

\begin{lmm}{Ito's Isometry I}{} Let $(\Omega,\mF,P)$ be a probability space. The map $I:\mH_0^2\to L^2(\Omega)$ is a linear isometry. 
\end{lmm}

\begin{defn}{Ito Integrable Functions}{} Let $(\Omega,\mF,P)$ be a probability space. Define the space of Ito integrable functions $\mH^2\subseteq L^2(\Omega\times[0,T])$ to be the vector subspace whose measurable functions $f\in L^2(\Omega\times[0,T])$ satisfies the fact that $$E\left[\int_{[0,T]}f^2(\omega,t)\;dt\right]=\int_\Omega\int_{[0,T]}f^2(\omega,t)\;dt\;dP<\infty$$
\end{defn}

\begin{lmm}{}{} Let $(\Omega,\mF,P)$ be a probability space. Then $\mH_0^2$ is dense in $\mH^2$. 
\end{lmm}

\begin{defn}{Ito Integral of Integrable Functions}{} Let $(\Omega,\mF,P)$ be a probability space. Let $f\in\mH^2$. Let $(f_n)_{n\in\N\setminus\{0\}}$ be a sequence such that $f_n\to f$ in $L^2(\Omega\times[0,T])$. Then define the Ito integral of $f$ by $$I(f)=\lim_{n\to\infty}I(f_n)$$ In particular, $I$ is a function $I:\mH^2\to L^2(\Omega)$. 
\end{defn}

\begin{lmm}{Ito's Isometry II}{} Let $(\Omega,\mF,P)$ be a probability space. The map $I:\mH^2\to L^2(\Omega)$ is a linear isometry. 
\end{lmm}

\subsection{The Integral as a Martingale}

\subsection{Ito's Lemma}

\subsection{Localization}

\pagebreak
\section{Stochastic Differential Equations}
\subsection{Basic Definitions}
\begin{defn}{Stochastic Differential Equations}{}
\end{defn}

Methods of solving differential equations can also be applied to stochastic differential equations. 

\subsection{Geometric Brownian Motions}
\begin{defn}{Geometric Brownian Motion}{} Let $(\Omega,E,P)$ be a probability space. Let $S:\Omega\times[0,T]\to\R$ be a continuous Martingale process with respect to the standard Brownian filtration $\mB=\{B_t\;|\;t\in[0,T]\}$. We say that $S$ follows a geometric Brownian motion if it satisfies the following stochastic differential equation $$dS_t=\mu S_tdt+\sigma S_tdB_t$$ where $\mu$ is called the percentage drift and $\sigma$ is called the percentage volitality. 
\end{defn}

Recall that $dS_t$ really means the Radon–Nikodym derivative $\frac{dS_t}{d\mu}$ where $\mu$ here (not the same one as above) refers to the standard measure on $\R$, while $dt$ means the genuine infinitesimally small increment in $\R$. 

\begin{lmm}{}{} Let $(\Omega,E,P)$ be a probability space. Let $S:\Omega\times[0,T]\to\R$ be a continuous Martingale process with respect to the standard Brownian filtration $\mB=\{B_t\;|\;t\in[0,T]\}$. Suppose that $S$ follows a geometric Brownian motion. Then the solution of the stochastic differential equation is given by $$S_t=S_0e^{(\mu-\sigma^2/2)t+\sigma B_t}$$
\end{lmm}

\begin{prp}{}{} Let $(\Omega,E,P)$ be a probability space. Let $S:\Omega\times[0,T]\to\R$ be a continuous Martingale process with respect to the standard Brownian filtration $\mB=\{B_t\;|\;t\in[0,T]\}$. Suppose that $S$ follows a geometric Brownian motion. Then the following are true. 
\begin{itemize}
\item For each $t$, $S_t$ has a log normal distrbution. 
\item For each $t$, we have $$E[S_t]=S_0e^{\mu t}$$
\item For each $t$, we have $$\text{Var}(S_t)=S_0^2e^{2\mu t}(e^{\sigma^2 t}-1)$$
\end{itemize}
\end{prp}

We also provide a discrete approach to solving the differential equation for easy simulation using the Monte Carlo method. 

\begin{prp}{}{} The discretization of the defining stochastic differential equation of a geometric Brownian motion is given by $$S_{t+dt}-S_t=\mu S_tdt+\sigma S_tN\sqrt{dt}$$ where $dt$ denotes a small change in time, and $N\sim N(0,1)$ refers a random variable with a normal distribution. 
\end{prp}

\subsection{Systems of SDEs}

\pagebreak
\section{Mathematical Finance}
Assets: Items
Stocks: Makes you own a small percentage of a company. 
Shares: The unit for stocks. (Owning share = Owning a certain number of stocks)
Bonds: Buying debt from a loan issuer. 
Options: Contract that gives the ability to buy / sell assets at a certain time in the future. 
Futures: Contract to buy / sell assets at a certain time in the future. 
Value: Total gain / loss from the whole ordeal. 

Derivatives: Just think of them as options / future for now. 
Strike price: the cost of exercising the derivative at the expiration date. 

Idea: Derivatives themselves can also be traded. 

Call: Looking to buy. 
Put: Looking to sell. 

European options: can only exercise at the specified date. 
American options: can exercise any time before the specified date. 

Derivative Traders: 
Hedger: Actually exercises the derivative. (To minimize loss)
Speculator: Actually bets on the ups and downs of the derivatives. 
Arbitrageurs: Finding certain entry point to guarantee some winnings by calling / putting a combination of derivatives. (Too powerful and rare so we usually assume no arbitrage opportunities occur)

The price of an option is a function of 
\begin{itemize}
\item $S_0$: Current stock price. 
\item $K$: Strike price. 
\item $T$: Time to expiration. 
\item $r$: Risk free interest rate. 
\item Dividends that are expected to be paid. 
\end{itemize}
More notation: 
\begin{itemize}
\item $S_T$: Stock price at expiration day. 
\item $C/P$: American call / put. 
\item $c/p$: European call / put. 
\end{itemize}

We assume no arbitrage opportunities (if there is then it disappears quickly). 

We have 
\begin{itemize}
\item $C\leq S_0$
\item $S_0-Ke^{rT}\leq c\leq S_0$. 
\item $P\leq K$
\item $Ke^{-rT}-S_0\leq p\leq Ke^{-rT}$
\end{itemize}

More:
\begin{itemize}
\item $c+Ke^{-rT}=p+S_0$ (idea: European call and put options worth the same price at time $T$)
\item $S_0-K\leq C-P\leq S_0-Ke^{-rT}$
\end{itemize}



\pagebreak
\section{Mathematical Models for Option Pricing}
\subsection{Background Terminology}
Risk-neutral: A situation where investors do not expect an increase in return when the risk increases. This has two consequences. 
\begin{itemize}
\item The expected return of a stock is the risk free rate. 
\item The discount rate for the expected pay off of an option is the risk free. 
\end{itemize}

Risk-less: A portfolio is risk-less when regardless of the outcome, the net total is constant. 

\subsection{One-Period Binomial Models}
\begin{defn}{The Binomial Model}{} Suppose that $t$ is the current time. An asset has a current stock price of $S_0$ with an unknown option price $\mO$. After a time step of $dt$, the stock price either increase by a multiplicative factor of $u>1$ or the stock price decreases by a multiplicative factor of $d<1$. Denote the corresponding option value (the net gain from the option) by $\mO_u$ and $\mO_d$ respectively. \\~\\
\adjustbox{scale=1.0,center}{\begin{tikzcd}
	t && {t+dt} \\
	&& \begin{array}{c} \substack{uS_0\\\mO_u} \end{array} \\
	\begin{array}{c} \substack{S_0\\\mO} \end{array} \\
	&& \begin{array}{c} \substack{dS_0\\\mO_d} \end{array}
	\arrow["dt", from=1-1, to=1-3]
	\arrow["p", from=3-1, to=2-3]
	\arrow["{1-p}"', from=3-1, to=4-3]
\end{tikzcd}} \\~\\
The number $0<p<1$ is an unknown quantity denoting the probability that the stock price increases. 
\end{defn}

\begin{lmm}{}{} Assume there is no arbitrage opportunity and the world is risk-neutral. Assume the binomial model. Suppose a portfolio of buying $\Delta\in\N$ stocks and shorting a call option. Then the portfolio is risk-less when $$\Delta=\frac{\mO_u-\mO_d}{uS_0-dS_0}$$
\end{lmm}

\begin{prp}{}{} Assume there is no arbitrage opportunity and the world is risk-neutral. Assume the binomial model. Denote $V(t)=S_0\Delta-\mO$ the value of the portfolio of buying $\Delta$ stocks and shorting a call option. Then the following are true. 
\begin{itemize}
\item The probability $p$ that the stock price increases is given by $$p=\frac{e^{rdt}-d}{u-d}$$
\item The option price is given by $$\mO=e^{-rdt}\left(p\mO_u+(1-p)\mO_d\right)$$
\end{itemize} \tcbline
\begin{proof}
Accounting for the discount rate, we have $$V(t+dt)=(uS_0\Delta-\mO_u)e^{-rdt}=(dS_0\Delta-\mO_d)e^{-rdt}$$ by definition of $\Delta$. Equating $V(t)$ and $V(t+dt)$ gives the desired result. 
\end{proof}
\end{prp}

\begin{lmm}{}{} Assume there is no arbitrage opportunity and the world is risk-neutral. Assume the binomial model. Suppose that the implied volatility is $\sigma$. Then we have $$u=e^{\sigma\sqrt{dt}}\;\;\;\;\text{ and }\;\;\;\;d=e^{-\sigma\sqrt{dt}}$$
\end{lmm}

\subsection{Multi-Period Binomial Models}
\begin{defn}{The Multi-Period Binomial Model}{} Suppose that $t$ is the current time. An asset has a current stock price of $S_0$ with an unknown option price $\mO$. After a time step of $dt$, the stock price either increase by a multiplicative factor of $u>1$ or the stock price decreases by a multiplicative factor of $d<1$. Denote the corresponding option value (the net gain from the option) by $\mO_u$ and $\mO_d$ respectively. \\~\\
\adjustbox{scale=1.0,center}{\begin{tikzcd}
	&&&& \begin{array}{c} \substack{u^2S_0\\\mO_{u^2}} \end{array} & \cdots \\
	&& \begin{array}{c} \substack{uS_0\\\mO_u} \end{array} \\
	\begin{array}{c} \substack{S_0\\\mO} \end{array} &&&& \begin{array}{c} \substack{udS_0\\\mO_{ud}} \end{array} & \cdots \\
	&& \begin{array}{c} \substack{dS_0\\\mO_d} \end{array} \\
	&&&& \begin{array}{c} \substack{d^2S_0\\\mO_{d^2}} \end{array} & \cdots
	\arrow["p", from=2-3, to=1-5]
	\arrow["{1-p}", from=2-3, to=3-5]
	\arrow["p", from=3-1, to=2-3]
	\arrow["{1-p}"', from=3-1, to=4-3]
	\arrow["p"', from=4-3, to=3-5]
	\arrow["{1-p}"', from=4-3, to=5-5]
\end{tikzcd}} \\~\\
The number $0<p<1$ is an unknown quantity denoting the probability that the stock price increases. The process iterates for $n$-times. 
\end{defn}

Notice at each time-step, the process is identical except with a different current stock and option price. Therefore we can iterate the one-period binomial model $n$-times to recover the option value. 

\begin{prp}{}{} Assume there is no arbitrage opportunity and the world is risk-neutral. Assume the mutli-period binomial model with period $n$. Then the European call option price is given by $$\mO=e^{-nrdt}\left(\sum_{i=0}^n\binom{n}{i}p^{n-i}(1-p)^i\mO_{u^{n-i}d^i}\right)$$
\end{prp}

\subsection{Black-Scholes-Merton Model}
\begin{defn}{Black-Scholes-Merton Model}{} Let $T$ be a set time (in years). Denote $S:\Omega\times[0,T]\to\R$ the stochastic processes that is the value of a certain stock. Let $\mu$ be the annual expected return. Let $\sigma$ be the annual volatility of the stock price. The Black-Scholes-Merton Model makes the following assumptions
\begin{itemize}
\item The percentage in stock is normally distributed: $$\frac{S_{t+dt}-S_t}{S_t}\sim N(\mu dt, \sigma^2dt)$$ 
\item $\mu$ and $\sigma$ are constants over time. 
\item The risk free interest rate is constant over time. 
\item There are no dividends during the lifetime of the stock. 
\item There are no arbitrage opportunities. 
\end{itemize}
\end{defn}

\begin{lmm}{}{} Let $T$ be a set time (in years). Denote $S:\Omega\times[0,T]\to\R$ the stochastic processes that is the value of a certain stock. Let $\mu$ be the annual expected return. Let $\sigma$ be the volatility of the stock price. Assume the Black-Scholes-Merton model. Then we have $$\ln(S_t)\sim N\left(\ln(S_0)+\left(\mu-\frac{\sigma^2}{2}\right)T,\sigma^2T\right)$$
\end{lmm}

Rate of return: The percentage return averaged over a certain amount of time. 

\begin{lmm}{}{} Let $T$ be a set time (in years). Denote $S:\Omega\times[0,T]\to\R$ the stochastic processes that is the value of a certain stock. Assume the Black-Scholes-Merton model. Let $r$ be the continuously compounded rate of return of $S$. Then we have $$r\sim N\left(\mu-\frac{\sigma^2}{2},\frac{\sigma^2}{T}\right)$$
\end{lmm}

Volatility: a measure of uncertainty of return. So it is precisely the variance of the rate of return. Hence the annual volatility is $\sigma$. Checks out. 

\begin{prp}{}{} Let $T$ be a set time (in years). Denote $S:\Omega\times[0,T]\to\R$ the stochastic processes that is the value of a certain stock. Assume the Black-Scholes-Merton model. Then the European call option price according to the model is $$c=S_0 F_N(d_1)-Ke^{-rT}F_N(d_2)$$ where $K$ is the strike price, $r$ is the risk-free interest rate and $F_N$ is the cumulative distribution function of $N(0,1)$ and $$d_1=\frac{\ln(S_0/K)+(r+\sigma^2/2)T}{\sigma\sqrt{T}}\;\;\;\;\text{ and }\;\;\;\;d_2=\frac{\ln(S_0/K)+(r-\sigma^2/2)T}{\sigma\sqrt{T}}=d_1-\sigma\sqrt{T}$$
\end{prp}











\end{document}