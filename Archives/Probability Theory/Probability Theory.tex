\documentclass[a4paper]{article}

\input{C:/Users/liula/Desktop/Latex/Headers V1.2.tex}

\pagestyle{fancy}
\fancyhf{}
\rhead{Labix}
\lhead{Probability Theory}
\rfoot{\thepage}


\title{Probability Theory}

\author{Labix}

\date{\today}
\begin{document}
\maketitle
\begin{abstract}
Notes for the basics of Probability Theory. 
\end{abstract}
\tableofcontents
\pagebreak

\section{Foundations of Probability Theory}
\subsection{Definition of Probability}
\begin{defn}{Probability Space}{} A probability space is a triple $(\Omega,\mF,P)$ consisting of the following data: 
\begin{itemize}
\item $\Omega\neq\emptyset$ is a set called the sample space. 
\item $\mF\subseteq\mP(\Omega)$ is a $\sigma$-algebra called events.  \item $P:\mF\to[0,1]$ is a set function. 
\end{itemize}
such that the following are true: 
\begin{itemize}
\item $P(\Omega)=1$. 
\item If $\{A_n|n\in\N\}\subseteq\mF$ are pairwise disjoint, then $$P\left(\bigcup_{k=1}^\infty A_k\right)=\sum_{k=1}^{\infty}P(A_k)$$
\end{itemize}
\end{defn}

\begin{prp}{}{} Let $(\Omega,\mF,P)$ be a probability space. Let $A,B\in\mF$ be events. Then the following are true. 
\begin{itemize}
\item $P(\Omega\setminus A)=1-P(A)$
\item $A\subset B\implies P(A)\leq P(B)$
\end{itemize} \tcbline
\begin{proof} Let $A\subset B\subset\Omega$ be events in $\Omega$. 
\begin{itemize}
\item $A$ and $\Omega\setminus A$ are disjoint and $P(\Omega)=P(A)+P(\Omega\setminus A)$ and $P(\Omega\setminus A)=1-P(A)$
\item We have that $A$ and $B\setminus A$ are disjoint. Thus $P(B)=P(A)+P(B\setminus A)$. Since $P(B\setminus A)\geq 0$, we have $P(A)\leq P(B)$.  
\end{itemize}
\end{proof}
\end{prp}

\begin{defn}{Uniform Probability Measure}{} Let $\Omega$ be a sample space. A probability measure $P$ is uniform if fo all $a,b\in\Omega$, $$P(\{a\})=P(\{b\})$$
\end{defn}

\begin{thm}{}{} Let $\Omega$ be a sample space and $P$ a uniform probability measure of $\Omega$. Then for all $A\subset\Omega$, $$P(A)=\frac{\abs{A}}{\abs{\Omega}}$$
\end{thm}
\begin{proof} Suppose that $A$ consists of $\abs{A}$ distinct elements and the event space $\abs{\Omega}$ contains $\abs{\Omega}$ distinct elements. Since every singleton set is pairwise disjoint, we have $P(A)=\abs{A}P(\{a\})$ for any $a\in A$. Similarly, we have $P(\Omega)=\abs{\Omega}P(\{a\})$.  Thus we have that $P(A)=\frac{\abs{A}P(\Omega)}{\abs{\Omega}}$ and $P(A)=\frac{\abs{A}}{\abs{\Omega}}$
\end{proof}

\begin{thm}{Principle of Inclusion Exclusion}{} Let $A,B\subset\Omega$ be a sample space and $P$ the probability measure. $$P(A\cup B)=P(A)+P(B)-P(A\cap B)$$
\end{thm}
\begin{proof} Note that 
\begin{align*}
A\cup(B\setminus A)&=A\cup(B\cap A^c)\\
&=(A\cup B)\cap(A\cup A^c)\\
&=A\cup B
\end{align*} Note also that $A\cap (B\setminus A)=\emptyset$. Thus $P(A\cup B)=P(A)+P(B\setminus A)=P(A)+P(B)-P(A\cap B)$
\end{proof}

\begin{thm}{Extended Principle of Inclusion Exclusion}{} Let $A_k\subset\Omega$ be a sample space and $P$ the probability measure for all $k\leq n\in\mathbb{N}$. Then $$P\left(\bigcup_{k=1}^n A_k\right)=\sum_{k=1}^n(-1)^{k+1}\sum_{1\leq i_1\leq\dots\leq n}P(A_{i_1}\cap A_{i_2}\cap\dots\cap A_{i_k})$$
\end{thm}

\subsection{Multiplication Principle}
\begin{thm}{The Multiplication Principle}{} Suppose that Experiment A has $a$ outcomes and Experiment B has $b$ outcomes. Then the performing both $A$ and $B$ results in $ab$ possible outcomes. 
\end{thm}

\begin{thm}{Sampling with replacement - Ordered}{} In the case of sampling $k$ balls with replacement from an urn containing $n$ balls, there are $\abs{\Omega}=n^k$ possible outcomes when the order of the objects matters, where $\Omega=\{(s_1,\dots,s_k):s_i\in\{1,\dots,n\}\forall i\in\{1,\dots,k\}\}$. 
\end{thm}

\begin{thm}{Sampling without replacement - Ordered}{} In the case of sampling $k$ balls without replacement from an urn containing $n$ balls, there are $\abs{\Omega}=\frac{n!}{(n-k)!}$ possible outcomes when the order of the objects matters, where $\Omega=\{(s_1,\dots,s_k):s_i\in\{1,\dots,n\}\forall i\in\{1,\dots,k\},i\neq j\implies s_i\neq s_j\}$. 
\end{thm}

\begin{thm}{Sampling without replacement - Unordered}{} In the case of sampling $k$ balls without replacement from an urn containing $n$ balls, there are $\abs{\Omega}=\binom{n}{k}$ possible outcomes when the order of the objects does not matter, where $\Omega=\{\omega\subset\{1,\dots,n\}:\abs{\omega}=k\}$. 
\end{thm}

\begin{thm}{Sampling with replacement - Unordered}{} In the case of sampling $k$ balls with replacement from an urn containing $n$ balls, there are $\abs{\Omega}=\binom{n+k-1}{k}$ possible outcomes when the order of the objects does not matter, where $\Omega=\{\omega\subset\{1,\dots,n\}:\omega\text{ is a $k$ element multiset with elements from }\{1,\dots,n\}\}$. 
\end{thm}

\subsection{Conditional Probability}
\begin{defn}{Conditional Probability}{} Consider a probability space $(\Omega,P)$. Let $A,B\subset\Omega$ with $P(B)>0$. Then the conditional probability of $A$ given $B$, denoted by $P(A|B)$ is defined as $$P(A|B)=\frac{P(A\cap B)}{P(B)}$$
\end{defn}

\begin{thm}{Multiplication Rule}{} Let $n\in\mathbb{N}$. Then for any events $A_1,\dots,A_n$ such that $P(A_2\cap\dots\cap A_n)>0$, we have $$P(A_1\cap\dots\cap A_n)=P(A_1)P(A_2|A_1)P(A_3|A_1\cap A_2)\dots P(A_n|A_1\cap\dots\cap A_{n-1})$$
\end{thm}
\begin{proof} From the right hand side, we have
\begin{align*}
&P(A_1)P(A_2|A_1)P(A_3|A_1\cap A_2)\dots P(A_n|A_1\cap\dots\cap A_{n-1})\\
&=P(A_1)\frac{P(A_2\cap A_1)}{P(A_1)}\frac{P(A_3\cap A_2\cap A_1)}{P(A_2\cap A_1)}\dots \frac{P(A_n\cap\dots\cap A_1)}{P(A_1\cap\dots\cap A_{n-1})}\\
&=P(A_1\cap\dots\cap A_n)
\end{align*}
\end{proof}

\begin{thm}{Bayes' Rule}{} Let $(\Omega,P)$ be a probability measure. Let $A,B\subset\Omega$ with $P(A),P(B)>0$. Then $$P(A|B)=\frac{P(B|A)P(A)}{P(B)}$$
\end{thm}
\begin{proof} We have that $P(A\cap B)=P(A|B)P(B)$ and $P(A\cap B)=P(B|A)P(A)$. 
\end{proof}

\begin{thm}{Law of Total Probability}{} Let $(\Omega,P)$ be a probability measure. Let $A_1,\dots,A_n$ be a partition of $\Omega$ with $P(A_i)>0$ for all $i=1,\dots,n$. Then for any $B\subset\Omega$, $$P(B)=\sum_{k=1}^nP(A_k)P(B|A_k)$$
\end{thm}
\begin{proof} Note that since $A_1,\dots,A_n$ is a partition, $B\cap A_1,\dots,B\cap A_n$ is also a parition. 
\begin{align*}
\sum_{k=1}^nP(A_k)P(B|A_k)&=\sum_{k=1}^nP(B\cap A_k)\\
&=P\left(\bigcup_{k=1}^nB\cap A_k\right)\\
&=P(B\cap\Omega)\\
&=P(B)
\end{align*}
\end{proof}

\begin{thm}{General Bayes' Rule}{} Let $(\Omega,P)$ be a probability measure. Let $A_1,\dots,A_n$ be a partition of $\Omega$ with $P(A_i)>0$ for all $i=1,\dots,n$. Then for any $B\subset\Omega$ with $P(B)>0$, $$P(A_i|B)=\frac{P(B|A_i)P(A_i)}{P(B)}=\frac{P(B|A_i)P(A_i)}{\sum_{k=1}^nP(B|A_i)P(A_i)}$$
\end{thm}
\begin{proof} Apply Bayes' rule and apply the mulitplication rule. 
\end{proof}

\subsection{Independence of Events}
\begin{defn}{Independent Events}{} Two events $A,B$ are said to be independent if $$P(A\cap B)=P(A)P(B)$$
\end{defn}

\begin{prp}{}{} If $A,B$ are independent, then $A^c,B$, $A,B^c$ and $A^c,B^c$ are independent. 
\end{prp}
\begin{proof} We only proof the first and third item. 
\begin{itemize}
\item Without loss of generality we prove the first and reader mirrors the second. 
\begin{align*}
P(A^c\cap B)&=P(B)-P(A\cap B)\\
&=P(B)(1-P(A))\\
&=P(B)P(A^c)
\end{align*}
\item Note that $P(A\cap B)=P(A)P(B)$
\begin{align*}
P(A^c\cap B^c)&=1-P(A\cap B)\\
&=1-P(A)-P(B)+P(A\cap B)\\
&=1-P(A)-P(B)+P(A)P(B)\\
&=(1-P(A))(1-P(B))\\
&=P(A^c)P(B^c)
\end{align*}
\end{itemize}
\end{proof}

\pagebreak
\section{Distributions}
\subsection{Random Variables}
\begin{defn}{Random Variable}{} Let $(\Omega,\mathcal{F},P)$ be a probability space. Let $(E,\mathcal{E})$ be a measurable space. An $(E,\mathcal{E})$ valued random variable is an $\mF$-measurable function $X:\Omega\to E$. 
\end{defn}

Recall that $X$ is an $\mF$-measurable function if $X^{-1}(B)\in\mF$ for $B\in\mE$. 

\begin{defn}{Probability Distribution}{} Let $(\Omega,E,\Prj)$ be a probability space. Let $(E,\mathcal{E})$ be a measurable space. Let $X:\Omega\to E$ be a measurable function. Define the probability distribution of $X$ to be the function $$P(X\in A)=P(X^{-1}(A))$$ for $A\in\mE$. 
\end{defn}

\begin{defn}{Discrete Random Variable}{} A discrete random variable on the probability space $(\Omega,E,\Prj)$ is a random variable such that $\im(X)=\{X(\omega):\omega\in\Omega\}$ is a countable subset of $\mathbb{R}$. 
\end{defn}

\begin{defn}{Continuous Random Variable}{} A continuous random variable on a probability space $(\Omega,E,\Prj)$ is a random variable if there exists a non-negative function $f_X:\R\to\R$ such that $$\Prj_X(A)=\Prj_X(X^{-1}(A))=\int_Af_X(r)\,dr$$
\end{defn}

\subsection{Probability Distribution}
\begin{defn}{Probability Mass Function}{} The probability mass function of the discrete random variable $X$ is defined as the function $p_X:\mathbb{R}\to[0,1]$ given by $$p_X(x)=\Prj(\{\omega\in\Omega:X(\omega)=x\})=\Prj(X^{-1}(x))=\Prj(X=x)$$
\end{defn}

\begin{lmm}{}{} Let $X$ be a random variable and $p_X$ the probability mass function. Then $$\sum_{x\in\im(X)}p_X(x)=1$$
\end{lmm}
\begin{proof} It is on a probability space. 
\end{proof}

\begin{defn}{Cumulative Distribution Function}{} Suppose that $X$ is a random variable on a probability space $(\Omega,E,\Prj)$. Then the cumulative distribution function of $X$ is defined as the mapping $F_X:\mathbb{R}\to[0,1]$ given by $$F_X(x)=\Prj(\{\omega\in\Omega:X(\omega)\leq x\})=\Prj(X^{-1}(-\infty,x])=\Prj_X(X\leq x)$$
\end{defn}

\begin{lmm}{}{} If $X$ is a discrete random variable, then the cummulative distribution function of $X$ is $$F_X(x)=\sum_{k\leq x}p_X(k)$$ If $X$ is a continuous random variable, then the cummulative distribution function of $X$ is $$F_X(t)=\int_{-\infty}^tf(x)\,dx$$
\end{lmm}







\begin{thm}{}{} A comparison of the two probability functions with discrete random variable and continuous random variable. 
\begin{center}
\begin{tabular}{ |c|c|c| } 
 \hline
  & Discrete & Continuous \\ \hline
 Probability Function & $p_X(x)\geq0$ for all $x\in\mathbb{R}$ & $f_X(x)\geq0$ for all $x\in\mathbb{R}$ \\ \hline
 Cumulative Probability & $\sum_{x\in\im(X)}p_X(x)=1$ & $\int_{-\infty}^{\infty}f_X(x)dx=1$ \\ \hline
 Cumulative Distribution Function & $F_X(x)=\sum_{x\in\im(X):u\leq x}p_X(u)$ & $F_X(x)=\int_{-\infty}^{x}f_X(u)\, du$ \\ 
 \hline
\end{tabular}
\end{center}
\end{thm}


\begin{thm}{}{} Suppose that $X$ is a random variable on a probability space $(\Omega,E,\Prj)$ with cumulative distribution function $F_X$. 
\begin{itemize}
\item $F_X$ is monotonically non-decreasing. $x\leq y\implies F_X(x)\leq F_X(y)$
\item $F_x$ is right continuous. If $(x_n)$ is a sequence such that $x_1\geq\dots\geq x_n\geq x_{n+1}\geq\dots\geq x$ and $(x_n)\to x$, then $F_X(x_n)\to F_X(x)$
\item $F_X(-\infty)=0$ and $F_X(\infty)=1$
\end{itemize}
\end{thm}


\begin{thm}{}{} Suppose that $X$ is a random variable on a probability space $(\Omega,E,\Prj)$ with cumulative distribution function $F_X$. If $a<b$, then $\Prj(a<X\leq b)=F_X(b)-F_X(a)$
\end{thm}

\begin{thm}{}{} For a continuous random variable $X$ iwth density $f_X$, we have 
\begin{itemize}
\item $\Prj(X=x)=0$ for all $x\in\mathbb{R}$
\item $\Prj(a\leq x\leq b)=\int_{a}^{b}f_X(u)du$ for all $a,b\in\mathbb{R}$ with $a\leq b$
\end{itemize}
\end{thm}







\subsection{Common Discrete Distribution}
\begin{defn}{Bernoulli Distribution}{} A discrete random variable $X$ is said to have Bernoulli Distribution with parameter $p\in(0,1)$ if $\im(X)=\{0,1\}$ and $p_X(1)=p$ and $p_X(0)=1-p$. 
\end{defn}

\begin{defn}{Binomial Distribution}{} A discrete random variable $X$ is said to have Binomial Distribution with parameters $n\in\mathbb{N}$ and $p\in(0,1)$ if $\im(X)=\{0,1,\dots,n\}$ and $$p_X(x)=\binom{n}{x}p^x(1-p)^{n-x}$$
\end{defn}

\begin{thm}{Bernoulli's Weak Law of Large Numbers}{} Let $p\in(0,1)$. For all $n\in\mathbb{N}$, let $X_n$ have the binomial distribution with parameters $n,p$. Then for any $\epsilon>0$, $$\lim_{n\to\infty}P\left(\abs{\frac{X_n}{n}-p}>\epsilon\right)=0$$ In other words, for any $\epsilon>0$, $$\lim_{n\to\infty}P(np-n\epsilon\leq X_n\leq np+n\epsilon)=1$$
\end{thm}

\begin{defn}{Poisson Distribution}{} A discrete random variable $X$ is said to have Poisson Distribution with parameter $\lambda>0$ if $\im(X)=\mathbb{N}_0$ and $$p_X(x)=\frac{\lambda^x}{x!}e^{-x}$$
\end{defn}

\begin{thm}{Poisson Approximation}{} Let $(p_n)$ be a sequence with $p_n\in[0,1]$ for all $n$ and $\lim_{n\to\infty}np_n=\lambda>0$. Denote $X\approx$Poisson$(\lambda)$ and $X_n\approx$Bin$(n,p_n)$. Then for every $x\in\mathbb{N}$, $$\lim_{n\to\infty}p_{X_n}(x)=p_X(x)$$
\end{thm}
\begin{proof}
\begin{align*}
\lim_{n\to\infty}p_{X_n}(x)&=\lim_{n\to\infty}\binom{n}{x}(p_n)^x(1-p_n)^{n-x}\\
&=\lim_{n\to\infty}\frac{n!}{x!(n-x)!}(p_n)^x(1-p_n)^{n-x}\\
&=\lim_{n\to\infty}\frac{n!}{x!(n-x)!}\left(\frac{\lambda}{n}\right)^x\left(1-\frac{\lambda}{n}\right)^{n-x}\\
&=\lim_{n\to\infty}\frac{n!}{x!(n-x)!}\left(\frac{\lambda}{n}\right)^x\left(1-\frac{\lambda}{n}\right)^n\left(1-\frac{\lambda}{n}\right)^{-x}\\
&=\lim_{n\to\infty}\frac{n!}{(n-x)!}\left(\frac{1}{n^k}\right)\left(\frac{\lambda^x}{x!}\right)\left(1-\frac{\lambda}{n}\right)^n\left(1-\frac{\lambda}{n}\right)^{-x}\\
&=\frac{\lambda^x}{x!}e^{-x}
\end{align*}
\end{proof}

\begin{defn}{Geometric Distribution}{} A discrete random variable $X$ is said to have Geometric Distribution with parameter $p\in(0,1)$ if $\im(X)=\mathbb{N}_0$ and $$p_X(x)=p(1-p)^{x-1}$$
\end{defn}


\subsection{Common Continuous Distributions}
\begin{defn}{Uniform Distribution}{} A conitnuous random variable $X$ is said to have Uniform Distribution on the interval $(a,b)$ if its density function is given by $$f_X(x)=\begin{cases}
\frac{1}{b-a} & \text{if $a<x<b$}\\
0 & \text{otherwise}
\end{cases}$$ and its cumulative function given by $$F_X(x)=\begin{cases}
0 & \text{if $x\leq a$}\\
\frac{x-a}{b-a} & \text{if $a<x<b$}\\
1 & \text{if $x\geq b$}
\end{cases}$$
\end{defn}

\begin{defn}{Exponential Distribution}{} A conitnuous random variable $X$ is said to have Exponential Distribution with parameter $\lambda>0$ if its density function is given by $$f_X(x)=\begin{cases}
\lambda e^{-\lambda x} & \text{if $x>0$}\\
0 & \text{otherwise}
\end{cases}$$ and its cumulative function given by $$F_X(x)=\begin{cases}
0 & \text{if $x\leq 0$}\\
1-e^{-\lambda x} & \text{if $x>0$}
\end{cases}$$
\end{defn}

\begin{defn}{Gamma Distribution}{} A conitnuous random variable $X$ is said to have Gamma Distribution with shape parameter $\alpha>0$ and rate parameter $\beta>0$ if its density function is given by $$f_X(x)=\begin{cases}
\frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x} & \text{if $x>0$}\\
0 & \text{otherwise}
\end{cases}$$
\end{defn}

\begin{defn}{Normal Distribution}{} A conitnuous random variable $X$ is said to have Normal Distribution with parameters $\mu\in\mathbb{R}$ and $\sigma>0$ if its density function is given by $$f_X(x)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$ for all $x\in\mathbb{R}$. 
\end{defn}

\begin{thm}{De Moivre-Laplace}{} Let $X_n$ denote the binomial distrbution with parameters $n,p$ for all $n\in\mathbb{N}$. For all $-\infty\leq z_1<z_2\leq \infty$, $$\lim_{n\to\infty}P\left(np+z_1\sqrt{np(1-p)}\leq X_n\leq np+z_2\sqrt{np(1-p)}\right)=\int_{z_1}^{z_2}\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}\, dx$$
\end{thm}

\subsection{Transformation of Random Variables}
\begin{thm}{}{} Let $X$ be a discrete random variable on $(\Omega,P)$ and let $g:\mathbb{R}\to\mathbb{R}$ denote a deterministic function. Then $Y=g(X)$ is a discrete random variable with probability mass function given by $$p_Y(y)=\sum_{x\in\im(X):g(x)=y}P(X=x)$$ for all $y\in\im(Y)$ and $0$ otherwise. 
\end{thm}

\begin{thm}{}{} Suppose that $X$ is a continuous random variable with density $f_X$ and $g:\mathbb{R}\to\mathbb{R}$ is strictly increasing or decreasing and differentiable with inverse function denoted $g^{-1}$, then $Y=g(X)$ has density $$f_Y(y)=f_X(g^{-1}(y))\abs{\frac{d}{dy}(g^{-1}(y))}$$ for all $y\in\mathbb{R}$
\end{thm}

\subsection{Multivariate Random Variables}
\begin{defn}{Joint Probability Mass Function}{} Let $X,Y$ be discrete random variables. The joint probability mass function of $X$ and $Y$ is the function $$p_{X,Y}(x,y)=P(\{\omega\in\Omega:X(\omega)=x,Y(\omega)=y\})=P((X,Y)=(x,y))$$ for all $(x,y)\in\mathbb{R}^2$
\end{defn}

\begin{thm}{}{} Let $p_{X,Y}$ be the joint probability mass function of two random variables $X,Y$. 
\begin{itemize}
\item $p_X(x)=\sum_{y}p_{X,Y}(x,y)$
\item $p_Y(y)=\sum_{x}p_{X,Y}(x,y)$
\end{itemize}
\end{thm}

\begin{defn}{Joint Cumulative Distribution Function}{} Let $X,Y$ be random variables. The joint cumulative distribution function of $X$ and $Y$ is the function $$F_{X,Y}(x,y)=P(\{\omega\in\Omega:X(\omega)\leq x,Y(\omega)\leq y\})=P(X\leq x,Y\leq y)$$ for all $(x,y)\in\mathbb{R}^2$
\end{defn}

\begin{thm}{}{} Let $F_{X,Y}$ be the joint cumulative distribution function of two random variables $X,Y$. 
\begin{itemize}
\item $\lim_{x,y\to-\infty}F_{X,Y}(x,y)=0$
\item $\lim_{x,y\to\infty}F_{X,Y}(x,y)=1$
\item $x\leq x'$ and $y\leq y'$ implies $F_{X,Y}(x,y)\leq F_{X,Y}(x',y')$
\item $F_X(x)=\lim_{y\to\infty}F_{X,Y}(x,y)$
\item $F_Y(x)=\lim_{x\to\infty}F_{X,Y}(x,y)$
\end{itemize}
\end{thm}

\begin{defn}{Jointly Continuous}{} Let $X,Y$ be random variables. $X$ and $Y$ are jointly continuous if $$F_{X,Y}(x,y)=\int_{-\infty}^{x}\int_{-\infty}^{y}f_{X,Y}(u,v)\,dv\,du$$ for a function $f_{X,Y}:\mathbb{R}^2\to\mathbb{R}^2$ satisfying
\begin{itemize}
\item $f_{X,Y}(u,v)\geq 0$
\item $\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f_{X,Y}(u,v)\,dv\,du=1$
\end{itemize}
We call $f_{X,Y}$ the joint density function of $(X,Y)$. 
\end{defn}

\begin{thm}{}{} Let $F_{X,Y}$ be the joint cumulative distribution function of two random variables $X,Y$. 
\begin{itemize}
\item $f_{X,Y}(x,y)=\begin{cases}
\frac{\partial^2}{\partial x\partial y}F_{X,Y}(x,y)&\text{if the derivative exists at $(x,y)$}\\
0&\text{otherwise}
\end{cases}$
\item $f_X(x)=\int_{-\infty}^{\infty}f_{X,Y}(x,y)\,dy$
\item $f_Y(x)=\int_{-\infty}^{\infty}f_{X,Y}(x,y)\,dx$
\end{itemize}
\end{thm}

\begin{defn}{}{} Two random variables $X,Y$ defined in the same probability space are said to be independent if $$P(X\leq x, Y\leq y)=P(X\leq x)P(Y\leq y)$$
\end{defn}

\begin{thm}{}{} Two random variables $X,Y$ defined in the same probability space are said to be independent if $$F_{X,Y}(x,y)=F_X(x)F_Y(y)$$
\end{thm}

\begin{thm}{}{} Two discrete random variables $X,Y$ defined in the same probability space are said to be independent if $$p_{X,Y}(x,y)=p_X(x)p_Y(y)$$
\end{thm}

\begin{thm}{}{} Two continuous random variables $X,Y$ defined in the same probability space are said to be independent if $$f_{X,Y}(x,y)=f_X(x)f_Y(y)$$
\end{thm}

\subsection{Distribution of Sums}
\begin{thm}{Sum of Discrete Random Variables}{} Let $X,Y$ be discrete random variables with density function $p_{X,Y}$. Then $Z=X+Y$ has density function given by $$p_Z(m)=\sum_{k\in\mathbb{Z}}p_{X,Y}(k,m-k)$$ In particular, if $X,Y$ are independent, then $$p_Z(m)=\sum_{k\in\mathbb{Z}}p_X(k)p_Y(m-k)$$
\end{thm}

\begin{prp}{}{} Let $X\approx$Poi$(\lambda)$ and $Y\approx$Poi$(\mu)$ be independent. $X+Y\approx$Poi$(\lambda+\mu)$. 
\end{prp}
\begin{proof} 
\begin{align*}
p_{X+Y}(m)&=\sum_{k\in\Z}\frac{\lambda^k}{k!}e^{-k}\frac{\mu^{m-k}}{(m-k)!}e^{k-m}\\
&=\frac{1}{m!}e^{-m}\sum_{k=0}^mm!\frac{\lambda^k}{k!}\frac{\mu^{m-k}}{(m-k)!}\\
&=\frac{1}{m!}e^{-m}\sum_{k=0}^m\binom{m}{k}\lambda^k\mu^{m-k}\\
&=\frac{(\lambda+\mu)^m}{m!}e^{-m}
\end{align*}
\end{proof}

\begin{prp}{}{} Let $X_1,\dots,X_n\approx$Bern$(p)$ be independent. $X_1+\dots+X_n\approx$Bin$(n,p)$. 
\end{prp}
\begin{proof} We prove by induction. When $n=2$,
\begin{align*}
p_{X_1+X_2}(0)&=p_{X_1}(0)p_{X_2}(0)\\
&=1-2p+p^2\\
p_{X_1+X_2}(1)&=p_{X_1}(0)p_{X_2}(1)+p_{X_1}(1)p_{X_2}(0)\\
&=(1-p)(p)+p(1-p)\\
&=2p(1-p)\\
p_{X_1+X_2}(2)&=p_{X_1}(0)p_{X_2}(2)+p_{X_1}(1)p_{X_2}(1)+p_{X_1}(2)p_{X_2}(0)\\
&=p^2\\
p_{\text{Bin}(2,p)}(x)&=\binom{2}{x}p^x(1-p)^{n-x}\\
\end{align*} For $x\in\{0,1,2\}$, the two probability density functions match thus for the case $n=2$, it is true. Now suppose that $X_1+\dots+X_{n-1}\approx$Bin$(n-1,p)$. Let $Y=$Bin$(n-1,p)+X_n$. For $m\in\{0,\dots,n\}$,
\begin{align*}
p_Y(m)&=\sum_{k\in\Z}p_{\text{Bin}(n-1,p)}(k)p_{X_n}(m-k)\\
&=\sum_{k=0}^mp_{\text{Bin}(n-1,p)}(k)p_{X_n}(m-k)\\
&=\sum_{k=0}^m\binom{n-1}{k}p^k(1-p)^{n-1-k}p_{X_n}(m-k)\\
&=\sum_{k=m-1}^m\binom{n-1}{k}p^k(1-p)^{n-1-k}p_{X_n}(m-k)\\
&=\binom{n-1}{m-1}p^{m-1}(1-p)^{n-m}p_{X_n}(1)+\binom{n-1}{m}p^m(1-p)^{n-1-m}p_{X_n}(0)\\
&=\binom{n-1}{m-1}p^{m}(1-p)^{n-m}+\binom{n-1}{m}p^m(1-p)^{n-m}\\
&=\binom{n}{m}p^{m}(1-p)^{n-m}
\end{align*} Thus for the case $X_1+\dots+X_n$ it is true. 
\end{proof}

\begin{prp}{}{} Let $X\approx$Bin$(m,p)$ and $Y\approx$Bin$(n,p)$ be independent. $X+Y\approx$Bin$(m+n,p)$. 
\end{prp}
\begin{proof}
\begin{align*}
p_{X+Y}(t)&=\sum_{k\in\Z}p_X(k)p_Y(t-k)\\
&=\sum_{k=0}^t\binom{m}{k}p^k(1-p)^{m-k}\binom{n}{t-k}p^{t-k}(1-p)^{n-t+k}\\
&=\sum_{k=0}^t\binom{m}{k}\binom{n}{t-k}p^t(1-p)^{m+n-t}\\
&=p^t(1-p)^{m+n-t}\sum_{k=0}^t\frac{m!}{k!(m-k)!}\frac{n!}{(t-k)!(n-t+k)!}\\
\end{align*}
\end{proof}

\begin{thm}{Sum of Independent Continuous Random Variables}{} Let $X,Y$ be independent continuous random variables with density function $f_X$ and $f_Y$ respectively. Then $Z=X+Y$ has density function given by $$f_Z(z)=\int_{-\infty}^{\infty}f_X(x)f_Y(z-x)\,dx=\int_{-\infty}^{\infty}f_X(z-y)f_Y(y)\,dy$$
\end{thm}

\begin{prp}{}{} Let $\lambda>0$. Let $n\in\mathbb{N}$. Let $T_1,\dots,T_n$ be independent random variables with exponential distribution parameter $\lambda$. Then $$Z=\sum_{k=1}^nT_k\approx\text{Gamma}(n,\lambda)$$
\end{prp}
\begin{proof} We prove by induction. When $n=2$, 
\begin{align*}
f_Z(z)&=\int_{-\infty}^\infty f_{T_1}(x)f_{T_2}(z-x)\,dx\\
&=\int_0^z\lambda e^{-\lambda x}\lambda e^{-\lambda(z-x)}\,dx\\
&=\lambda^2e^{-\lambda z}\int_0^z\,dx\\
&=\lambda^2ze^{-\lambda z}
\end{align*} Thus the case $n=2$ is true. Suppose that it is true for $n=k-1$. Let $X\approx$Gamma$(n-1,\lambda)$. 
\begin{align*}
f_Z(z)&=\int_{-\infty}^\infty f_X(x)f_{T_n}(z-x)\,dx\\
&=\int_0^z\frac{\lambda^{n-1}}{\Gamma(n-1)}x^{n-2}e^{-\lambda x}\lambda e^{-\lambda(z-x)}\,dx\\
&=\frac{\lambda^{n}}{\Gamma(n-1)}e^{-\lambda z}\int_0^zx^{n-2}\,dx\\
&=\frac{\lambda^{n}}{\Gamma(n-1)}e^{-\lambda z}\frac{1}{n-1}z^{n-1}\\
&=\frac{\lambda^{n}}{\Gamma(n)}z^{n-1}e^{-\lambda z}\\
\end{align*} Thus we are done
\end{proof}

\begin{prp}{}{} Let $m,n\in\mathbb{N}$ and $\lambda>0$. Let $X\approx$Gamma$(m,\lambda)$ and $Y\approx$Gamma$(n,\lambda)$ be independent. $X+Y\approx$Gamma$(m+n,\lambda)$. 
\end{prp}
\begin{proof}
\begin{align*}
f_Z(z)&=\int_{-\infty}^\infty f_X(x)f_Y(z-x)\,dx\\
&=\int_0^z\frac{\lambda^{m}}{\Gamma(m)}x^{m-1}e^{-\lambda x}\frac{\lambda^{n}}{\Gamma(n)}(z-x)^{n-1}e^{-\lambda(z-x)}\,dx\\
&=\frac{\lambda^{m+n}}{\Gamma(m)\Gamma(n)}e^{-\lambda z}\int_0^zx^{m-1}(z-x)^{n-1}\,dx\\
&=\frac{\lambda^{m+n}}{\Gamma(m)\Gamma(n)}e^{-\lambda z}\int_0^zx^{m-1}\sum_{k=0}^{n-1}\binom{n-1}{k}z^{n-1-k}(-x)^k\,dx\\
&=\frac{\lambda^{m+n}}{\Gamma(m)\Gamma(n)}e^{-\lambda z}\sum_{k=0}^{n-1}\binom{n-1}{k}z^{n-1-k}(-1)^k\int_0^zx^{m-1+k}\,dx\\
&=\frac{\lambda^{m+n}}{\Gamma(m)\Gamma(n)}z^{m+n-1}e^{-\lambda z}\sum_{k=0}^{n-1}\binom{n-1}{k}(-1)^k\frac{1}{m+k}\\
\end{align*}
\end{proof}

\begin{thm}{}{} Suppose that $T_1,T_2,\dots$ are independent random variables with exponential distribution parameter $\lambda$. Define for $t\geq0$, $$N_t=\begin{cases}
0&\text{if $T_1>t$}\\
1&\text{if $T_1\leq t<T_1+T_2$}\\
2&\text{if $T_1+T_2\leq t<T_1+T_2+T_3$}\\
\cdots\\
\end{cases}$$
Then, for any $t\geq0$, we have that $N_t\approx$Poi$(\lambda t)$. 
\end{thm}

\begin{defn}{Poisson Process}{} The family of random variables $\{N_t:t\geq 0\}$ is said to be Poisson process of intensity $\lambda$ if 
\begin{itemize}
\item $N_0=0$
\item for any $t_0,\dots,t_n$ with $0=t_0<t_1<t_2<\dots<t_n$, the random variables $N_{t_1}$, $N_{t_2}-N_{t_1}$, $N_{t_3}-N_{t_2}$, $\dots$, $N_{t_n}-N_{t_{n-1}}$ are independent, and $N_{t_i}-N_{t_{i-1}}\approx$Poi$(\lambda(t_i-t_{i-1}))$
\end{itemize}
\end{defn}

\pagebreak
\section{Expectation and Variance}
\subsection{Expectations}
\begin{defn}{Expectation Random Variables}{} Let $X$ be a random variable. The expectation of $X$ is defined as 
\begin{itemize}
\item $E(X)=\sum_{x\in\im(X)}x\Prj_X(X=x)$ if $X$ is discrete
\item $E(X)=\int_{-\infty}^{\infty}xf_X(x)\,dx$ if $X$ is continuous
\end{itemize}
\end{defn}

\begin{prp}{}{} Let $X$ be a random variable and $g:\mathbb{R}\to\mathbb{R}$. 
\begin{itemize}
\item If $X$ is discrete then $E(g(X))=\sum_{x\in\im(X)}g(x)p_X(x)$ whenever it converges absolutely
\item If $X$ is continuous then $E(g(X))=\int_{-\infty}^{\infty}g(x)f_X(x)\,dx$ whenever it converges absolutely
\end{itemize}
\end{prp}

\begin{prp}{Law of the Unconscious Staticians}{} Let $X_1,\dots,X_n$ be random variables and $g:\mathbb{R}^n\to\mathbb{R}$ be a function. 
\begin{itemize}
\item If $X_1,\dots,X_n$ are discrete, then $$E(g(X_1,\dots,X_n))=\sum_{\substack{x_1\in\im(X_1)\\\vdots\\x_n\in\im(X)}}g(x_1,\dots,x_n)p_X(x_1,\dots,x_n)$$
\item If $X_1,\dots,X_n$ are continuous, then $$E(g(X_1,\dots,X_n))=\int_{\R^n}g(x_1,\dots,x_n)f_X(x_1,\dots,x_n)\,dx_1\cdots\,dx_n$$
\end{itemize}
\end{prp}

\begin{prp}{}{} Assume that all the random variables that appear in the following statements have a well defined expectation. 
\begin{itemize}
\item Suppose that $X$ is a random variable such that there exists a constant $c$ such that $P(X=c)=1$. Then $$E(X)=c$$
\item If $X,Y$ are random variables and $a,b\in\mathbb{R}$, then $$E(aX+bY)=aE(X)+bE(Y)$$
\item If $X$ is a random variable such that $P(X\geq 0)=1$, then $E(X)\geq 0$. If $P(X>0)>0$, then $E(X)>0$. If $X_1$ and $X_2$ are random variables with $P(X_1\geq X_2)=1$, then $E(X_1)\geq E(X_2)$
\item If $X,Y$ are independent random variables, then $$E(XY)=E(X)E(Y)$$
\end{itemize}
\end{prp}

\begin{thm}{}{} Two random variables $X,Y$ are independent if and only if $$E(g(X)h(Y))=E(g(X))E(h(Y))$$ for any two functions $g,h:\mathbb{R}\to\mathbb{R}$ for which the expectation exists. 
\end{thm}

\subsection{Variance}
\begin{defn}{Variance}{} Let $X$ be a random variable. The variance of $X$ is defined as $$\vari(X)=E((X-E(X))^2)$$ whenever the expectation exists. The standard deviation of $X$ is defined as $\sigma_X=\sqrt{\vari(X)}$ whenever the variance exists
\end{defn}

\begin{thm}{}{} Let $X$ be a random variable whose variance is well defined. 
\begin{itemize}
\item $\vari(X)\geq0$
\item $\vari(X)=0$ if and only if $P(X=E(X))=1$
\item $\vari(X)=E(X^2)-E(X)^2$
\end{itemize}
\end{thm}

\begin{prp}{}{} Let $X$ be a random variable whose variance is well defined. $$\vari(aX+b)=a^2\vari(X)$$ for all $a,b\in\mathbb{R}$. 
\end{prp}

\begin{thm}{}{} Suppose that $X_1,\dots,X_n$ are independent variables with finite variance. Then $$\vari\left(\sum_{k=1}^nX_k\right)=\sum_{k=1}^n\vari(X_k)$$
\end{thm}

\begin{thm}{}{} A list of expectations and variances of different distributions. \begin{center}
\begin{tabular}{ |c|c|c| } 
\hline
$X$&$E(X)$&$\vari(X)$\\\hline
Ber$(p)$&$p$&$p(1-p)$\\\hline
Bin$(n,p)$&$np$&$np(1-p)$\\\hline
Poi$(\lambda)$&$\lambda$&$\lambda$\\\hline
Geo$(p)$n&$\frac{1}{p}$&$\frac{1-p}{p^2}$\\\hline
Uni$(a,b)$&$\frac{a+b}{2}$&$\frac{1}{12}(b-a)^2$\\\hline
Exp$(\lambda)$&$\frac{1}{\lambda}$&$\frac{1}{\lambda}$\\\hline
Gamma$(\alpha,\beta)$&$\frac{\alpha}{\beta}$&$\frac{\alpha}{\beta^2}$\\\hline
Norm$(\mu,\sigma^2)$&$\mu$&$\sigma^2$\\\hline
\end{tabular}
\end{center}
\end{thm}

\subsection{Covariance}
\begin{defn}{Covariance}{} Let $X,Y$ be two random variables. The covariance of $X,Y$ is defined as $$\cov(X,Y)=E[(X-E(X))(Y-E(Y))]$$
\end{defn}

\begin{prp}{}{} Suppose that $X,Y$ are random variables. 
\begin{itemize}
\item $\cov(X,Y)=\cov(Y,X)$
\item $\cov(X,X)=\vari(X)$
\item $\cov(X,Y)=E(XY)-E(X)E(Y)$
\item If $X,Y$ are independent, $\cov(X,Y)=0$
\item $\cov(aX+bY,Z)=a\cov(X,Z)+b\cov(Y,Z)$
\end{itemize}
\end{prp}

\begin{prp}{Variance of Sums}{} For random variables $X_1,\dots,X_n$, we have $$\vari\left(\sum_{i=1}^{n}X_i\right)=\sum_{i=1}^n\vari\left(X_i\right)+2\sum_{1\leq i<j\leq n}\cov\left(X_i,X_j\right)$$
\end{prp}

\begin{thm}{}{} Given two random variables $X$ and $Y$, we have $$\abs*{\cov(X,Y)}\leq\sqrt{\vari(X)\vari(Y)}$$
\end{thm}

\begin{thm}{Correlation Coefficient}{} The correlation coefficient between two random variables $X$ and $Y$ is given by $$\rho(X,Y)=\frac{\cov(X,Y)}{\sqrt{\vari(X)\vari(Y)}}$$
\end{thm}

\begin{prp}{}{} Let $X$ and $Y$ be random variables. We have $$-1\leq\rho(X,Y)\leq 1$$ Moreover, for any $a,b,c,d\in\mathbb{R}$ with $a,c>0$, we have $$\rho(aX+b,cY+d)=\rho(X,Y)$$
\end{prp}

\begin{prp}{}{} Let $X$, $Y$ be random variables. 
\begin{itemize}
\item $\rho(X,X)=1$
\item $\rho(X,-X)=-1$
\item $X,Y$ are uncorrelated if $\rho(X,Y)=0$
\end{itemize}
\end{prp}

\subsection{Moments}
\begin{defn}{$k$th Moment}{} Let $X$ be a random variable. For $k\in\mathbb{N}$ we define the $k$th  moment of $X$ as $E[X^k]$ whenever the expectation exists. 
\end{defn}

\begin{defn}{Moment Generating Function}{} The moment-generating function of a random variable $X$ is the function $M_X$ defined as $$M_X(t)=E[e^{tX}]$$ for all $t\in\mathbb{R}$ for which the expectation is well defined. 
\end{defn}

\begin{thm}{}{} Assume that $M_X$ exists in a neighbourhood of $0$, that is, there exists $\epsilon>0$ such that for all $t\in(-\epsilon,\epsilon)$ we have $M_X(t)<\infty$. Then for all $k\in\mathbb{N}$ the $k$th moment of $X$ exists, and $$E[X^k]=\frac{d^k}{dt^k}M_X(t)\bigg|_{t=0}$$
\end{thm}
\begin{proof} We have that $E[X^k]=\int_{-\infty}^\infty x^kf_X(x)\,dx$ for any continuous cummulative probability. On the other hand, 
\begin{align*}
\frac{d^k}{dt^k}M_X(t)\bigg|_{t=0}&=\frac{d^k}{dt^k}\int_{-\infty}^\infty e^{tx}f_X(x)\,dx\bigg|_{t=0}\\
&=\int_{-\infty}^\infty\frac{\partial^k}{\partial t^k}e^{tx}f_X(x)\,dx\bigg|_{t=0}\\
&=\int_{-\infty}^\infty x^ke^{tx}f_X(x)\,dx\bigg|_{t=0}\\
&=\int_{-\infty}^\infty x^kf_X(x)\,dx\\
\end{align*}
\end{proof}

\begin{prp}{}{} Assume that all expectations in the statement are well defined. 
\begin{itemize}
\item For any $a,b\in\mathbb{R}$, $M_{aX+b}(t)=e^{tb}M_X(at)$
\item If $X$, $Y$ are independent, then $M_{X+Y}(t)=M_X(t)M_Y(t)$
\end{itemize}
\end{prp}

\begin{thm}{}{} Let $X$, $Y$ be two random variables. Assume that the moment generating functions of $X$, $Y$ exists and are finite on an interval of the form $(-\epsilon,\epsilon)$. Assume further that $M_X(t)=M_Y(t)$ for all $t\in(-\epsilon,\epsilon)$. Then $X$, $Y$ have the same distribution. 
\end{thm}

\begin{thm}{}{} Let $X$ be a non-negative random variable whose expectation is well defined. We then have $$P(X\geq x)\leq\frac{E(X)}{x}$$
\end{thm}

\begin{thm}{}{} Let $X$ be a random variable whose variance is well defined. Then $$P(\abs{X-E(X)}\geq x)\leq\frac{\vari(X)}{x^2}$$ for all $x>0$
\end{thm}

\pagebreak
\section{Convergence of Random Variables}
\subsection{Convergence}
\begin{defn}{Convergence in Mean Square}{} We say that a sequence of random variables $X_1,X_2,\dots$ converges in mean square to a random variable $X$ if $$\lim_{n\to\infty}E[(X_n-X)^2]=0$$
\end{defn}

\begin{defn}{Convergence in Probability}{} We say that a sequence of random variables $X_1,X_2,\dots$ converges in probability to a random variable $X$ if for every $\epsilon>0$, we have $$\lim_{n\to\infty}P(\abs{X_n-X}>\epsilon)=0$$
\end{defn}

\begin{thm}{}{} Let $X_1,X_2,\dots$ be a sequence of random variables, and $X$ another random variable. If $X_n\to X$ in mean square as $n\to\infty$ then $X_n\to X$ in probability as $n\to\infty$. 
\end{thm}

\begin{defn}{Convergence in Distribution}{} We say that a sequence of random variables $X_1,X_2,\dots$ converges in distribution to a random variable $X$ if $$\lim_{n\to\infty}F_{X_n}(x)=F_X(x)$$ for every $x$ in the set $C=\{x\in\mathbb{R}:F_X\text{ is continuous at } x\}$. 
\end{defn}

\begin{thm}{}{} For any random variable $X$, the set of discontinuity points of $F_X$ is countable. 
\end{thm}

\begin{thm}{}{} Let $X_1,X_2,\dots$ be a sequence of random variables, and $X$ another random variable. If $X_n\to X$ in probability, then $X_n\to X$ in distribution. 
\end{thm}

\begin{thm}{}{} Let $X_1,X_2,\dots$ be a sequence of random variables such that $X_n\to c$ in distribution, where $c\in\mathbb{R}$, then $X_n$ converges in probability to $c$. 
\end{thm}

\begin{thm}{Law of large numbers in mean square}{} Let $X_1,X_2,\dots$ be a sequence of independent random variable, each with mean $\mu$ and variance $\sigma^2$. Then $$\lim_{n\to\infty}\frac{X_1+\dots+X_n}{n}\to\mu$$ in mean square. 
\end{thm}

\begin{thm}{Weak law of large numbers}{} Let $X_1,X_2,\dots$ be a sequence of independent random variable, each with mean $\mu$ and variance $\sigma^2\neq 0$. Then $$\lim_{n\to\infty}\frac{X_1+\dots+X_n}{n}\to\mu$$ in probability. 
\end{thm}

\subsection{Standardized Random Variables}
\begin{defn}{Standardized Random Variables}{} Let $X$ be a random variable with finite variance. We define the standardized version of $X$ to be the random variable $Z$ given by $$Z=\frac{X-E(X)}{\sqrt{\vari(X)}}$$
\end{defn}

\begin{thm}{Central Limit Theorem}{} Let $X_1,X_2,\dots$ be a sequence of independent and identically distributed random variables, each with mean $\mu$ and variance $\sigma^2\neq 0$. Let $S_n=X_1+\dots+X_n$. Then the standardized version of $S_n$, $$Z_n=\frac{S_n-E(S_n)}{\sqrt{\vari(S_n)}}=\frac{S_n-n\mu}{\sigma\sqrt{n}}$$ converges in distribution as $n\to\infty$ to a Gaussian random variable with mean $0$ and variance $1$. That is, $$\lim_{n\to\infty}P(Z_n\leq x)=\lim_{n\to\infty}F_{Z_n}(x)=F_Y(y)=\int_{-\infty}^{x}-\frac{1}{\sqrt{2\pi}}e^{-y^2/2}$$
\end{thm}

\pagebreak
\section{Stochastic Processes}
\subsection{Markov Chains}
\begin{defn}{Stochastic Matrix}{} Let $P=(p_{i,j})\in M_n(\R)$ be a matrix. We say that $P$ is a stochastic matrix if the following are true. 
\begin{itemize}
\item $0\leq p_{i,j}\leq 1$ for $1\leq i,j\leq n$. 
\item For any fixed $1\leq k\leq n$, we have $$\sum_{j=1}^np_{k,j}=1$$
\end{itemize}
\end{defn}

\begin{defn}{Markov Chain}{} Let $(\Omega,\mF,\Prj)$ be a probability space. Let $I=\{1,\dots,k\}$. Let $\{X_n:\Omega\to I\;|\;n\in\N\}$ be a sequence of random variables. Let $\lambda:I\to[0,1]$ be a probability distribution. Let $P=(p_{i,j})$ be a stochastic matrix. We say that $(X_n)_{n\geq 0}$ is a Markov chain with initial distribution $\lambda$ and transition matrix $P$ if the following are true. 
\begin{itemize}
\item $\Prj(X_0=i)=\lambda(i)$ for any $i\in I$. 
\item For any $i_0,\dots,i_{n+1}\in I$, we have $$\Prj(X_{n+1}=i_{n+1}\;|\;X_k=i_k\text{ for }0\leq k\leq n)=\lambda(i_0)\cdot\prod_{j=0}^n p_{i_j,i_{j+1}}$$ 
\end{itemize}
In this case we say that $(X_n)_{n\geq 0}$ is $\text{Markov}(\lambda,P)$. 
\end{defn}

In other words, Markov chains are a sequence of random variables where the next step does not depends on what states you visited previously, but only on the state now. There is an important property that makes studying Markov chains worth while. 

\begin{prp}{}{} Let $(\Omega,\mF,\Prj)$ be a probability space. Let $I=\{1,\dots,k\}$. Let $\{X_n:\Omega\to I\;|\;n\in\N\}$ be a sequence of random variables. Let $\lambda:I\to[0,1]$ be a probability distribution. Then $(X_n)_{n\geq 0}$ is a Markov chain if and only if $$\Prj(X_{n+1}=i_{n+1}\;|\;X_k=i_k\text{ for }0\leq k\leq n)=\Prj(X_{n+1}=i_{n+1}\;|\;X_n=i_n)$$ for any $i_0,\dots,i_{n+1}\in I$. In this case, the transition matrix of the Markov chain is given by $$P=\begin{pmatrix}
\Prj(X_1=1\;|\;X_0=1) & \cdots & \Prj(X_1=k\;|\;X_0=1)\\
\vdots & \ddots & \vdots\\
\Prj(X_1=1\;|\;X_0=k) & \cdots & \Prj(X_1=k\;|\;X_0=k)
\end{pmatrix}$$
\end{prp}

\begin{thm}{Markov Property}{} Let $(X_n)_{n\geq0}$ be a markov chain. Suppose that $X_m=E_m$ is given. Then $(X_{m+n})_{n\geq 0}$ is a Markov chain. 
\end{thm}


Notice that the transition probabbility $\Prj(X_{n+1}=j|X_n=i)$ still depedns on $i,j,n$. We further restrict out study of Markov chains to the following type. 

\begin{defn}{Homogenous Markov Chains}{} We say that a Markov chain $(X_n)_{n\geq 0}$ is homogenous if $$\Prj(X_{n+1}=j|X_n=i)=\Prj(X_1=j|X_0=i)$$
\end{defn}

This means that homogenous Markov chains are time independent. 

\begin{lmm}{}{} The transition matrix $P$ of a homogenous Markov chain is a Stochastic matrix. 
\end{lmm}

\begin{thm}{}{} Let $(X_n)_{n\geq 0}$ be Markov. Then for all $m,n\geq0$, we have
\begin{itemize}
\item $\Prj(X_n=j|X_0=i)=\Prj(X_1=j|X_0=i)^n$
\item $\Prj(X_n=j)=\sum_{i\in I}\Prj(X_0=i)\Prj(X_n=j|X_0=i)$
\end{itemize}
\end{thm}

This theorem is saying that $\Prj(X_n=j|X_0=i)$ is equal to the $i,j$th entry of the matrix $P^n$ if $P$ is the transition matrix. To find out the total probability of reaching the $j$th state at the $n$the step regardless of the starting point, you sum up the $\Prj(X_n=j|X_0=i)$ multiplying the probability that you start at state $i$. \\~\\

We often like to create new Markov chains from old (at least from exams). If we want to show that a squence $(X_n)_{n\geq 0}$ of random vriables is Markov, try and write $X_{n+1}=X_n+\text{ some term only related to }n+1$. 

\subsection{Communicating Classes}
\begin{defn}{Talks and Communicates}{} Let $i,j$ be states. We say that $i$ talks to $j$ which is $i\rightarrow j$ if there exists $n\in\N$ such that $$\Prj(X_n=j|X_0=i)>0$$ We say that two states $i$ and $j$ communicate if $i\leftrightarrow j$. 
\end{defn}

\begin{defn}{Communicating Class}{} Let $C\subseteq I$. $C$ is a communicating class if $\forall i,j\in C$, $i\leftrightarrow j$ and $\forall i\in C$ and $\forall k\in I\setminus C$, $i\not{\rightarrow} k$ 
\end{defn}

\begin{defn}{Closed and Absorbing}{} We say that a class is closed if no states in the class talks to any states outside of that class. We say that a class is absorbing if it forms a closed class by itself. 
\end{defn}

\begin{defn}{Irreducible Markov Chains}{} A Markov chain is said to be irreducible if for all $i,j\in I$, $i\leftrightarrow j$. 
\end{defn}

\subsection{Hitting Times}
\begin{defn}{Stopping Times}{} Let $(X_n)_{n\geq 0}$ be Markov. Then a random variable $$T:\Omega\to\N\cup\{\infty\}$$ is a stopping time if for all $n\geq 0$, the event $\{T=n\}$ depends only on $X_0,\dots,X_n$. 
\end{defn}

\begin{defn}{Hitting Times}{} Let $(X_n)_{n\geq 0}$ be Markov. The hitting time of a subset $A\subseteq I$ is a random variable $$H_A:\Omega\to\N\cup\{\infty\}$$ defined by $$H_A(\omega)=\inf\{n\geq 0|X_n(\omega)\in A\}$$ We use $$h_i^A=\Prj(H^A<\infty|X_0=i)$$ to denote the corresponding probability. We use $$k_i^A=E_i[H_A]=\sum_{k=0}^\infty k\Prj(H_A=k|X_0=i)+\infty\Prj(H_A=\infty|X_0=i)$$ to denote the expectation. 
\end{defn}

\begin{prp}{}{} We can find the probabilities of all hitting times by solving the system of linear equations $$\begin{cases}
\Prj(H_A<\infty|X_0=i)=1 & i\in A\\
\Prj(H_A<\infty|X_0=i)=\sum_{j\in I}\Prj(X_1=j|X_0=i)\Prj(H_A<\infty|X_0=j) & x\notin A
\end{cases}$$
\end{prp}

\begin{prp}{}{} We can find the expected number of hitting times by solving the system of linear equations $$\begin{cases}
E_i[H_A]=0 & i\in A\\
E_i[H_A]=1+\sum_{j\notin A}\Prj(X_1=j|X_0=i)E_j[H_A] & i\notin A
\end{cases}$$
\end{prp}

\subsection{Strong Markov Property}
\begin{thm}{Strong Markov Property}{} Let $(X_n)_{n\geq 0}$ be Markov$(\lambda,P)$ and $T$ a stopping time of $(X_n)_{n\geq 0}$. Then conditional on both $\{X_T=i\}$ and $\{T<\infty\}$, we have $(X_{T+n})_{n\geq 0}$ is Markov$(\delta_i,P)$ and independent of $X_0,\dots,X_T$. 
\end{thm}

\subsection{Recurrence and Transcience}
\begin{defn}{Recurrence and Transcience}{} A state $i\in I$ is recurrent if $\Prj_i(X_n=i\text{ for infinitely many }n)=1$. A state $i\in I$ is transient if $\Prj_i(X_n=i\text{ for infinitely many }n)=0$. 
\end{defn}

\begin{defn}{$k$-th Passage Time}{} The first passage time of state $i$ is a stopping time such that $$T_i(\omega)=\inf\{n\geq 1|X_n(\omega)=i\}$$ The $k$-th passage time is a stopping time such that $$T_i^{(k)}(\omega)=\inf\{n\geq T_i^{(k-1)}|X_n(\omega)=i\}$$ The $k$-th excursion time is defined by $$S_i^{(k)}=\begin{cases}
T_i^{(k)}-T_i^{(k-1)} & \text{ if }T_i^{(k-1)}<\infty\\
\infty & \text{ otherwise }
\end{cases}$$
\end{defn}

Intuitively, $T_i^{(k)}$ outputs the time it takes for the Markov chain to reach state $i$ for the $k$th time and $S_i^k$ outputs the number of steps taken between consecutive visits. 

\begin{lmm}{}{} For $k=2,3,\dots$ and if $T_i^{(k-1)}<\infty$, then $S_i^{(k)}$ is independent of $\{X_m:m\leq T_i^{(k-1)}\}$ and $$\Prj(S_i^{(k)}=n|T_i^{(k-1)}<\infty)=\Prj_i(T_i=n)$$
\end{lmm}

\begin{defn}{Visit Counting Function}{} Let $i\in I$ be a state. Define $$V_i=\sum_{k=0}^\infty1_{\{X_n=i\}}$$ the number of visits ever to state $i$. 
\end{defn}

\begin{lmm}{}{} If $V_i$ counts the number of vists to state $i$, then $$E_i(V_i)=\sum_{k=0}^\infty P_{ii}^{(k)}$$
\end{lmm}

\begin{lmm}{}{} For $k=0,1,2,\dots$, we have $$\Prj_i(V_i>k)=(\Prj_i(T_i<\infty))^k$$
\end{lmm}

This lemma means that $V_i$ asserts a geometric distribution. 

\begin{thm}{Characteristic of Recurrent and Transient States}{} Let $(X_n)_{n\geq 0}$ be a Markov chain. Let $T_i$ be the first passage time of state $i$. If $\Prj_i(T_i<\infty)=1$, then $i$ is recurrent and $\sum_{k=1}^\infty\Prj(X_1=k|X_0=k)^n=\infty$. If $\Prj_i(T_i<\infty)<1$, then $i$ is transient and $\sum_{k=1}^\infty\Prj(X_1=k|X_0=k)^n<\infty$. 
\end{thm} 

\begin{thm}{}{} Let $C$ be a communicating class. Then either all states in $C$ are transient or all are recurrent. 
\end{thm}

\begin{thm}{}{} Let $C\subset I$ be a class of a Markov chain. Then
\begin{itemize}
\item Every recurrent class is closed
\item Every finite closed class is recurrent
\end{itemize}
\end{thm}

\begin{thm}{}{} If $P$ is irreducible and recurrent then for all $j\in I$, $$\Prj(T_j<\infty)=1$$
\end{thm}

\begin{defn}{Component Independent Simple Random Walk on $\Z^d$}{} A component independent simple random walk on $\Z^d$ for $d\in\N$ is defined as $$X_{n+1}=X_n+Z_{n+1}$$ where $Z_{n+1}=(Z_{n+1}^1,\dots,Z_{n+1}^d)$ with $$Z_m^j=\begin{cases}
1 & \text{ with probability }p_j\\
-1 & \text{ with probabilty }q_j
\end{cases}$$
where $Z_m^j$ are independent for all $j,m$. 
\end{defn}

\begin{prp}{}{} Let $(X_n)_{n\geq 0}$ be a CISRW on $\Z^d$. If there exists $j\in\{1,\dots,d\}$ such that $p_j\neq\frac{1}{2}$ then $(X_n)$ is transient. 
\end{prp}

\begin{prp}{}{} Let $(X_n)_{n\geq 0}$ be a CISRW on $\Z^d$ and $p_j=q_j=\frac{1}{2}$ for all $j$. Then
\begin{itemize}
\item If $d\leq 2$ then all states are recurrent
\item If $d\geq 3$ then all states are transient
\end{itemize}
\end{prp}

\subsection{Branching Process}
\begin{defn}{Branching Process}{} Let $(X_n)_{n\geq 0}$ be the number of individuals in a population at time $n$ where $X_n\in\{0\}\cup\N$. At every time step each individual in the population gives birth to a random number of offspring. Thus $X_{n+1}$ is defined to be $$X_{n+1}=\sum_{k=1}^{X_n}Z_k^n$$ where $Z_k^n\sim Z$ and $\Prj(Z\geq 0)=1$. Then $(X_n)_{n\geq 0}$ is said to be a branching process. 
\end{defn}

\begin{prp}{}{} Define $G(s)=E[s^Z]$ and $F_{n+1}(s)=E[S^{X_{n+1}}|X_0=1]$ for $s\in(0,1)$. Then $$F_n(s)=G(F_{n-1}(s))$$
\end{prp}

\begin{prp}{}{} For a braching process $(X_n)_{n\geq 0}$ with off-spring distribution $Z$ where $E[Z]=\mu$, we have $$E[X_n]=\mu^n$$
\end{prp}

\begin{thm}{Extinction Probability}{} The extinction probability is the smallest non-negative root of $G(\alpha)=\alpha$. 
\end{thm}

\begin{thm}{}{} Suppose that $G(0)>0$, $\mu=E[Z]=G'(1)$. Then $\mu\leq 1$ implies certain extiction. $\mu>1$ implies uncertain extinction. 
\end{thm}

\subsection{Invariant Distributions on a Markov Chain}
\begin{defn}{Invariant Distribution}{} A measure $\lambda=(\lambda_i:i\in I)$ with non-negative entries is said to be an invariant distribution if 
\begin{itemize}
\item $\lambda P=P$
\item $\pi_i\in[0,1]$ for all $i$
\item $\sum_{i\in I}\pi_i=1$
\end{itemize}
\end{defn}

The following theorem shows that after applying a Markov process to an invariant distribution, the new distribution will be the same as the old one. Notice here that right multiplication of a distribution gives the next step on the Markov chain instead of the usual left multiplication. 

\begin{thm}{}{} Let $(X_n)_{n\geq 0}$ be Markov$(\pi,P)$, where $\pi$ is an invariant distribution for $P$. Then $\forall m\geq 0$, $(X_{n+m})_{n\geq 0}$ is Markov$(\pi,P)$. \tcbline
\begin{proof}
By the Markov property, the new sequence will be Markov. But we do not yet know its distribution. We have that 
\begin{align*}
\Prj(X_m=i)&=(\pi P^m)_i\tag{this denotes the $i$th entry of $P$}\\
&=(\pi PP^{m-1})_i\\
&=(\pi P^{m-1})_i
\end{align*}
Thus by induction, we see that $\Prj(X_m=i)=\pi_i$ thus we are done. 
\end{proof}
\end{thm}

\begin{thm}{}{} Let $I$ be finite. Suppose that for some $i\in I$, $$P_{ij}^n\to\pi_j$$ for all $j\in I$. Then $\pi=(\pi_j:j\in I)$ is an invariant distribution. 
\end{thm}

Let us look at an example. 

\begin{eg}{Gene Mutation}{} Recall the gene mutation example that has transition matrix $P=\begin{pmatrix}
1-\alpha & \alpha\\
\beta & 1-\beta
\end{pmatrix}$. Recall that $$P^n=\begin{pmatrix}
\frac{\beta}{\alpha+\beta}+\frac{\alpha}{\alpha+\beta}(1-\alpha-\beta)^n & \frac{\alpha}{\alpha+\beta}-\frac{\alpha}{\alpha+\beta}(1-\alpha-\beta)^n\\
? & ?
\end{pmatrix}$$
As $n\to\infty$, observe that the first entry tends to $\frac{\beta}{\alpha+\beta}$ the second entry tends to $\frac{\alpha}{\alpha+\beta}$. This means that $\pi=\left(\frac{\beta}{\alpha+\beta},\frac{\alpha}{\alpha+\beta}\right)$ is an invariant distribution for the gene mutation model. \\~\\
We can also find the invariant distribution directly: Let $\pi=(\pi_1,\pi_2)$ be an invariant distribution for $P$. We have that 
\begin{align*}
\pi P&=\pi\\
\begin{pmatrix}
\pi_1(1-\alpha)+\pi_2\beta & \pi_1\alpha+\pi_2(1-\beta)
\end{pmatrix}&=\begin{pmatrix}
\pi_1 & \pi_2
\end{pmatrix}\\
\begin{pmatrix}
\pi_2\beta-\pi_1\alpha & \pi_1\alpha-\pi_2\beta
\end{pmatrix}&=0\\
\end{align*} Notice that these two equations mean the same thing. (In general if you have an $n\times n$ transition matrix, only the first $n-1$ equations will be useful and the last one will be redundant.) This is why we need to use the fact that $\pi_1+\pi_2=1$ as our final equation. \\~\\
Now solving it, we have that $\pi=\left(\frac{\beta}{\alpha+\beta},\frac{\alpha}{\alpha+\beta}\right)$ which is the exact same answer as the one given in the first method. \\~\\
A third way to think about it is that notice that $P^T\pi^T=\pi^T$ is the equation that we are attempting to solve. Recall that $1$ is necessarily an eigenvector of a transtition matrix which means that this equation really is just a question of finding eigenvectors from the eigenvalue $1$. 
\end{eg}

The above examples gives a lot of practical information when calculating the invariant distribution of a transition matrix. 

\begin{defn}{Expected Time Spent}{} Define the expected time spent in state $i$ in between visits to state $k$ by $$\gamma_i^k=E_k\left(\sum_{n=0}^{T_k-1}\mathcal{X}_{X_n=i}\right)$$
\end{defn}

This uses $k$ as a reference, and as we keep going back to $k$, this records how much time we have spent in $i$ in the process of leaving and returning to $k$. 

\begin{thm}{}{} Let $P$ be irreducible and recurrent. Then
\begin{itemize}
\item $\gamma_k^k=1$
\item $\gamma^k=(\gamma_i^k:i\in I)$ is an invariant measure satisfying $\gamma^kP=\gamma^k$
\item $0<\gamma_i^k<\infty\;\forall i\in I$
\end{itemize}
\end{thm}

\begin{thm}{}{} Let $P$ be irreducible and $\lambda$ and invariant measure and $\lambda_k=1$ for some $k$. Then $\lambda\geq\gamma^k$. If we also have that $P$ is recurrent, then $\lambda=\gamma^k$
\end{thm}

This theorem assesrts that as long as $P$ is irreducible and recurrent, then any invariant measure is exactly equal to the $\gamma^k$ we defined, which means that there is really only one invariant measure up to rescaling. 

\begin{defn}{Positive Recurrent}{} A state $i\in I$ is positive recurrent if it is recurrent and $$E_i[T_i]<\infty$$ A state which is not positive recurrent but is recurrent is called null recurrent. 
\end{defn}

\begin{thm}{}{} Let $P$ be irreducible. Then the following are equivalent. 
\begin{itemize}
\item Every state is positive recurrent
\item Some state $i$ is positive recurrent
\item $P$ has an invariant distribution $\pi$. More over, $\pi_i=\frac{1}{E_i[T_i]}$
\end{itemize}
\end{thm}












\end{document}