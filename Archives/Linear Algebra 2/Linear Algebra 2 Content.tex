\section{Linear Forms and Quadratic Forms}
\subsection{Linear Forms}
\begin{defn}{Linear Forms}{} A linear form on $V$ is a linear map from $V$ to $\F$. 
\end{defn}

\begin{prp}{Dual Space}{} The set of all linear forms on $V$ forms a vector space called the dual space $V'$. \tcbline
\begin{proof}
Simply a check on the axioms of vector space. 
\end{proof}
\end{prp}

\begin{lmm}{}{} Let $V$ be a finite dimensional vector space. Then $V'$ is also finite dimensional and $\dim(V')=\dim(V)$. 
\end{lmm}

\begin{defn}{Dual Basis}{} Let $v_1,\dots,v_n$ be a basis of $V$, then the dual basis of $v_1,\dots,v_n$ is the list $\phi_1\dots,\phi_n$ of elements of $V'$, where $\phi_k$ is a linear functional such that $$\phi_k(v_i)=
\begin{cases}
1 & \text{if }k=i\\
0 & \text{if }k\neq i
\end{cases}$$
\end{defn}

\begin{prp}{}{} The dual basis of a basis of $V$ is a basis of $V'$
\end{prp}

\begin{defn}{Dual Map}{} Let $T\in\mathcal{L}(V,W)$. The dual map of $T$ is the linear map $T'\in\mathcal{L}(W',V')$ defined by $T'(\phi)=\phi\circ T$ for $\phi\in W'$. 
\end{defn}

\begin{prp}{}{} Let $S,T\in\mathcal{L}(V,W)$ and $\lambda\in\F$. 
\begin{itemize}
\item $(S+T)'=S'+T'$
\item $(\lambda T)'=\lambda T'$
\item $(ST)'=T'S'$. 
\end{itemize}
\end{prp}

\subsection{Quadratic Forms}
\begin{defn}{Quadratic Forms}{} A quadratic form in $n$ variables $x_1,\dots,x_n$ over a field $K$ is a polynomial $$q(x_1,\dots,x_n)=\sum_{i=1}^n\sum_{j=1}^na_{ij}x_ix_j$$
\end{defn}

\begin{prp}{}{} Every quadratic form $q(x_1,\dots,x_n)=\sum_{i=1}^n\sum_{j=1}^na_{ij}x_ix_j$ can be represented by a matrix multiplication, namely $$q(x_1,\dots,x_n)=\begin{pmatrix}x_1 & \cdots & x_n\end{pmatrix}\begin{pmatrix}a_{11} & \frac{1}{2}a_{12} & \cdots & \frac{1}{2}a_{1n}\\
\frac{1}{2}a_{21} & a_{22} & \cdots & \frac{1}{2}a_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
\frac{1}{2}a_{n1} & \frac{1}{2}a_{n2} & \dots & a_{nn}
\end{pmatrix}\begin{pmatrix}x_1 \\ \vdots \\ x_n\end{pmatrix}$$ In particular, this matrix is symmetric with $a_{ij}=a_{ji}$ for $i,j\in\{1,\dots,n\}$. \tcbline
\begin{proof}
Multiplying out the entries of the matrix multiplication gives the original quadratic form. 
\end{proof}
\end{prp}

\begin{prp}{}{} A change of basis via the change of basis matrix $P$ also changes the symmetric matrix of the quadratic form by $P^TAP$
\end{prp}

\begin{defn}{Congruent Matrices}{} Two matrices $A,B$ are said to be congruent if there exists some invertible matrix $P$ such that $B=P^TAP$
\end{defn}

Beware that congruences does not apply to only symmetric matrices. We will see more of it in action in bilinear forms. 

\begin{prp}{}{} Two symmetric matrices are congruent if and only if they represent the same quadratic form with respect to different bases. 
\end{prp}

\begin{thm}{}{} Let $q(x_1,\dots,x_n)$ be a quadratic form in $n$ variables over a field $K$ whose characteristic is not $2$. Then there exists a basis such that $q(y_1,\dots,y_n)=c_1y_1^2+\dots+c_ny_n^2$ for some $c_1,\dots,c_n\in K$. \tcbline
\begin{proof}
There is a shorter proof for this theorem, but for the sake of the construction of $c_1,\dots,c_n$, we will prove the theorem constructively. Suppose that $q$ is represented by the symmetric matrix $A=(a_{ij})_{n\times n}$ with respect to the basis $b_1,\dots,b_n$. There are three steps in the construction. I use $b_1,\dots,b_n$ to indicate the old basis and $b_1',\dots,b_n'$ to indicate the basis after the step. \\~\\
Step 1: Arrange such that $q(b_1)\neq 0$. There are four cases here. 
\begin{itemize}
\item If $a_{11}\neq 0$, then we are done. 
\item If $a_{11}=0$ but $a_{kk}\neq 0$ for some $1<k\leq n$. Then just set $b_1'=b_k$ and $b_k'=b_1$. At the same time, the matrix for the quadratic form is changed by swapping rows $r_1$ and $r_k$, and then swapping the columns $c_1$ and $c_k$
\item If $a_{kk}=0$ for all $k\in\{1,\dots,n\}$, but there are some $i,j$ such that $a_{ij}\neq 0$, then set $b_i'=b_i+b_j$ since $q(b_i+b_j)=2a_{ij}\neq 0$ and so we reduced this case to the previous two cases.The matrix then becomes $$\begin{pmatrix}
2a_{1k} & a_{12}+a_{k2} & \dots & a_{1k} & \cdots & a_{1n}+a_{kn} \\
 a_{12}+a_{k2} & 0 & \cdots & a_{2k} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
a_{1k} & a_{k2} & \cdots & 0 & \cdots & a_{kn}\\
\vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
a_{1n}+a_{kn} & a_{n2} & \cdots & a_{nk} & \cdots & 0
\end{pmatrix}$$
\item If $a_{ij}=0$ for all $i,j\in\{1,\dots,n\}$ then it is the zero function. 
\end{itemize}
In this step the change of basis matrix is just the elementary matrices. \\~\\

Step 2: Now we modify $b_2,\dots,b_n$ to make them orthogonal to $b_1$. Now set $b_k'=b_k-\frac{a_{1k}}{a_{11}}b_1$. This way, the matrix entry $a_{1k}$ becomes zero. Now the change of basis matrix becomes $$P=\begin{pmatrix}
1 & -\frac{a_{12}}{a_{11}} & \cdots & -\frac{a_{1n}}{a_{11}}\\
0 & 1 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & 0
\end{pmatrix}$$
After this step all the change of basis matrix should be compiled and calculated so that the new matrix for the quadratic form can be formed. \\~\\
Step 3: Since the matrix for the quadratic form is now $$\begin{pmatrix}
a_{11} & 0 & \cdots & 0\\
0 & ? & \cdots & ?\\
\vdots & \vdots & \ddots & \vdots\\
0 & ? & \cdots & ?
\end{pmatrix}$$
We can induct on $n$ by repeating the process of step 1 with the entry $a_{22}$ until we reach $a_{nn}$. 
\end{proof}
\end{thm}

This main theorem of quadratic forms shows that every quadratic form is congruent to a diagonal matrix. To illustrate the process of reduction, we look an example. 

\begin{eg}{}{} Find a nice basis for the quadratic form $q\left(\begin{pmatrix}x\\y\\z\end{pmatrix}\right)=xy+3yz-5xz$. \tcbline
\begin{proof}
Using the formula, we construct the matrix of $q$ as $$A=\begin{pmatrix}
0 & \frac{1}{2} & -\frac{5}{2}\\
\frac{1}{2} & 0 & \frac{3}{2}\\
-\frac{5}{2} & \frac{3}{2} & 0
\end{pmatrix}$$ ~\\
We start the first change of basis. Notice that $a_{11}=a_{22}=a_{33}=0$ in the matrix while $a_{12}$ is not. Denote the standard basis by $b_1,b_2,b_3$. We perform the first basis change by $\{b_1'=b_1+b_2,b_2'=b_2,b_3'=b_3\}$. This means that the change of basis matrix from new to old is $$P'=\begin{pmatrix}
1 & 0 & 0\\
1 & 1 & 0\\
0 & 0 & 1
\end{pmatrix}$$
To change $A$ into the new matrix $A'$, we simply replace $r_1$ by $r_1+r_2$ and replace $c_1$ by $c_1+c_2$. This gives $$A'=\begin{pmatrix}
1 & \frac{1}{2} & -1\\
\frac{1}{2} & 0 & \frac{3}{2}\\
-1 & \frac{3}{2} & 0
\end{pmatrix}$$
We keep track of changing the basis for clarity. We now have $A'=(P')^TAP'$. \\~\\
The next step is to set the new basis to $b_2''=b_2'-\frac{1}{2}b_1'$ and $b_3''=b_3'+b_1'$. This means that the new basis is now $\{b_1+b_2,\frac{1}{2}(b_2-b_1),b_1+b_2+b_3\}$. The change of basis from this basis back to the old one is now $$P''=\begin{pmatrix}
1 & -\frac{1}{2} & 1\\
1 & \frac{1}{2} & 1\\
0 & 0 & 1
\end{pmatrix}$$ Now the new matrix $A''$ is formed by replacing $r_2$ with $r_2$ by $r_2-\frac{1}{2}r_1$ and $r_3$ with $r_3+r_1$. Noticing that $A''$ must be symmetric, we need to take the new elements and replace the remanining lower traingualr elements so that $A''$ maintains symmetric. Also observe that the replacement of rows is exactly one changes into the new basis from the previous basis, where $b_2''=b_2'-\frac{1}{2}b_1'$ etc. Now we have $$A''=\begin{pmatrix}
1 & 0 & 0\\
0 & -\frac{1}{4} & 2\\
0 & 2 & -1
\end{pmatrix}$$
We now have $A''=(P'')^TAP''$. \\~\\
Now we perform the next change of basis. We set $b_3'''=b_3''-\frac{2}{-\frac{1}{4}}b_2''=b_3''+8b_2''$. Now the new basis is $\{b_1+b_2,\frac{1}{2}(b_2-b_1),-3b_1+5b_2+b_3\}$. The change of basis matrix is now $$P'''=\begin{pmatrix}
1 & -\frac{1}{2} & -3\\
1 & \frac{1}{2} & 5\\
0 & 0 & 1
\end{pmatrix}$$ Similar to the above, we replace $r_3$ with the same transformation and adjust $A'''$ so that it remains symmetric. Thus now we have $$A'''=\begin{pmatrix}
1 & 0 & 0\\
0 & -\frac{1}{4} & 0\\
0 & 0 & 15
\end{pmatrix}$$
This means that we are done with $A'''=(P''')^TAP'''$. 
\end{proof}
\end{eg}

In general, this result of diagonalization is different from that of similar matrices. One should not be confusing reduction of congruent matrices into diagonal matrices and reduction of similar matrices into JCF as well as reduction of diagonalizable matrices into diagonal matrices. 

\subsection{Bilinear Forms}
\begin{defn}{Bilinear Maps}{} Let $V,W$ be vector spaces over a field $\F$. A bilinear map on $V$ and $W$ is a map $\tau:V\times W\to\F$ such that 
\begin{itemize}
\item $\tau(a_1+a_2v_2,w)=a_1\tau(v_1,w)+a_2\tau(v_2,w)$
\item $\tau(v,b_1w_1+b_2w_2)=b_1\tau(v,w_1)+b_2\tau(v,w_2)$
\end{itemize}
\end{defn}

\begin{thm}{}{} Let $V$ and $W$ has basis $e_1,\dots e_n$ and $f_1,\dots,f_m$ respectively. Let $\tau(v,w)$ be a bilinear map. Then $$\tau(v,w)=v^T\begin{pmatrix}
\tau(e_1,f_1) & \cdots & \tau(e_1,f_m)\\
\vdots & \ddots & \vdots\\
\tau(e_n,f_1) & \cdots & \tau(e_n,f_m)
\end{pmatrix}w$$ \tcbline
\begin{proof}
Simple to see by expanding the matrix multiplication. 
\end{proof}
\end{thm}

\begin{prp}{}{} Let $A$ be the matrix of the bilinear map $\tau V\times W\to\F$ with respect to the basis $e_1,\dots,e_n$ and $f_1,\dots,f_m$ of $V$ and $W$. Let $B$ be the matrix with respect to the basis of $e_1',\dots,e_n'$ and $f_1',\dots,f_m'$ of $V$ and $W$. Let $P$ and $Q$ be the change of basis matrix where $Pv'=v$ and $Qw'=w$ for $v\in V$ and $w\in W$. Then $$B=P^TAQ$$
\end{prp}

\begin{defn}{Bilinear Forms}{} A bilinear form is a bilinear map that maps $V\times V$ to $\F$. 
\end{defn}

\begin{defn}{Congruent Matrices}{} Two matrices are congruent if there exists an invertible matrix $P$ such that $B=P^TAP$. 
\end{defn}
Note that this definition of congruence in matrices coincides with the definition given with symmetric matrices. 

\begin{defn}{Types of Bilinear Forms}{} A bilinear form is said to be
\begin{itemize}
\item symmetric if $\tau(v,w)=\tau(w,v)$
\item antisymmetric if $\tau(v,w)=-\tau(w,v)$
\item positive definite if $\tau(v,v)>0$ for all $v\in V$. 
\end{itemize}
\end{defn}

\begin{prp}{}{} Let $q:V\to K$ be a function. Then the following are equivalent. 
\begin{itemize}
\item $q$ is a quadratic form
\item $q(cv)=c^2v$ for $c\in K$ and $v\in V$ and $\tau(v,w)=\frac{1}{2}(q(v+w)-q(v)-q(w))$ is a bilinear form on $V$
\item $q(v)=\tau(v,v)$ is a symmetric bilinear form on $V$
\end{itemize} \tcbline
\begin{proof}
Let $q:V\to K$ be a function. 
\begin{itemize}
\item $(1)\implies (2)$: Since every term in a quadratic form is quadratic, $q(cv)=c^2q(v)$ is natural. The fact that $\tau(v,w)$ is bilinear is also easy to check. 
\item $(2)\implies (3)$: From $(2)$ we know that $\tau(v,v)=q(v)$ by substituting $v$ in the position of $w$ and thus it clearly is a bilinear form. The position of $w$ and $v$ are also interchangable and thus it is symmetric. 
\item $(3)\implies (1)$: If $\tau(v,v)$ is a symmetric bilinear form then the matrix of $\tau$ is symmetric since $a_{ij}=\tau(e_i,f_j)=\tau(f_j,e_i)=a_{ji}$. Thus $q(v)=\tau(v,v)$ defines a quadratic form. 
\end{itemize}
\end{proof}
\end{prp}

\subsection{Sesquilinear Forms}

\pagebreak
\section{Inner Product Spaces}
\subsection{Norms}
Throught this section, $\F$ means either $\R$ or $\C$. In general normed vector spaces only perform well in these two fields. 
\begin{defn}{Norm}{} Let $V$ be a vector space. A norm on $V$ is a function $\|\cdot\|:V\to\F$ such that
\begin{itemize}
\item $\|x\|\geq 0$ for all $x\in V$ with equality if and only if $x=0$
\item $\|\lambda x\|=\abs{\lambda}\|x\|$ for all $x\in V$ and $\lambda\in\F$
\item $\|x+y\|\leq\|x\|+\|y\|$ for all $x,y\in V$
\end{itemize}
\end{defn}

\begin{defn}{Normed Vector Space}{} A normed vector space is a pair $(V,\|\cdot\|)$ where $V$ is a vector space and $\|\cdot\|$ is a norm on $V$. 
\end{defn}

\begin{prp}{}{} Every normed vector space is a metric space. \tcbline
\begin{proof} Can easily be seen that setting $d(x,y)=\|x-y\|$ allows the norm to become a metric. 
\end{proof}
\end{prp}

\begin{defn}{Convex Set}{} Let $V$ be a vector space. A subset $K$ of $V$ is convex if $x,y\in K$ implies $$\lambda x+(1-\lambda)y\in K$$ for $0\leq\lambda\leq 1$. 
\end{defn}

\begin{lmm}{}{} For every normed vector space, the unit ball $B_1(0)=\{v\in V|\|v\|\leq 1\}$ is convex. 
\end{lmm}

\begin{prp}{}{} Let $N:V\to\R^+$ be a function that satisfies the first two requirements of a norm. If $N$ also satisfies the fact that $\{x\in V|N(x)\leq 1\}$ is convex, then $N$ is a norm. 
\end{prp}

\begin{defn}{Isometries}{} If $(X,d_1)$ and $(Y,d_2)$ are metric spaces, then a distancing preserving map between $X$ and $Y$ is a map $$f:X\to Y$$ such that for any $P,Q\in X$, we have $$d(f(P),f(Q))=d(P,Q)$$ A bijective distancing preserving map is called an isometry. 
\end{defn}


\subsection{Inner Products}
Inner products are only properly defined for vector spaces over $\R$ and $\C$. From this point onwards we will limit our discussions with $V=\R^n$ or $\C^n$ and $K=\R$ or $\C$. 
\begin{defn}{Inner Products}{} An inner product on $V$ is a function that takes each ordered pair $(x,y)$ of a vector space $V$ to a number $\langle x,y\rangle\in K$ such that 
\begin{itemize}
\item $\langle x,x\rangle\geq0$ for all $x\in V$ with equality if and only if $x=0$. 
\item $\langle x+z,y\rangle=\langle x,y\rangle+\langle z,y\rangle$ for all $x$ for all $x,y,z\in V$
\item $\langle \lambda x,y\rangle=\lambda\langle x,y\rangle$ for all $x,y\in V$ and $\lambda\in K$
\item $\langle x,y\rangle=\overline{\langle y,x\rangle}$ for all $x,y\in V$
\end{itemize}
In this case $V$ is called an inner product space. 
\end{defn}

\begin{prp}{}{} Let $u,v,w\in V$. Let $\lambda\in K$. Then the following are true. 
\begin{itemize}
\item $\langle 0,u\rangle=\langle u,0\rangle=0$
\item $\langle u,v+w\rangle=\langle u,v\rangle+\langle u,w\rangle$
\item $\langle u,\lambda v\rangle=\overline{\lambda}\langle u,v\rangle$
\end{itemize}\tcbline
\begin{proof} Let $\langle \cdot,\cdot\rangle$ be an inner product over $V$. 
\begin{itemize}
\item $\langle 0,u\rangle=\langle 0,u\rangle+\langle 0,u\rangle$ thus $\langle 0,u\rangle=0$. $\langle u,0\rangle=0$ can be proven using the below property. 
\item Let $u,v,w\in V$. Then 
\begin{align*}
\langle u,v+w\rangle&=\overline{\overline{\langle u,v+w\rangle}}\\
&=\overline{\langle v+w,u\rangle}\\
&=\overline{\langle v,u\rangle+\langle w,u\rangle}\\
&=\overline{\langle v,u\rangle}+\overline{\langle w,u\rangle}\\
&=\langle u,v\rangle+\langle u,w\rangle\\
\end{align*}
\item Applying the same technique as above gives the desired result. 
\end{itemize}
\end{proof}
\end{prp}

\begin{prp}{}{} Every inner product induces a norm. \tcbline
\begin{proof} Define the norm to be $\|x\|=\sqrt{\langle x,x\rangle}$. 
\end{proof}
\end{prp}

\begin{prp}{Cauchy-Schwartz Inquality}{} For all $x,y\in V$, $$\abs{\langle x,y\rangle}\leq\|x\|\cdot\|y\|$$ with equality if and only if $y=\lambda x$ for some $\lambda\in\F$. 
\begin{proof} Let $z=x-\frac{\abs{\langle x,y\rangle}}{\|y\|^2}y$. We have $\|z\|\geq0$. 
\begin{align*}
\|z\|^2&=\left\langle x-\frac{\abs{\langle x,y\rangle}}{\|y\|^2}y,x-\frac{\abs{\langle x,y\rangle}}{\|y\|^2}y\right\rangle\\
&=\langle x,x\rangle-2\left\langle x,\frac{\abs{\langle x,y\rangle}}{\|y\|^2}y\right\rangle+\left\langle \frac{\abs{\langle x,y\rangle}}{\|y\|^2}y,\frac{\abs{\langle x,y\rangle}}{\|y\|^2}y\right\rangle\\
&=\langle x,x\rangle-2\frac{\abs{\langle x,y\rangle}^2}{\|y\|^2}+\frac{\abs{\langle x,y\rangle}^2}{\|y\|^4}\langle y,y\rangle\\
&=\langle x,x\rangle-2\frac{\abs{\langle x,y\rangle}^2}{\|y\|^2}+\frac{\abs{\langle x,y\rangle}^2}{\|y\|^2}\\
&=\|x\|^2-\frac{\abs{\langle x,y\rangle}^2}{\|y\|^2}\\
\end{align*}
Thus we have
\begin{align*}
\|x\|^2&\geq\frac{\abs{\langle x,y\rangle}^2}{\|y\|^2}\\
\|x\|&\geq\frac{\abs{\langle x,y\rangle}}{\|y\|}\tag{Since they are all positive}\\
\|x\|\cdot\|y\|&\geq\abs{\langle x,y\rangle}
\end{align*}
Note that we have equality if and only if $\|z\|=0$, meaning $x=\frac{\abs{\langle x,y\rangle}}{\|y\|^2}y$. We are done by taking $\lambda=\frac{\|y\|^2}{\abs{\langle x,y\rangle}}$. 
\end{proof}
\end{prp}

\pagebreak
\section{Orthogonality}
\subsection{Orthogonal Vectors}
\begin{defn}{Orthogonal Vectors}{} Two vectors $u,v\in V$ an inner product space are called orthogonal if $\langle u,v\rangle=0$. 
\end{defn}

\begin{crl}{}{} Let $V$ be an inner product space. Let $u,v\in V$. Then the following are true. 
\begin{itemize}
\item $0$ is orthogonal to every vector in $V$
\item $0$ is the only vector in $V$ that is orthogonal to itself. 
\end{itemize} \tcbline
\begin{proof}
Easy check involving properties of the inner product. 
\end{proof}
\end{crl}

\begin{thm}{Pythagorean Theorem}{} Suppose that $u,v\in V$ an inner product space and $u,v$ are orthogonal. Then $$\|u+v\|^2=\|u\|^2+\|v\|^2$$ \tcbline
\begin{proof}
The norm here is induced by the inner product and thus $\|x\|=\sqrt{\langle x,x\rangle}$. We have that 
\begin{align*}
\|u+v\|^2&=\langle u+v,u+v\rangle\\
&=\langle u,u\rangle+\langle u,v\rangle+\langle v,u\rangle+\langle v,v\rangle\\
&=\langle u,u\rangle+\langle v,v\rangle\\
&=\|u\|^2+\|v\|^2
\end{align*}
\end{proof}
\end{thm}

\begin{thm}{Orthogonal Decomposition}{} Let $u,v\in V$ and $v\neq 0$. Set $c=\frac{\langle u,v\rangle}{\|v\|^2}$ and $w=u-\frac{\langle u,v\rangle}{\|v\|^2}v$. Then $\langle w,v\rangle=0$ and $u=cv+w$. \tcbline
\begin{proof}
The fact that $u=cv+w$ is natural so we only have to prove that $\langle w,v\rangle=0$. We have that 
\begin{align*}
\langle w,v\rangle&=\left\langle u-\frac{\langle u,v\rangle}{\|v\|^2}v,v\right\rangle\\
&=\langle u,v\rangle-\left\langle \frac{\langle u,v\rangle}{\|v\|^2}v,v\right\rangle\\
&=\langle u,v\rangle-\frac{\langle u,v\rangle}{\|v\|^2}\langle v,v\rangle\\
&=\langle u,v\rangle-\frac{\langle u,v\rangle}{\|v\|^2}\|v\|^2\\
&=0
\end{align*}
Thus we are done. 
\end{proof}
\end{thm}

\begin{prp}{}{} Every orthonormal list of vectors are linearly independent. \tcbline
\begin{proof}
Suppose that $v_1,\dots,v_n$ are orthnormal. We want to show that $v_n=\sum_{k=1}^{n-1}a_kv_k$ imnplies $a_1=\dots=a_{n-1}=0$. Then $$\langle v_n, v_i\rangle=\sum_{k=1}^{n-1}a_k(v_k\cdot v_i)=a_i\|v_i\|^2$$ for $i\in\{1,\dots,n-1\}$ since $v_1,\dots,v_{n-1}$ are orthonormal. But since $\langle v_n, v_k\rangle=0$ we must have $a_i=0$. This means that $a_1=\dots=a_{n-1}=0$ and thus $v_1,\dots,v_n$ are linearly independent. 
\end{proof}
\end{prp}

\subsection{Orthonormal Basis}
\begin{defn}{Orthonormal Basis}{} A basis $v_1,\dots,v_n$ of an inner product space $V$ with $\dim(V)=n$ is called orthonormal if 
\begin{itemize}
\item $\|b_i\|=1$ for $i\in\{1,\dots,n\}$
\item $\langle b_i,b_j\rangle=\delta_{ij}$ for $i,j\in\{1,\dots,n\}$
\end{itemize}
\end{defn}

\begin{prp}{}{} The orthonormal basis is indeed a basis for an inner product space $V$. \tcbline
\begin{proof}
Since lists of orthonormal vectors are linearly independent and there are $n$ vectors, they must also span $V$ and thus is a basis. 
\end{proof}
\end{prp}

\begin{thm}{}{} Let $b_1,\dots,b_n$ be an orthonormal basis and $v=\sum_{k=1}^na_kb_k$. Then $$\|v\|^2=\sum_{k=1}^n\abs{a_k}^2$$ \tcbline
\begin{proof}
We have that 
\begin{align*}
\|v\|^2&=\left\langle\sum_{k=1}^na_kb_k, \sum_{k=1}^na_kb_k\right\rangle \\
&=\sum_{i=1}^n\sum_{j=1}^na_ia_j(b_i\cdot b_j)\\
&=\sum_{i=1}^n\sum_{j=1}^na_ia_j\delta_{ij}\\
&=\sum_{k=1}^n\abs{a_k}^2
\end{align*}
and we are done. 
\end{proof}
\end{thm}

\begin{prp}{}{} Let $b_1,\dots,b_n$ be an orthonormal basis of $V$. Then $$v=\sum_{k=1}^n\langle v,b_k\rangle b_k$$ \tcbline
\begin{proof}
Applying the inner product with $b_i$ for each $i\in\{1,\dots,n\}$ gives $a_i=\langle v,b_i\rangle$ since $\langle b_i,b_k\rangle=0$ for any $k\neq i$. Thus if $v=\sum_{k=1}^na_kb_k$ then $v=\sum_{k=1}^n\langle v,b_k\rangle b_k$ and we are done. 
\end{proof}
\end{prp}

\begin{thm}{Gram-Schmidt Procedure}{} Let $v_1,\dots v_m$ be a list of linearly independent vectors of $V$. Let $b_1=\frac{v_1}{\|v_1\|}$. For $i=2,\dots,m$. Define $$b_i=\frac{v_i-\sum_{k=0}^{i-1}\langle v_i,b_k\rangle b_k}{\|v_i-\sum_{k=0}^{i-1}\langle v_i,b_k\rangle b_k\|}$$ Then $b_1,\dots,b_m$ are orthonormal and has the same span as $v_1,\dots,v_m$. 
\end{thm}

\begin{thm}{}{} Every finite dimensional inner product space has an orthonormal basis. \tcbline
\begin{proof}
By the Gram-Schmidt procedure, every basis can be transformed into an orthonormal basis. 
\end{proof}
\end{thm}

\subsection{Orthogonal Complements}
\begin{defn}{Orthogonal Complement}{} Let $U$ be a subset of an inner product space $V$. The orthogonal complement of $U$ is defined as $$U^{\perp}=\{v\in V|\langle v,u\rangle=0\text{ for all }u\in U\}$$
\end{defn}

\begin{prp}{}{} Let $U$ be a subset of $V$. 
\begin{itemize}
\item $U$ is a subspace of $V$ if and only if $U^\perp$ is a subspace of $V$. 
\item $\{0\}^\perp=V$
\item $V^\perp=\{0\}$
\item $U\cap U^\perp=\{0\}$
\item If $W\subseteq U$, then $U^\perp\subseteq W^\perp$
\end{itemize}
\end{prp}

\begin{prp}{}{} Let $U$ be a finite dimensional subspace of $V$. Then $$U=(U^\perp)^\perp$$
\end{prp}

\begin{thm}{}{} Suppose $U$ is a finite dimensional subspace of $V$. Then $$V=U\oplus U^\perp$$ and $$\dim(U)+\dim(U^\perp)=\dim(V)$$
\end{thm}

\subsection{Orthogonal Maps}
\begin{defn}{Orthgonal Maps}{} A linear map $T:V\to V$ is said to be orthogonal if $$\langle T(v), T(w)\rangle=\langle v, w\rangle$$ for all $v,w\in V$. 
\end{defn}

One can think of orthgonal maps as orthgonality preserving maps. If $\langle v,w\rangle=0$ then $\langle T(v),T(w)\rangle=0$ which means orthogonality is preserved.

\begin{prp}{}{} Let $T:V\to V$ be a linear map over an inner product space $V$. Let $A$ represent the linear map $T$. Then the following are equivalent. 
\begin{itemize}
\item $T$ is orthogonal
\item $A$ is orthogonal
\item $T$ maps orthonormal bases to orthonormal bases
\end{itemize} \tcbline
\begin{proof} Suppose that $T:V\to V$ is represented by $A$. 
\begin{itemize}
\item $(1)\iff(2)$: We have that $\langle T(v),T(w)\rangle=v^TA^TAw$. Thus it is equal to $v^Tw$ if and only if $A^TA=I$. 
\item $(1)\iff(3)$: Suppose that $T$ is orthogonal. Suppose that $\{b_1,\dots,b_n\}$ is orthonormal. Then $\langle T(b_i),T(b_j)\rangle=\langle b_i,b_j\rangle=0$ for $i,j\in\{1,\dots,n\}$. Thus $\{T(b_1),\dots,T(b_n)\}$ is orthogonal. But they are also orthonormal since $\|T(b_i)\|^2=\langle T(b_i),T(b_i)\rangle=\langle b_i,b_i\rangle=\|b_i\|^2=1$. This means that $\|T(b_i)\|=1$ for $i\in\{1,\dots,n\}$. \\~\\
Now suppose that $T$ maps orthonormal bases to orthonormal bases. Then if $\{b_1,\dots,b_n\}$ is orthonormal, we have 
\begin{align*}
\langle T(v),T(w)\rangle&=\left\langle\left(\sum_{k=1}^nv_kT(b_k)\right),\left(\sum_{k=1}^nw_kT(b_k)\right)\right\rangle\\
&=\sum_{k=1}^n(v_kw_k)\langle T(b_k),T(b_k)\rangle\tag{$\langle T(b_i),T(b_j)\rangle=0$ if $i\neq j$}\\
&=\sum_{k=1}^nv_kw_k\\
&=\langle v,w\rangle
\end{align*}
Thus we are done. 
\end{itemize}
\end{proof}
\end{prp}

\pagebreak
\section{Orthgonality in $\R^n$}
\subsection{Reduction of Quadratic Forms over $\R$}
Orthogonality is interesting for real matrices because the notion of similarity and congruence coinincide under orthogonality. Notice that being similar and congruent to a diagonal matrix at the same time means that there exists an invertible $P$ such that $P^TAP=P^{-1}AP=D$. \\~\\

In the remaining sections we treat the adjugate in the case of $\R^n$ and save the case for $\C^n$ in another chapter. Then $V$ in the remaining sections will only denote $\R^n$. 

\begin{defn}{Euclidean Vector Space}{} An Euclidean vector space is $\R^n$ equipped with an inner product. 
\end{defn}

\begin{prp}{}{} A function $b:V\times V\to\R$ is an inner product over $\R$ if and only if $b$ is bilinear and positive definite. 
\end{prp}

\begin{lmm}{Polarization Identity}{} For $x,y\in\R^n$, $$\langle x,y\rangle=\frac{1}{4}\|x+y\|^2-\frac{1}{4}\|x-y\|^2$$\tcbline
\begin{proof} Simple proof using the fact that $\|x\|^2=\langle x,x\rangle$. 
\end{proof}
\end{lmm}

\begin{prp}{Sylvester's Law of Inertia}{} A quadratic form $q$ over $\R$ has the form $$q(x_1,\dots,x_n)=\sum_{k=1}^tx_k^2-\sum_{k=1}^ux_k^2$$ where $t+u=\text{rank}(q)$. The pair $(t,u)$ is called the signature of $q$. This reduced quadratic form is also unique in the sense that the number of positives and number of negatives of any two reduced forms are the same. \\~\\ Moreover, every symmetric matrix is congruent to a matrix of the form $$\begin{pmatrix}
I_t & 0 & 0\\
0 & -I_u & 0\\
0 & 0 & 0\\
\end{pmatrix}$$ where $(t,u)$ is the signature of the quadratic form. \tcbline
\begin{proof}
We saw in theorem 1.2.7 that every quadratic form can be expressed as $$q(y_1,\dots,y_n)=\sum_{k=1}^nc_ky_k^2$$ By doing a basis change with $b_k'=\frac{1}{\sqrt{c_k}}b_k$ whenever $c_k\neq 0$ will give us the above sum. For those that have $c_k=0$, the terms vanish and are exactly in the kernel of the quadratic form thus $t+u=\text{rank}(q)$. \\~\\
The second part is direct from the fact that same quadratic forms with different matrix representations imply their representations are similar. 
\end{proof}
\end{prp}

\subsection{Reduction of Inner Products}
\begin{defn}{Dot Product}{} The dot product in $\R^n$ is defined to be the inner product given by $$x\cdot y=x_1y_1+\dots+x_ny_n$$ in standard basis. 
\end{defn}

\begin{thm}{}{} Let $\langle \cdot,\cdot\rangle$ be an inner product on a real vector space $V$. Then there exists an basis $b_1,\dots,b_n$ of $V$ such that the inner product, when represented in the orthonormal basis, takes the form of exactly the dot product. \tcbline
\begin{proof}
Let $\langle \cdot,\cdot\rangle$ be our inner product in question. Define a quadratic form by $$q(x)=\langle x,x\rangle=\|x\|^2$$ We know that this quadratic form can be reduced to $$q(x_1,\dots,x_n)=x_1^2+\dots+x_t^2-x_{t+1}^2-\dots x_{t+u}^2$$ Now we must have $u=0$ since if $u>0$, then the basis vector $b_{t+1}$ satisfies $q(b_{t+1})=-1$ and $q(b_{t+1})=\langle b_{t+1},b_{t+1}\rangle$ which is a contradiction since inner products are positive definite. Also $t=n$ since if $t<n$, then $\langle b_{t+1},b_{t+1}\rangle=0$ which is again a contradiction. \\~\\
Using polarization, we see that $\langle x,y\rangle=x_1y_1+\dots+x_ny_n$ in that basis and we are done. 
\end{proof}
\end{thm}

With this theorem, we know that any inner product can be expressed in the dot product as long as it is under a suitable basis. Thus we now reduce our discussion to only the dot product, as our standard inner product in $\R^n$. \\~\\
The below theorem, while unrelated to the reduction of inner products, is a result of Gram-schmidt process that is only true for real matrices. 

\begin{thm}{QR Decomposition}{} Let $A$ be an $n\times n$ matrix over $\R$. Then we can write $A=QR$ where $Q$ is orthogonal and $R$ is upper triangular. \tcbline
\begin{proof}We split the matrices into two cases. Firstly consider the case where $A$ is invertible. We can treat $A$ as a change of basis matrix from the basis $\{a_1,\dots,a_n\}$ where $a_k$ is the column of $A$ for $k\in\{1,\dots,n\}$. This change of basis matrix takes $\{a_1,\dots,a_n\}$ to $\{e_1,\dots,e_n\}$ which is the standard basis. Apply the Gram-schmidt process to $\{a_1,\dots,a_n\}$ to get $\{b_1,\dots,b_n\}$ which is an orthonormal basis. Let $Q$ be the change of basis matrix from $\{b_1,\dots,b_n\}$ to $\{e_1,\dots,e_n\}$. Let $R$ be the change of basis matrix from $\{a_1,\dots,a_n\}$ to $\{b_1,\dots,b_n\}$. Then clearly $A=QR$. We just have to show that $Q$ is orthogonal and $R$ is upper triangular. \\~\\
$Q$ being orthonormal is trivial since columns of $Q$ are just $b_1,\dots,b_n$. Using the Gram-schimdt process, we can see that the change of basis from $\{b_1,\dots,b_n\}$ to $\{a_1,\dots,a_n\}$, each $b_k$ is only affected by $a_1,\dots,a_k$ from the old basis. This means that the change of basis matrix must be upper triangular and its inverse must also be upper triangular. \\~\\
Now we also have the case when $A$ is not invertible. 
\end{proof}
\end{thm}

We now give an example of QR decomposition, in conjunction with the Gram-schmidt procedure. 

\begin{eg}{}{} Find the QR decomposition of $$A=\begin{pmatrix}
-1 & 0 & -2\\
2 & 0 & -1\\
0 & -2 & -2
\end{pmatrix}$$ \tcbline
\begin{proof}
A quick check shows that $A$ is invertible. Let $a_1,a_2,a_3$ be the columns of $A$. We start the Gram-schimdt process by taking the new basis $f_1=\frac{a_1}{\|a_1\|}=\begin{pmatrix}-\frac{1}{\sqrt{5}}\\ \frac{2}{\sqrt{5}}\\ 0\end{pmatrix}$. We also need to keep track on the change of basis matrix. We have that $a_1=\sqrt{5}f_1$. \\~\\
For the next step, we find that 
\begin{align*}
f_2&=\frac{a_2-(a_2\cdot f_1)f_1}{\|a_2-(a_2\cdot f_1)f_1\|}\\
&=\frac{a_2}{\|a_2\|}\\
&=\begin{pmatrix} 0\\ 0\\ -1\end{pmatrix}
\end{align*}
This means that $a_2=2f_2$. \\~\\
Finally, we have that 
\begin{align*}
f_3&=\frac{a_3-(a_3\cdot f_1)f_1-(a_3\cdot f_2)f_2}{\|a_3-(a_3\cdot f_1)f_1-(a_3\cdot f_2)f_2\|}\\
&=\frac{a_3-2f_2}{\|a_3-2f_2\|}\\
&=\frac{a_3-2f_2}{\sqrt{5}}\\
&=\begin{pmatrix}-\frac{2}{\sqrt{5}}\\ -\frac{1}{\sqrt{5}}\\ 0\end{pmatrix}
\end{align*}
We have that $a_3=2f_2+\sqrt{5}f_3$. Combining everything together, we have that $$\begin{pmatrix}
-1 & 0 & -2\\
2 & 0 & -1\\
0 & -2 & -2
\end{pmatrix}=\begin{pmatrix}
-\frac{1}{\sqrt{5}} & 0 & -\frac{2}{\sqrt{5}}\\
\frac{2}{\sqrt{5}} & 0 & -\frac{1}{\sqrt{5}}\\
0 & -1 & 0
\end{pmatrix}\begin{pmatrix}
\sqrt{5} & 0 & 0\\
0 & 2 & 2\\
0 & 0 & \sqrt{5}
\end{pmatrix}$$
\end{proof}
\end{eg}

\subsection{Adjoints}
\begin{prp}{}{} Let $V$ be an inner product space and $T:V\to V$ be a linear map. Then there exists a unique linear map $T^\ast:V\to V$ such that $$\langle T(v), w\rangle=\langle v, T^\ast(w)\rangle$$ for all $v,w\in V$. \tcbline
\begin{proof}
Let $T$ be a linear map. Then the function $\tau(v,w)=\langle T(v), w\rangle$ is a bilinear form since the inner product is bilinear. But we know that bilinear forms can be represented by a matrix multiplication, namely $\tau(v,w)=v^TAw$ where $A$ is defined as in theorem 1.3.2. Then treating $Aw$ as the linear map $T^\ast(w)$ and since $v^Tw=v\cdot w$, we have that $\tau(v,w)=v\cdot T^\ast(w)$ thus proving existence. Uniqueness follows naturally by construction of the matrix $A$. 
\end{proof}
\end{prp}

\begin{defn}{Adjoint of a Linear Map}{} $T^\ast$ in the above case is called the adjoint of $T$. 
\end{defn}

\begin{defn}{Self-Adjoint}{} A linear map $T:V\to V$ is said to be self-adjoint if $T^*=T$
\end{defn}

\begin{prp}{}{} Let $T$ be a linear map represented by a matrix $A$. Then the following are true. 
\begin{itemize}
\item $T$ is self-adjoint if and only if $A$ is symmetric. 
\item $T$ is orthogonal if and only if $T^\ast=T^{-1}$. 
\end{itemize}
\tcbline
\begin{proof}
Let $T$ be self-adjoint. Then $Av\cdot w=v\cdot Aw$ for all $v,w\in V$. 
\end{proof}
\end{prp}

\begin{prp}{}{} Let $T:\R^n\to\R^n$ be self-adjoint. Then every eigenvalues of $T$ are real. \tcbline
\begin{proof}
Suppose that $T(v)=Av$ where $A$ is a representation of $T$. Suppose that $\lambda$ is an eigenvalue of $T$. Then $Av=\lambda v$ for some eigenvector $v\in\R^n$. Then taking complex conjugates give 
\begin{align*}
\overline{Av}&=\overline{\lambda v}\\
A\overline{v}&=\overline{\lambda}\overline{v}
\end{align*}
Taking the transpose of $Av=\lambda v$ gives $v^TA^T=\lambda v^T$ and $v^TA=\lambda v^T$. Multiplying $\overline{v}$ on bothe sides give 
\begin{align*}
v^TA\overline{v}&=\lambda v^T\overline{v}\\
\overline{\lambda}v^T\overline{v}&=\lambda v^T\overline{v}
\end{align*}
But $v^T\overline{v}=v_1\overline{v_1}+\dots+v_n\overline{v_n}=\abs{v_1}^2+\dots+\abs{v_n}^2$ which is $0$ if and only if $v$ is $0$. Since eigenvectors are taken to be nonzero, we must have $\lambda=\overline{\lambda}$ and thus $\lambda$ is real. 
\end{proof}
\end{prp}

\begin{thm}{}{} Let $T:\R^n\to\R^n$ be a linear map on the inner product space $\R^n$ that is seldf-adjoint. Then there exists an orthonormal basis consisting of eigenvectors of $T$. \\~\\
Equivalently, for every quadratic form $q$ on $V$, there exists an orthonormal basis $b_1,\dots,b_n$ such that $$q(x_1,\dots,x_n)=\sum_{k=1}^nc_kx_k^2$$ for some $c_1,\dots,c_n\in\R$. \tcbline
\begin{proof}
Notice that the two statements are exactly the same and I will ommit the reason. \\~\\
We prove by induction on $n$. Suppose that the theorem holds for $n-1$. Let $T$ be the linear map. By the above we know that $T$ has at least one eigenvalue in $\R$, say $\lambda_1$. Let $f_1$ be the corresponding eigenvector with magnitude $1$. \\~\\
Consider the orthogonal complement $W=\{w\in V|w\cdot f_1=0\}$ of $f_1$. Since $W$ is the kernel of the linear map $S:V\to\R$ defined by $S(v)=v\cdot f_1$, it is a subspace of $V$ dimension $n-1$. I claim that $T(W)\subseteq W$. \\~\\
Let $w\in W$. We have $$T(w)\cdot f_1=w\cdot T(f_1)=w\cdot\lambda_1f_1=0$$ by self-adjoint. Thus we have shown that $T(W)\subseteq W$. \\~\\
Applying the induction hypothesis on $W$, we have an orthonormal basis $f_2,\dots,f_n$ of $W$ consisting of eigenvectors of $T$. By definition, $f_1,\dots,f_n$ is an orthonormal basis and we are done. 
\end{proof}
\end{thm}

Notice that the above two statements are also equivalent to saying that every real symmetric matrix is congruent and similar to a diagonal matrix. 

\begin{prp}{}{} If $T:\R^n\to\R^n$ is self-adjoint, and $\lambda,\mu$ are distinct eigenvalues of $T$ with eigenvectors $v,w$, then $v\cdot w=0$. \tcbline
\begin{proof}
We have that
\begin{align*}
v^TAw&=v\cdot Aw\\
&=v^T\mu w\\
&=\mu(v\cdot w)
\end{align*} and 
\begin{align*}
v^TAw&=v^TA^Tw\\
&=(Av)^Tw\\
&=(\lambda v)^Tw\\
&=\lambda v^Tw\\
&=\lambda(v\cdot w)
\end{align*}
Comparing the two results, we have that $(\mu-\lambda)(v\cdot w)=0$ and thus $v\cdot w=0$. 
\end{proof}
\end{prp}

The proposition will prove itself to be useful in finding an orthonormal basis for self-adjoint linear maps. 

\subsection{Singular Value Decomposition}
\begin{thm}{Singular Value Decomposition for Linear Maps}{} Let $T:\R^n\to\R^m$ be a linear map of rank $k$. Then there eixsts unique positive numbers $\gamma_1\geq\gamma_2\geq\dots\geq\gamma_k\geq0$ and orthonormal bases of $\R^n$ and $\R^m$ such that the matrix of $T$ with respect to these bases is $$\begin{pmatrix}
D & 0\\
0 & 0
\end{pmatrix}$$ where $D=\text{diag}(\gamma_1,\dots,\gamma_k)$. In fact, the $\gamma_i$ are exactly the nonzero eigenvalues of $T^*T$, each one appearing as many times as the dimension of the corresponding eigenspace. 
\end{thm}

\begin{thm}{Singular Value Decomposition for Matrices}{} Let $A_{m\times n}$ be a matrix. There exists unique singular values $\gamma_1\geq\gamma_2\geq\dots\gamma_k\geq0$ where $k=$rank$(A)$, and orthogonal matrices $P,Q$ such that $$\begin{pmatrix}
D & 0\\
0 & 0
\end{pmatrix}=P^TAQ$$ where $D=\text{diag}(\gamma_1,\dots,\gamma_k)$. 
\end{thm}

We present an example of singular value decomposition for illustration. 

\begin{eg}{}{} Find the singular value decomposition of the matrix $$A=\begin{pmatrix}
4 & 11 & 14\\
8 & 7 & -2
\end{pmatrix}$$ \tcbline
\begin{proof}
Step 1: We compute the singular values of $A$, which is just the squareroot of the eigenvalues of $A^TA$. Now 
$$A^TA=\begin{pmatrix}
80 & 100 & 40\\
100 & 170 & 140\\
40 & 140 & 200
\end{pmatrix}$$
We have that $c_{A^TA}(x)=x(360-x)(90-x)$. This means that the singular values are $\gamma_1=\sqrt{360}=6\sqrt{10}$ and $\gamma_2=\sqrt{90}=3\sqrt{10}$. Now we want $P$ and $Q$ such that $$P^TAQ=\begin{pmatrix}
6\sqrt{10} & 0 & 0\\
0 & 3\sqrt{10} & 0
\end{pmatrix}$$~\\
Step 2: We find the orthonormal eigenvectors of $A^TA$ so that it forms the matrix $Q$. This gives $$Q=\begin{pmatrix}
\frac{1}{3} & -\frac{2}{3} & \frac{2}{3}\\
\frac{2}{3} & -\frac{1}{3} & -\frac{2}{3}\\
\frac{2}{3} & \frac{2}{3} & \frac{1}{3}
\end{pmatrix}$$~\\
Step 3: We now calculate $P$ by finding the image of the above basis under $A$, and dividing it with the nonzero singular values. This gives 
\begin{align*}
P&=\begin{pmatrix}
\frac{1}{6\sqrt{10}}Ab_1 & \frac{1}{3\sqrt{10}}Ab_2
\end{pmatrix}\\
&=\begin{pmatrix}
\frac{3}{\sqrt{10}} & \frac{1}{\sqrt{10}}\\
\frac{1}{\sqrt{10}} & -\frac{3}{\sqrt{10}}
\end{pmatrix}
\end{align*}
This means that we are done. 
\end{proof}
\end{eg}
